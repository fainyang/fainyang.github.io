<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Operation Course Record]]></title>
    <url>%2Fpost%2Foperation-course%2F</url>
    <content type="text"><![CDATA[This blog is for record what I learn from this Operation Course shellSHORTCUTSCtrl + r：find the histroyCtrl + l：clear the terminalCtrl + a \ Ctrl + e：move to head\end in rowCtrl + w \ Ctrl + k：delete before\after cursor ZZ : VIM save and quit Ctrl+C: Interrupt (kill) the current foreground process running in in the terminal. This sends the SIGINT signal to the process, which is technically just a request—most processes will honor it, but some may ignore it.Ctrl+Z: Suspend the current foreground process running in bash. This sends the SIGTSTP signal to the process. To return the process to the foreground later, use the fg process_name command.Ctrl+D: Close the bash shell. This sends an EOF (End-of-file) marker to bash, and bash exits when it receives this marker. This is similar to running the exit command. top SHORTCUTS Shift + p：Sort by CPUShift + m：Sort by Memory find large file1234567891011121314151617du -x --max-depth=1 / |sort -k1 -nrdu -x -d 1 -h / |sort -k1 -nr# find which folder has most inodefind -type f | awk -F / -v OFS=/ '&#123;$NF="";dir[$0]++&#125;END&#123;for(i in dir)print dir[i]" "i&#125;'| sort -k1 -nr | head769 ./wrk/obj/openssl-1.1.1b/test/551 ./.cache/mozilla/firefox/hha91z74.default-release/cache2/entries/462 ./wrk/obj/openssl-1.1.1b/doc/man3/229 ./wrk/obj/LuaJIT-2.1.0-beta3/src/212 ./wrk/obj/openssl-1.1.1b/test/certs/207 ./wrk/obj/openssl-1.1.1b/apps/199 ./wrk/obj/openssl-1.1.1b/crypto/asn1/194 ./.cache/gnome-software/icons/185 ./wrk/obj/openssl-1.1.1b/crypto/evp/157 ./wrk/obj/openssl-1.1.1b/test/recipes/ rename123456# find consumer.xml file and replace all aaaaaa to bbbbbbfind ./ -type f -name consumer.xml -exec sed -i "s/aaaaaa/bbbbbb/g" &#123;&#125; \;# find all .txt file and compress then copy to /home/.(find . -name "*.txt"|xargs tar -cvf test.tar) &amp;&amp; cp -f test.tar /home/. network12345# network statusnetstat -n | awk '/^tcp/ &#123;++S[$NF]&#125; END &#123;for(a in S) print a, S[a]&#125;'# get IP ip a|grep "global"|awk '&#123;print $2&#125;'| awk -F/ '&#123;print $1&#125;' nginx]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019 Reviews]]></title>
    <url>%2Fpost%2Freviews-2019%2F</url>
    <content type="text"><![CDATA[The pasted 2019 is so fast, this blog is to record my progress and regret of 2019. 2019 I migrate from seagroup to shopee due to organize migration, I can learn more complex and advanced architecture. Read 4 books, http, linux security, promethues, golang. I begin to learn algorithm and golang. I have more experience in ELK, Zabbix, Promethues and Network I begin to learn front-end react to develop shopee CDN platform. I also make some difference in my work, Ocha project migrate to new cluster, add more monitors for shopee order team. But there are also some regret. Less chance to gym and exercise, so weight increase. Never go out for travel in 2019. Oral English still so bad, need more practice. No some special area. I plan to deep into ELK What I will do in 2020 ? More exercise, keep weight around 70 kg Learn elk and pass Elastic Certified Engineer Read book 《Advanced Programming in the UNIX Environment》Third Edition Continue to learn algorithm one question one week.]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>review</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux performance optimization]]></title>
    <url>%2Fpost%2Flinux%2F</url>
    <content type="text"><![CDATA[This blog record some useful tools for linux.Linux performance tools CPUWhat is the Linux system load ? Can refer this article or thisLinux load averages are “system load averages” that show the running thread (task) demand on the system as an average number of running plus waiting threads. R + D state: Running + Uninterruptible Sleep stress sysstatWe can do some test by command stress12345678910111213apt install stress sysstat# stress i cpustress --cpu 1 --timeout 600# check uptime loadwatch -d uptime# check ALL cpu statusmpstat -P ALL 5# check every process or thread cpu usage pidstat -u 5 1 A thread is the basic unit of scheduling, and the process is the basic unit of resource owners Thread smallest sequence of programming instructions that can be managed independently by a scheduler Has its own register e.g. PC (program counter), SP (stack pointer) Process instance of a computer porgram that is being executed A process can have one or multiple thread Most programs are single threaded Parallel computing Run program currently on one or more CPUs Multi-threading (shared-memory) Multi-processing (independent-memory) context switch 12345678910# interval 5s output 1 row data$ vmstat 5procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 0 7005360 91564 818900 0 0 0 0 25 33 0 0 100 0 0# cs（context switch）# in（interrupt）# r（Running or Runnable）# b（Blocked） vmstat only provide overall context switch of system, pidstat -w shows process context switch 12345678910# interval 5s output 1 row data$ pidstat -w 5Linux 4.15.0 (ubuntu) 09/23/18 _x86_64_ (2 CPU) 08:18:26 UID PID cswch/s nvcswch/s Command08:18:31 0 1 0.20 0.00 systemd08:18:31 0 8 5.40 0.00 rcu_sched# cswch: voluntary context switches# nvcswch: non voluntary context switches we use sysbench to simulate a system multi-threaded scheduling switch. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748sysbench --threads=10 --max-time=300 threads run# check all context switchvmstat 1procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 6 0 0 6487428 118240 1292772 0 0 0 0 9019 1398830 16 84 0 0 0 8 0 0 6487428 118240 1292772 0 0 0 0 10191 1392312 16 84 0 0 0# check which process cause high cs 1398830# -w Report task switching activity # -u Report CPU utilization.pidstat -w -u 108:06:33 UID PID %usr %system %guest %wait %CPU CPU Command08:06:34 0 10488 30.00 100.00 0.00 0.00 100.00 0 sysbench08:06:34 0 26326 0.00 1.00 0.00 0.00 1.00 0 kworker/u4:2 08:06:33 UID PID cswch/s nvcswch/s Command08:06:34 0 8 11.00 0.00 rcu_sched08:06:34 0 16 1.00 0.00 ksoftirqd/108:06:34 0 471 1.00 0.00 hv_balloon08:06:34 0 1230 1.00 0.00 iscsid08:06:34 0 4089 1.00 0.00 kworker/1:508:06:34 0 4333 1.00 0.00 kworker/0:308:06:34 0 10499 1.00 224.00 pidstat08:06:34 0 26326 236.00 0.00 kworker/u4:208:06:34 1000 26784 223.00 0.00 sshd# -t display statistics for threads associated with selected tasks.pidstat -wt 108:14:05 UID TGID TID cswch/s nvcswch/s Command...08:14:05 0 10551 - 6.00 0.00 sysbench08:14:05 0 - 10551 6.00 0.00 |__sysbench08:14:05 0 - 10552 18911.00 103740.00 |__sysbench08:14:05 0 - 10553 18915.00 100955.00 |__sysbench08:14:05 0 - 10554 18827.00 103954.00 |__sysbench# check /proc/interruptswatch -d cat /proc/interrupts CPU0 CPU1...RES: 2450431 5279697 Rescheduling interrupts... cpu utilization12345678910111213141516171819202122232425262728cat /proc/stat | grep ^cpucpu 280580 7407 286084 172900810 83602 0 583 0 0 0cpu0 144745 4181 176701 86423902 52076 0 301 0 0 0cpu1 135834 3226 109383 86476907 31525 0 282 0 0 0# perf is a good toolsperf topperf record # Ctrl+C stopperf report # show record# Best practice how to perf container process# perf record in host then perf report in container# install perf in host apt-get install -y linux-tools-common linux-tools-generic linux-tools-$(uname -r)）perf record -g -p &lt; pid&gt;docker cp perf.data test:/tmp docker exec -i -t test bashcd /tmp/ apt-get update &amp;&amp; apt-get install -y linux-tools linux-perf procpsperf_4.9 report# pstress display a tree of processespstree | grep stress many Z (zombie) state123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687toptop - 05:56:23 up 17 days, 16:45, 2 users, load average: 2.00, 1.68, 1.39Tasks: 247 total, 1 running, 79 sleeping, 0 stopped, 115 zombie%Cpu0 : 0.0 us, 0.7 sy, 0.0 ni, 38.9 id, 60.5 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu1 : 0.0 us, 0.7 sy, 0.0 ni, 4.7 id, 94.6 wa, 0.0 hi, 0.0 si, 0.0 st... PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 4340 root 20 0 44676 4048 3432 R 0.3 0.0 0:00.05 top 4345 root 20 0 37280 33624 860 D 0.3 0.0 0:00.01 app 4344 root 20 0 37280 33624 860 D 0.3 0.4 0:00.01 app 1 root 20 0 160072 9416 6752 S 0.0 0.1 0:38.59 systemd...# dstat check cpu and IOdstat 1 10You did not select any stats, using -cdngy by default.--total-cpu-usage-- -dsk/total- -net/total- ---paging-- ---system--usr sys idl wai stl| read writ| recv send| in out | int csw 0 0 96 4 0|1219k 408k| 0 0 | 0 0 | 42 885 0 0 2 98 0| 34M 0 | 198B 790B| 0 0 | 42 138 0 0 0 100 0| 34M 0 | 66B 342B| 0 0 | 42 135 0 0 84 16 0|5633k 0 | 66B 342B| 0 0 | 52 177 0 3 39 58 0| 22M 0 | 66B 342B| 0 0 | 43 144 0 0 0 100 0| 34M 0 | 200B 450B| 0 0 | 46 147 0 0 2 98 0| 34M 0 | 66B 342B| 0 0 | 45 134 0 0 0 100 0| 34M 0 | 66B 342B| 0 0 | 39 131 0 0 83 17 0|5633k 0 | 66B 342B| 0 0 | 46 168 0 3 39 59 0| 22M 0 | 66B 342B| 0 0 | 37 134# find the D pidtop... PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 4340 root 20 0 44676 4048 3432 R 0.3 0.0 0:00.05 top 4345 root 20 0 37280 33624 860 D 0.3 0.0 0:00.01 app 4344 root 20 0 37280 33624 860 D 0.3 0.4 0:00.01 app...# for example pid = 4344pidstat -d -p 4344 1 306:38:50 UID PID kB_rd/s kB_wr/s kB_ccwr/s iodelay Command06:38:51 0 4344 0.00 0.00 0.00 0 app06:38:52 0 4344 0.00 0.00 0.00 0 app06:38:53 0 4344 0.00 0.00 0.00 0 app# check all process, find 6082pidstat -d 1 20...06:48:46 UID PID kB_rd/s kB_wr/s kB_ccwr/s iodelay Command06:48:47 0 4615 0.00 0.00 0.00 1 kworker/u4:106:48:47 0 6080 32768.00 0.00 0.00 170 app06:48:47 0 6081 32768.00 0.00 0.00 184 app 06:48:47 UID PID kB_rd/s kB_wr/s kB_ccwr/s iodelay Command06:48:48 0 6080 0.00 0.00 0.00 110 app 06:48:48 UID PID kB_rd/s kB_wr/s kB_ccwr/s iodelay Command06:48:49 0 6081 0.00 0.00 0.00 191 app 06:48:49 UID PID kB_rd/s kB_wr/s kB_ccwr/s iodelay Command 06:48:50 UID PID kB_rd/s kB_wr/s kB_ccwr/s iodelay Command06:48:51 0 6082 32768.00 0.00 0.00 0 app06:48:51 0 6083 32768.00 0.00 0.00 0 app 06:48:51 UID PID kB_rd/s kB_wr/s kB_ccwr/s iodelay Command06:48:52 0 6082 32768.00 0.00 0.00 184 app06:48:52 0 6083 32768.00 0.00 0.00 175 app 06:48:52 UID PID kB_rd/s kB_wr/s kB_ccwr/s iodelay Command06:48:53 0 6083 0.00 0.00 0.00 105 app...# use stracestrace -p 6082strace: attach: ptrace(PTRACE_SEIZE, 6082): Operation not permittedps aux | grep 6082root 6082 0.0 0.0 0 0 pts/0 Z+ 13:43 0:00 [app] &lt;defunct&gt;# find blkdev_direct_IO perf record -gperf report 12345678# find father of 3084pstree -aps 3084systemd,1 └─dockerd,15006 -H fd:// └─docker-containe,15024 --config /var/run/docker/containerd/containerd.toml └─docker-containe,3991 -namespace moby -workdir... └─app,4009 └─(app,3084) soft interruptLinux divides the interrupt handling process into two phases, the upper half and the lower half: The first part is used to quickly handle interrupts. It runs in interrupt disable mode and mainly deals with hardware-related or time-sensitive tasks. The second part is used to delay processing of the unfinished work in the upper half, usually running as a kernel thread. 12345678910111213141516171819202122232425262728293031323334# /proc/softirqs provides the operation of soft interrupts;# /proc/interrupts provides the operation of hard interrupts.cat /proc/softirqs CPU0 CPU1 HI: 0 0 TIMER: 811613 1972736 NET_TX: 49 7 NET_RX: 1136736 1506885 BLOCK: 0 0 IRQ_POLL: 0 0 TASKLET: 304787 3691 SCHED: 689718 1897539 HRTIMER: 0 0 RCU: 1330771 1354737ps aux | grep softirqroot 7 0.0 0.0 0 0 ? S Oct10 0:01 [ksoftirqd/0]root 16 0.0 0.0 0 0 ? S Oct10 0:01 [ksoftirqd/1]# sar - Collect, report, or save system activity information.sar -n DEV 115:03:46 IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s %ifutil15:03:47 eth0 12607.00 6304.00 664.86 358.11 0.00 0.00 0.00 0.0115:03:47 docker0 6302.00 12604.00 270.79 664.66 0.00 0.00 0.00 0.0015:03:47 lo 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0015:03:47 veth9f6bbcd 6302.00 12604.00 356.95 664.66 0.00 0.00 0.00 0.05# check TCP package SYN ddos attacktcpdump -i eth0 -n tcp port 8015:11:32.678966 IP 192.168.0.2.18238 &gt; 192.168.0.30.80: Flags [S], seq 458303614, win 512, length 0... summary Memory buffer &amp; cacheBuffers are temporary storage of raw disk blocks, that is, used to cache data on the disk, usually not particularly large (about 20MB). In this way, the kernel can centralize scattered writes and optimize disk writes in a unified manner. For example, multiple small writes can be combined into a single large write. Cached is a page cache that reads files from disk, that is, it is used to cache data read from files. This way, the next time you access these file data, you can quickly get it directly from memory without having to access the slow disk again. SReclaimable is part of Slab. Slab consists of two parts, the recyclable part is recorded with SReclaimable; the non-recyclable part is recorded with SUnreclaim. 1234567891011121314151617181920212223# Before using cachestat and cachetop, we must first install the bcc packagesudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 4052245BD4284CDDecho "deb https://repo.iovisor.org/apt/xenial xenial main" | sudo tee /etc/apt/sources.list.d/iovisor.listsudo apt-get updatesudo apt-get install -y bcc-tools libbcc-examples linux-headers-$(uname -r)export PATH=$PATH:/usr/share/bcc/tools#cachestat provides read and write hits for the entire system cache.# 1s interval 3 row datacachestat 1 3 TOTAL MISSES HITS DIRTIES BUFFERS_MB CACHED_MB 2 0 2 1 17 279 2 0 2 1 17 279 2 0 2 1 17 279 #cachetop provides cache hits for each process.cachetop11:58:50 Buffers MB: 258 / Cached MB: 347 / Sort: HITS / Order: ascendingPID UID CMD HITS MISSES DIRTIES READ_HIT% WRITE_HIT%13029 root python 1 0 0 100.0% 0.0% Check the specify file cache, we can use pcstat123456789101112131415161718192021export GOPATH=~/goexport PATH=~/go/bin:$PATHgo get golang.org/x/sys/unixgo get github.com/tobert/pcstat/pcstat# first trypcstat /bin/ls+---------+----------------+------------+-----------+---------+| Name | Size (bytes) | Pages | Cached | Percent ||---------+----------------+------------+-----------+---------|| /bin/ls | 133792 | 33 | 0 | 000.000 |+---------+----------------+------------+-----------+---------+# second trylspcstat /bin/ls+---------+----------------+------------+-----------+---------+| Name | Size (bytes) | Pages | Cached | Percent ||---------+----------------+------------+-----------+---------|| /bin/ls | 133792 | 33 | 33 | 100.000 |+---------+----------------+------------+-----------+---------+ memory leak Check the whole memory by vmstat1234567891011vmstat 3procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----r b swpd free buff cache si so bi bo in cs us sy id wa stprocs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----r b swpd free buff cache si so bi bo in cs us sy id wa st0 0 0 6601824 97620 1098784 0 0 0 0 62 322 0 0 100 0 00 0 0 6601700 97620 1098788 0 0 0 0 57 251 0 0 100 0 00 0 0 6601320 97620 1098788 0 0 0 3 52 306 0 0 100 0 00 0 0 6601452 97628 1098788 0 0 0 27 63 326 0 0 100 0 02 0 0 6601328 97628 1098788 0 0 0 44 52 299 0 0 100 0 00 0 0 6601080 97628 1098792 0 0 0 0 56 285 0 0 100 0 0 The free column of memory is constantly changing and is in a downward trend; the buffer and cache remain basically unchanged. memleak is a tool in the bcc package123456789101112131415161718192021222324252627/usr/share/bcc/tools/memleak -a -p $(pidof app)WARNING: Couldn't find .text section in /appWARNING: BCC can't handle sym look ups for /app addr = 7f8f704732b0 size = 8192 addr = 7f8f704772d0 size = 8192 addr = 7f8f704712a0 size = 8192 addr = 7f8f704752c0 size = 8192 32768 bytes in 4 allocations from stack [unknown] [app] [unknown] [app] start_thread+0xdb [libpthread-2.27.so] # [unknown] is app is in docker# copy exec file from docker, then to checkdocker cp app:/app /app/usr/share/bcc/tools/memleak -p $(pidof app) -aAttaching to pid 12512, Ctrl+C to quit.[03:00:41] Top 10 stacks with outstanding allocations: addr = 7f8f70863220 size = 8192 addr = 7f8f70861210 size = 8192 addr = 7f8f7085b1e0 size = 8192 addr = 7f8f7085f200 size = 8192 addr = 7f8f7085d1f0 size = 8192 40960 bytes in 5 allocations from stack fibonacci+0x1f [app] child+0x4f [app] start_thread+0xdb [libpthread-2.27.so] swapData that has been modified by the application and has not yet been written to the disk (that is, dirty pages) must be written to the disk before memory can be released.These dirty pages can generally be written to disk in two ways. You can use the system call fsync in your application to synchronize dirty pages to disk; It can also be left to the system, and the kernel thread pdflush is responsible for refreshing these dirty pages.Swap writes these infrequently accessed memory to disk, then releases this memory for use by other more needed processes. When the memory is accessed again, it is sufficient to re-read it from disk.In the NUMA architecture, multiple processors are divided into different nodes, and each node has its own local memory space. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687numactl --hardwareavailable: 2 nodes (0-1)node 0 cpus: 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30node 0 size: 32674 MBnode 0 free: 418 MBnode 1 cpus: 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31node 1 size: 32768 MBnode 1 free: 212 MBnode distances:node 0 1 0: 10 21 1: 21 10 cat /proc/zoneinfo...Node 0, zone Normal pages free 227894 min 14896 low 18620 high 22344... nr_free_pages 227894 nr_zone_inactive_anon 11082 nr_zone_active_anon 14024 nr_zone_inactive_file 539024 nr_zone_active_file 923986...# -r memory，-S Swap $ sar -r -S 104:39:56 kbmemfree kbavail kbmemused %memused kbbuffers kbcached kbcommit %commit kbactive kbinact kbdirty04:39:57 6249676 6839824 1919632 23.50 740512 67316 1691736 10.22 815156 841868 4 04:39:56 kbswpfree kbswpused %swpused kbswpcad %swpcad04:39:57 8388604 0 0.00 0 0.00 04:39:57 kbmemfree kbavail kbmemused %memused kbbuffers kbcached kbcommit %commit kbactive kbinact kbdirty04:39:58 6184472 6807064 1984836 24.30 772768 67380 1691736 10.22 847932 874224 20 04:39:57 kbswpfree kbswpused %swpused kbswpcad %swpcad04:39:58 8388604 0 0.00 0 0.00 … 04:44:06 kbmemfree kbavail kbmemused %memused kbbuffers kbcached kbcommit %commit kbactive kbinact kbdirty04:44:07 152780 6525716 8016528 98.13 6530440 51316 1691736 10.22 867124 6869332 0 04:44:06 kbswpfree kbswpused %swpused kbswpcad %swpcad04:44:07 8384508 4096 0.05 52 1.27# check cachetop 50 hit ratecachetop 512:28:28 Buffers MB: 6349 / Cached MB: 87 / Sort: HITS / Order: ascendingPID UID CMD HITS MISSES DIRTIES READ_HIT% WRITE_HIT%18280 root python 22 0 0 100.0% 0.0%18279 root dd 41088 41022 0 50.0% 50.0%# pages_free fluctuationwatch -d grep -A 15 'Normal' /proc/zoneinfoNode 0, zone Normal pages free 21328 min 14896 low 18620 high 22344 spanned 1835008 present 1835008 managed 1796710 protection: (0, 0, 0, 0, 0) nr_free_pages 21328 nr_zone_inactive_anon 79776 nr_zone_active_anon 206854 nr_zone_inactive_file 918561 nr_zone_active_file 496695 nr_zone_unevictable 2251 nr_zone_write_pending 0# sort by VmSwapfor file in /proc/*/status ; do awk '/VmSwap|Name|^Pid/&#123;printf $2 " " $3&#125;END&#123; print ""&#125;' $file; done | sort -k 3 -n -r | headdockerd 2226 10728 kBdocker-containe 2251 8516 kBsnapd 936 4020 kBnetworkd-dispat 911 836 kBpolkitd 1004 44 kB summary What’s the difference between system and disk ?A disk is a storage device (to be exact, a block device) that can be divided into different disk partitions. On a disk or disk partition, you can also create a file system and mount it in a directory on the system. In this way, the system can read and write files through this mount directory. In other words, a disk is a block device that stores data and is the carrier of a file system. Therefore, the file system still needs to ensure the persistent storage of data through disk. You will see this sentence in many places, everything in Linux is a file. In other words, you can access disks and files through the same file interface (such as open, read, write, close, etc.). What we usually mean by “documents” is actually ordinary documents. The disk or partition refers to the block device file. I-OThe Linux file system allocates two data structures for each file, an index node and a directory entry. They are mainly used to record the meta information and directory structure of files. 1234567891011121314151617181920# slabtop - display kernel slab cache information in real timeslabtopActive / Total Objects (% used) : 503302 / 506242 (99.4%) Active / Total Slabs (% used) : 10730 / 10730 (100.0%) Active / Total Caches (% used) : 100 / 143 (69.9%) Active / Total Size (% used) : 153659.58K / 155417.98K (98.9%) Minimum / Average / Maximum Object : 0.01K / 0.31K / 8.00K OBJS ACTIVE USE OBJ SIZE SLABS OBJ/SLAB CACHE SIZE NAME 103782 103782 100% 0.19K 2471 42 19768K dentry 73749 73749 100% 0.10K 1891 39 7564K buffer_head 45900 45900 100% 0.13K 765 60 6120K kernfs_node_cache 39985 39985 100% 0.58K 727 55 23264K inode_cache 34830 34830 100% 1.05K 1161 30 37152K ext4_inode_cache 18462 18462 100% 0.04K 181 102 724K ext4_extent_status 17784 17784 100% 0.20K 456 39 3648K vm_area_struct 15456 15456 100% 0.69K 336 46 10752K squashfs_inode_cache 14336 14144 0% 0.50K 224 64 7168K kmalloc-512 check io123456iostat -d -x 1 Device r/s w/s rkB/s wkB/s rrqm/s wrqm/s %rrqm %wrqm r_await w_await aqu-sz rareq-sz wareq-sz svctm %util loop0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 loop1 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 sda 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 sdb 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 % util is the disk I / O usage r/s + w/s is IOPS rkB/s + wkB/s is the throughput r_await + w_await is the response time 12345678910# check processpidstat -d 1 13:39:51 UID PID kB_rd/s kB_wr/s kB_ccwr/s iodelay Command 13:39:52 102 916 0.00 4.00 0.00 0 rsyslogdiotopTotal DISK READ : 0.00 B/s | Total DISK WRITE : 7.85 K/s Actual DISK READ: 0.00 B/s | Actual DISK WRITE: 0.00 B/s TID PRIO USER DISK READ DISK WRITE SWAPIN IO&gt; COMMAND 15055 be/3 root 0.00 B/s 7.85 K/s 0.00 % 0.00 % systemd-journald write issue12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758 top top - 14:43:43 up 1 day, 1:39, 2 users, load average: 2.48, 1.09, 0.63 Tasks: 130 total, 2 running, 74 sleeping, 0 stopped, 0 zombie %Cpu0 : 0.7 us, 6.0 sy, 0.0 ni, 0.7 id, 92.7 wa, 0.0 hi, 0.0 si, 0.0 st %Cpu1 : 0.0 us, 0.3 sy, 0.0 ni, 92.3 id, 7.3 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem : 8169308 total, 747684 free, 741336 used, 6680288 buff/cache KiB Swap: 0 total, 0 free, 0 used. 7113124 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 18940 root 20 0 656108 355740 5236 R 6.3 4.4 0:12.56 python 1312 root 20 0 236532 24116 9648 S 0.3 0.3 9:29.80 python3 # 92.7 io_wait # memroy most is used for buff/cache # check which deviceiostat -x -d 1 Device r/s w/s rkB/s wkB/s rrqm/s wrqm/s %rrqm %wrqm r_await w_await aqu-sz rareq-sz wareq-sz svctm %util loop0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 sdb 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 sda 0.00 64.00 0.00 32768.00 0.00 0.00 0.00 0.00 0.00 7270.44 1102.18 0.00 512.00 15.50 99.20# check which processpidstat -d 1 15:08:35 UID PID kB_rd/s kB_wr/s kB_ccwr/s iodelay Command 15:08:36 0 18940 0.00 45816.00 0.00 96 python 15:08:36 UID PID kB_rd/s kB_wr/s kB_ccwr/s iodelay Command 15:08:37 0 354 0.00 0.00 0.00 350 jbd2/sda1-8 15:08:37 0 18940 0.00 46000.00 0.00 96 python 15:08:37 0 20065 0.00 0.00 0.00 1503 kworker/u4:2 # check pid 18940strace -p 18940 strace: Process 18940 attached ...mmap(NULL, 314576896, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f0f7aee9000 mmap(NULL, 314576896, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f0f682e8000 write(3, "2018-12-05 15:23:01,709 - __main"..., 314572844 ) = 314572844 munmap(0x7f0f682e8000, 314576896) = 0 write(3, "\n", 1) = 1 munmap(0x7f0f7aee9000, 314576896) = 0 close(3) = 0 stat("/tmp/logtest.txt.1", &#123;st_mode=S_IFREG|0644, st_size=943718535, ...&#125;) = 0 # check pid 18940 open fileslsof -p 18940 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME python 18940 root cwd DIR 0,50 4096 1549389 / python 18940 root rtd DIR 0,50 4096 1549389 / … python 18940 root 2u CHR 136,0 0t0 3 /dev/pts/0 python 18940 root 3w REG 8,1 117944320 303 /tmp/logtest.txt # root cause is python wirte 300MB/s to /tmp/logtest.txt io_wait issue12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273top top - 14:27:02 up 10:30, 1 user, load average: 1.82, 1.26, 0.76 Tasks: 129 total, 1 running, 74 sleeping, 0 stopped, 0 zombie %Cpu0 : 3.5 us, 2.1 sy, 0.0 ni, 0.0 id, 94.4 wa, 0.0 hi, 0.0 si, 0.0 st %Cpu1 : 2.4 us, 0.7 sy, 0.0 ni, 70.4 id, 26.5 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem : 8169300 total, 3323248 free, 436748 used, 4409304 buff/cache KiB Swap: 0 total, 0 free, 0 used. 7412556 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 12280 root 20 0 103304 28824 7276 S 14.0 0.4 0:08.77 python 16 root 20 0 0 0 0 S 0.3 0.0 0:09.22 ksoftirqd/1 1549 root 20 0 236712 24480 9864 S 0.3 0.3 3:31.38 python3 # iowait is high, memory is ok.# util = 98%iostat -d -x 1Device r/s w/s rkB/s wkB/s rrqm/s wrqm/s %rrqm %wrqm r_await w_await aqu-sz rareq-sz wareq-sz svctm %util loop0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 sda 0.00 71.00 0.00 32912.00 0.00 0.00 0.00 0.00 0.00 18118.31 241.89 0.00 463.55 13.86 98.40 # check which porcesspidstat -d 1 14:39:14 UID PID kB_rd/s kB_wr/s kB_ccwr/s iodelay Command 14:39:15 0 12280 0.00 335716.00 0.00 0 python strace -p 12280 strace: Process 12280 attached select(0, NULL, NULL, NULL, &#123;tv_sec=0, tv_usec=567708&#125;) = 0 (Timeout) stat("/usr/local/lib/python3.7/importlib/_bootstrap.py", &#123;st_mode=S_IFREG|0644, st_size=39278, ...&#125;) = 0 stat("/usr/local/lib/python3.7/importlib/_bootstrap.py", &#123;st_mode=S_IFREG|0644, st_size=39278, ...&#125;) = 0 # filter result strace -p 12280 2&gt;&amp;1 | grep write # introduce new tool bcc# install bccsudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 4052245BD4284CDD echo "deb https://repo.iovisor.org/apt/$(lsb_release -cs) $(lsb_release -cs) main" | sudo tee /etc/apt/sources.list.d/iovisor.list sudo apt-get update sudo apt-get install bcc-tools libbcc-examples linux-headers-$(uname -r)cd /usr/share/bcc/tools./filetop -C TID COMM READS WRITES R_Kb W_Kb T FILE 514 python 0 1 0 2832 R 669.txt 514 python 0 1 0 2490 R 667.txt 514 python 0 1 0 2685 R 671.txt 514 python 0 1 0 2392 R 670.txt 514 python 0 1 0 2050 R 672.txt ... TID COMM READS WRITES R_Kb W_Kb T FILE 514 python 2 0 5957 0 R 651.txt 514 python 2 0 5371 0 R 112.txt 514 python 2 0 4785 0 R 861.txt 514 python 2 0 4736 0 R 213.txt 514 python 2 0 4443 0 R 45.txt # -T thread check thread 514ps -efT | grep 514root 12280 514 14626 33 14:47 pts/0 00:00:05 /usr/local/bin/python /app.py # new tool opensnoopopensnoop 12280 python 6 0 /tmp/9046db9e-fe25-11e8-b13f-0242ac110002/650.txt 12280 python 6 0 /tmp/9046db9e-fe25-11e8-b13f-0242ac110002/651.txt 12280 python 6 0 /tmp/9046db9e-fe25-11e8-b13f-0242ac110002/652.txt sql slow123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# use top to check overviewtoptop - 12:02:15 up 6 days, 8:05, 1 user, load average: 0.66, 0.72, 0.59Tasks: 137 total, 1 running, 81 sleeping, 0 stopped, 0 zombie%Cpu0 : 0.7 us, 1.3 sy, 0.0 ni, 35.9 id, 62.1 wa, 0.0 hi, 0.0 si, 0.0 st%Cpu1 : 0.3 us, 0.7 sy, 0.0 ni, 84.7 id, 14.3 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem : 8169300 total, 7238472 free, 546132 used, 384696 buff/cacheKiB Swap: 0 total, 0 free, 0 used. 7316952 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND27458 999 20 0 833852 57968 13176 S 1.7 0.7 0:12.40 mysqld27617 root 20 0 24348 9216 4692 S 1.0 0.1 0:04.40 python 1549 root 20 0 236716 24568 9864 S 0.3 0.3 51:46.57 python322421 root 20 0 0 0 0 I 0.3 0.0 0:01.16 kworker/u# io wait is high, so we check ioiostat -d -x 1Device r/s w/s rkB/s wkB/s rrqm/s wrqm/s %rrqm %wrqm r_await w_await aqu-sz rareq-sz wareq-sz svctm %util...sda 273.00 0.00 32568.00 0.00 0.00 0.00 0.00 0.00 7.90 0.00 1.16 119.30 0.00 3.56 97.20# rkB/s is 32 MB, util is 97.2%# find the pid of high IOpidstat -d 112:04:11 UID PID kB_rd/s kB_wr/s kB_ccwr/s iodelay Command12:04:12 999 27458 32640.00 0.00 0.00 0 mysqld12:04:12 0 27617 4.00 4.00 0.00 3 python12:04:12 0 27864 0.00 4.00 0.00 0 systemd-journal# next use strace+ lsof strace -f -p 27458[pid 28014] read(38, "934EiwT363aak7VtqF1mHGa4LL4Dhbks"..., 131072) = 131072[pid 28014] read(38, "hSs7KBDepBqA6m4ce6i6iUfFTeG9Ot9z"..., 20480) = 20480[pid 28014] read(38, "NRhRjCSsLLBjTfdqiBRLvN9K6FRfqqLm"..., 131072) = 131072[pid 28014] read(38, "AKgsik4BilLb7y6OkwQUjjqGeCTQTaRl"..., 24576) = 24576[pid 28014] read(38, "hFMHx7FzUSqfFI22fQxWCpSnDmRjamaW"..., 131072) = 131072[pid 28014] read(38, "ajUzLmKqivcDJSkiw7QWf2ETLgvQIpfC"..., 20480) = 20480# Thread 28014 is reading a large amount of data, and the descriptor read is 38. # We can execute the following lsof command and specify the thread number 28014, # specifically view this suspicious thread and suspicious file:lsof -p 28014# no output previous command # check the previous command, failedecho $?1#-t thread -a show commandpstree -t -a -p 27458mysqld,27458 --log_bin=on --sync_binlog=1... ├─&#123;mysqld&#125;,27922 ├─&#123;mysqld&#125;,27923 └─&#123;mysqld&#125;,28014# use pid not thread idlsof -p 27458COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME...​mysqld 27458 999 38u REG 8,1 512440000 2601895 /var/lib/mysql/test/products.MYD slow redis123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081toptop - 12:46:18 up 11 days, 8:49, 1 user, load average: 1.36, 1.36, 1.04Tasks: 137 total, 1 running, 79 sleeping, 0 stopped, 0 zombie%Cpu0 : 6.0 us, 2.7 sy, 0.0 ni, 5.7 id, 84.7 wa, 0.0 hi, 1.0 si, 0.0 st%Cpu1 : 1.0 us, 3.0 sy, 0.0 ni, 94.7 id, 0.0 wa, 0.0 hi, 1.3 si, 0.0 stKiB Mem : 8169300 total, 7342244 free, 432912 used, 394144 buff/cacheKiB Swap: 0 total, 0 free, 0 used. 7478748 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 9181 root 20 0 193004 27304 8716 S 8.6 0.3 0:07.15 python 9085 systemd+ 20 0 28352 9760 1860 D 5.0 0.1 0:04.34 redis-server 368 root 20 0 0 0 0 D 1.0 0.0 0:33.88 jbd2/sda1-8 149 root 0 -20 0 0 0 I 0.3 0.0 0:10.63 kworker/0:1H 1549 root 20 0 236716 24576 9864 S 0.3 0.3 91:37.30 python3 # high io wait 84.7%iostat -d -x 1Device r/s w/s rkB/s wkB/s rrqm/s wrqm/s %rrqm %wrqm r_await w_await aqu-sz rareq-sz wareq-sz svctm %util...sda 0.00 492.00 0.00 2672.00 0.00 176.00 0.00 26.35 0.00 1.76 0.00 0.00 5.43 0.00 0.00# why has write operation ? redis is just readpidstat -d 112:49:35 UID PID kB_rd/s kB_wr/s kB_ccwr/s iodelay Command12:49:36 0 368 0.00 16.00 0.00 86 jbd2/sda1-812:49:36 100 9085 0.00 636.00 0.00 1 redis-server# -f follow process and thread，-T system call time，-tt show timestrace -f -T -tt -p 9085[pid 9085] 14:20:16.826131 epoll_pwait(5, [&#123;EPOLLIN, &#123;u32=8, u64=8&#125;&#125;], 10128, 65, NULL, 8) = 1 &lt;0.000055&gt;[pid 9085] 14:20:16.826301 read(8, "*2\r\n$3\r\nGET\r\n$41\r\nuuid:5b2e76cc-"..., 16384) = 61 &lt;0.000071&gt;[pid 9085] 14:20:16.826477 read(3, 0x7fff366a5747, 1) = -1 EAGAIN (Resource temporarily unavailable) &lt;0.000063&gt;[pid 9085] 14:20:16.826645 write(8, "$3\r\nbad\r\n", 9) = 9 &lt;0.000173&gt;[pid 9085] 14:20:16.826907 epoll_pwait(5, [&#123;EPOLLIN, &#123;u32=8, u64=8&#125;&#125;], 10128, 65, NULL, 8) = 1 &lt;0.000032&gt;[pid 9085] 14:20:16.827030 read(8, "*2\r\n$3\r\nGET\r\n$41\r\nuuid:55862ada-"..., 16384) = 61 &lt;0.000044&gt;[pid 9085] 14:20:16.827149 read(3, 0x7fff366a5747, 1) = -1 EAGAIN (Resource temporarily unavailable) &lt;0.000043&gt;[pid 9085] 14:20:16.827285 write(8, "$3\r\nbad\r\n", 9) = 9 &lt;0.000141&gt;[pid 9085] 14:20:16.827514 epoll_pwait(5, [&#123;EPOLLIN, &#123;u32=8, u64=8&#125;&#125;], 10128, 64, NULL, 8) = 1 &lt;0.000049&gt;[pid 9085] 14:20:16.827641 read(8, "*2\r\n$3\r\nGET\r\n$41\r\nuuid:53522908-"..., 16384) = 61 &lt;0.000043&gt;[pid 9085] 14:20:16.827784 read(3, 0x7fff366a5747, 1) = -1 EAGAIN (Resource temporarily unavailable) &lt;0.000034&gt;[pid 9085] 14:20:16.827945 write(8, "$4\r\ngood\r\n", 10) = 10 &lt;0.000288&gt;[pid 9085] 14:20:16.828339 epoll_pwait(5, [&#123;EPOLLIN, &#123;u32=8, u64=8&#125;&#125;], 10128, 63, NULL, 8) = 1 &lt;0.000057&gt;[pid 9085] 14:20:16.828486 read(8, "*3\r\n$4\r\nSADD\r\n$4\r\ngood\r\n$36\r\n535"..., 16384) = 67 &lt;0.000040&gt;[pid 9085] 14:20:16.828623 read(3, 0x7fff366a5747, 1) = -1 EAGAIN (Resource temporarily unavailable) &lt;0.000052&gt;[pid 9085] 14:20:16.828760 write(7, "*3\r\n$4\r\nSADD\r\n$4\r\ngood\r\n$36\r\n535"..., 67) = 67 &lt;0.000060&gt;[pid 9085] 14:20:16.828970 fdatasync(7) = 0 &lt;0.005415&gt;[pid 9085] 14:20:16.834493 write(8, ":1\r\n", 4) = 4 &lt;0.000250&gt;lsof -p 9085redis-ser 9085 systemd-network 3r FIFO 0,12 0t0 15447970 piperedis-ser 9085 systemd-network 4w FIFO 0,12 0t0 15447970 piperedis-ser 9085 systemd-network 5u a_inode 0,13 0 10179 [eventpoll]redis-ser 9085 systemd-network 6u sock 0,9 0t0 15447972 protocol: TCPredis-ser 9085 systemd-network 7w REG 8,1 8830146 2838532 /data/appendonly.aofredis-ser 9085 systemd-network 8u sock 0,9 0t0 15448709 protocol: TCP# Descriptor number 3 is a pipe, number 5 is an eventpoll# number 7 is an ordinary file, and number 8 is a TCP socket.# Only the 7th ordinary file will generate disk write, # and the file path it operates on is /data/appendonly.aof. # The corresponding system calls include write and fdatasync.strace -f -p 9085 -T -tt -e fdatasyncstrace: Process 9085 attached with 4 threads[pid 9085] 14:22:52.013547 fdatasync(7) = 0 &lt;0.007112&gt;[pid 9085] 14:22:52.022467 fdatasync(7) = 0 &lt;0.008572&gt;[pid 9085] 14:22:52.032223 fdatasync(7) = 0 &lt;0.006769&gt;...[pid 9085] 14:22:52.139629 fdatasync(7) = 0 &lt;0.008183&gt;# final check the process in dockerPID=$(docker inspect --format &#123;&#123;.State.Pid&#125;&#125; app)nsenter --target $PID --net -- lsof -iCOMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEredis-ser 9085 systemd-network 6u IPv4 15447972 0t0 TCP localhost:6379 (LISTEN)redis-ser 9085 systemd-network 8u IPv4 15448709 0t0 TCP localhost:6379-&gt;localhost:32996 (ESTABLISHED)python 9181 root 3u IPv4 15448677 0t0 TCP *:http (LISTEN)python 9181 root 5u IPv4 15449632 0t0 TCP localhost:32996-&gt;localhost:6379 (ESTABLISHED) summary Network Bandwidth, which indicates the maximum transmission rate of the link. The unit is usually b/s (bits per second). Throughput, which indicates the amount of data successfully transmitted per unit of time. The unit is usually b/s (bits/second) or B/s (bytes/second). Throughput is limited by bandwidth, and throughput/bandwidth is the utilization of the network. Delay means the delay from the time the network request is sent until the remote response is received. PPS is the abbreviation of Packet Per Second (packet per second), which means the transmission rate in network packets. basic command123456789101112131415161718root@ubuntu:/home/feiyang# ifconfig ens33: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.17.3 netmask 255.255.255.0 broadcast 192.168.17.255 inet6 fe80::20c:29ff:fe35:6403 prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:0c:29:35:64:03 txqueuelen 1000 (Ethernet) RX packets 2193 bytes 150818 (150.8 KB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 203 bytes 24234 (24.2 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10&lt;host&gt; loop txqueuelen 1000 (Local Loopback) RX packets 17791 bytes 1264539 (1.2 MB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 17791 bytes 1264539 (1.2 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 errors indicates the number of packets with errors, such as check errors, frame synchronization errors, etc. dropped indicates the number of dropped packets, that is, the packet has received the Ring Buffer, but the packet was lost due to insufficient memory and other reasons; overruns indicates the number of overrun packets, that is, the network I / O speed is too fast, causing the packets in the Ring Buffer to be too late to be processed (the queue is full) and the packet loss; carrier indicates the number of packets with carrirer errors, such as mismatch in duplex mode, problems with physical cables, etc .; collisions indicates the number of collision packets. 123456789root@ubuntu:/home/feiyang# netstat -nlp | head -n 3Active Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 127.0.0.53:53 0.0.0.0:* LISTEN 549/systemd-resolve root@ubuntu:/home/feiyang# ss -lntp | head -n 3State Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 128 127.0.0.53%lo:53 0.0.0.0:* users:(("systemd-resolve",pid=549,fd=13)) LISTEN 0 128 0.0.0.0:22 0.0.0.0:* users:(("sshd",pid=1040,fd=3)) When the socket is connected (Established),Recv-Q indicates the number of bytes (that is, the length of the receive queue) that the socket buffer has not been taken away by the application.Send-Q indicates the number of bytes (that is, the length of the send queue) that have not been acknowledged by the remote host. When the socket is in the listening state (Listening),Recv-Q represents the current value of the syn backlog.Send-Q represents the largest syn backlog value. The syn backlog is the length of the semi-connected queue in the TCP protocol stack, and accordingly there is also a fully connected queue (accept queue) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293# netstatroot@ubuntu:/home/feiyang# netstat -sIp: Forwarding: 2 19787 total packets received 0 forwarded 0 incoming packets discarded 19785 incoming packets delivered 19373 requests sent outIcmp: 0 ICMP messages received 0 input ICMP message failed ICMP input histogram: 0 ICMP messages sent 0 ICMP messages failed ICMP output histogram:Tcp: 4 active connection openings 1 passive connection openings 4 failed connection attempts 0 connection resets received 1 connections established 294 segments received 195 segments sent out 0 segments retransmitted 0 bad segments received 4 resets sentUdp: 19388 packets received 0 packets to unknown port received 0 packet receive errors 19180 packets sent 0 receive buffer errors 0 send buffer errors IgnoredMulti: 107UdpLite:TcpExt: 3 delayed acks sent 47 packet headers predicted 2 acknowledgments not containing data payload received 170 predicted acknowledgments TCPBacklogCoalesce: 9 TCPAutoCorking: 12 TCPOrigDataSent: 180 TCPDelivered: 180IpExt: InMcastPkts: 279 OutMcastPkts: 66 InBcastPkts: 107 OutBcastPkts: 7 InOctets: 1405727 OutOctets: 1393405 InMcastOctets: 19672 OutMcastOctets: 6290 InBcastOctets: 9737 OutBcastOctets: 327 InNoECTPkts: 19788# ssroot@ubuntu:/home/feiyang# ss -sTotal: 958 (kernel 2224)TCP: 6 (estab 1, closed 0, orphaned 0, synrecv 0, timewait 0/0), ports 0Transport Total IP IPv6* 2224 - - RAW 0 0 0 UDP 6 4 2 TCP 6 4 2 INET 12 8 4 FRAG 0 0 0 # sarroot@ubuntu:/home/feiyang# sar -n DEV 1Linux 5.0.0-37-generic (ubuntu) 12/29/2019 _x86_64_ (2 CPU)08:42:20 PM IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s %ifutil08:42:21 PM ens33 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0008:42:21 PM lo 80.00 80.00 5.55 5.55 0.00 0.00 0.00 0.0008:42:21 PM IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s %ifutil08:42:22 PM ens33 34.00 34.00 1.99 3.25 0.00 0.00 0.00 0.0008:42:22 PM lo 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00# pingroot@ubuntu:/home/feiyang# ping -c3 114.114.114.114PING 114.114.114.114 (114.114.114.114) 56(84) bytes of data.64 bytes from 114.114.114.114: icmp_seq=1 ttl=128 time=222 ms64 bytes from 114.114.114.114: icmp_seq=2 ttl=128 time=221 ms64 bytes from 114.114.114.114: icmp_seq=3 ttl=128 time=222 msroot@ubuntu:/home/feiyang# ethtool ens33 | grep Speed Speed: 1000Mb/s C10K and C1000K select or poll Apache epoll Nginx Asynchronous I/O one master, multiple worker multi-process listen same port, need enable SO_REUSEPORT C1000K’s solution is essentially built on epoll’s non-blocking I / O model. C10M: To solve this problem, the most important thing is to skip the lengthy path of the kernel protocol stack and send the network packets directly to the application to be processed. There are two common mechanisms here, DPDK and XDP. DPDK is the standard for user mode networks. It skips the kernel protocol stack and directly processes the network reception by the user mode process through polling. XDP (eXpress Data Path) is a high-performance network data path provided by the Linux kernel. It allows network packets to be processed before entering the kernel protocol stack, which can also bring higher performance. The bottom layer of XDP, like the bcc-tools we used before, is implemented based on the eBPF mechanism of the Linux kernel. performance test123456789# enable pktgenroot@ubuntu:/home/feiyang# modprobe pktgenroot@ubuntu:/home/feiyang# ps -ef | grep pktgen | grep -v greproot 2478 2 0 22:25 ? 00:00:00 [kpktgend_0]root 2480 2 0 22:25 ? 00:00:00 [kpktgend_1]root 2481 2 0 22:25 ? 00:00:00 [kpktgend_0]root 2482 2 0 22:25 ? 00:00:00 [kpktgend_1]root@ubuntu:/home/feiyang# ls /proc/net/pktgen/kpktgend_0 kpktgend_1 pgctrl do some test 1234567891011121314151617181920212223242526272829303132333435363738394041# define functionfunction pgset() &#123; local result echo $1 &gt; $PGDEV result=`cat $PGDEV | fgrep "Result: OK:"` if [ "$result" = "" ]; then cat $PGDEV | fgrep Result: fi&#125; # bind 0 thread to eth0PGDEV=/proc/net/pktgen/kpktgend_0pgset "rem_device_all" # clear bindpgset "add_device eth0" # add eth0 # config eth0 PGDEV=/proc/net/pktgen/eth0pgset "count 1000000" # total send packagepgset "delay 5000" # interval pgset "clone_skb 0" # SKB package duplicatepgset "pkt_size 64" # package sizepgset "dst 192.168.0.30" # dest IPpgset "dst_mac 11:11:11:11:11:11" # dest MAC # start testPGDEV=/proc/net/pktgen/pgctrlpgset "start"cat /proc/net/pktgen/eth0Params: count 1000000 min_pkt_size: 64 max_pkt_size: 64 frags: 0 delay: 0 clone_skb: 0 ifname: eth0 flows: 0 flowlen: 0...Current: pkts-sofar: 1000000 errors: 0 started: 1534853256071us stopped: 1534861576098us idle: 70673us...Result: OK: 8320027(c8249354+d70673) usec, 1000000 (64byte,0frags) 120191pps 61Mb/sec (61537792bps) errors: 0 TCP test 123456789101112131415161718# Ubuntuapt-get install iperf3# CentOSyum install iperf3root@ubuntu:/home/feiyang# iperf3 -s -i 1 -p 10000-----------------------------------------------------------Server listening on 10000-----------------------------------------------------------iperf3 -c 192.168.0.30 -b 1G -t 15 -P 2 -p 10000[ ID] Interval Transfer Bandwidth...[SUM] 0.00-15.04 sec 0.00 Bytes 0.00 bits/sec sender[SUM] 0.00-15.04 sec 1.51 GBytes 860 Mbits/sec receiver HTTP test 123456789101112131415161718192021222324252627282930313233343536Ubuntuapt-get install -y apache2-utils# CentOSyum install -y httpd-toolsab -c 1000 -n 10000 http://192.168.0.30/...Server Software: nginx/1.15.8Server Hostname: 192.168.0.30Server Port: 80 ... Requests per second: 1078.54 [#/sec] (mean)Time per request: 927.183 [ms] (mean)Time per request: 0.927 [ms] (mean, across all concurrent requests)Transfer rate: 890.00 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median maxConnect: 0 27 152.1 1 1038Processing: 9 207 843.0 22 9242Waiting: 8 207 843.0 22 9242Total: 15 233 857.7 23 9268 Percentage of the requests served within a certain time (ms) 50% 23 66% 24 75% 24 80% 26 90% 274 95% 1195 98% 2335 99% 4663 100% 9268 (longest request) App test123456789101112131415161718192021222324252627282930313233343536373839git clone https://github.com/wg/wrkcd wrkapt-get install build-essential -ymakesudo cp wrk /usr/local/bin/wrk -c 1000 -t 2 http://192.168.0.30/Running 10s test @ http://192.168.0.30/ 2 threads and 1000 connections Thread Stats Avg Stdev Max +/- Stdev Latency 65.83ms 174.06ms 1.99s 95.85% Req/Sec 4.87k 628.73 6.78k 69.00% 96954 requests in 10.06s, 78.59MB read Socket errors: connect 0, read 0, write 0, timeout 179Requests/sec: 9641.31Transfer/sec: 7.82MB# setup add parameter-- example script that demonstrates response handling and-- retrieving an authentication token to set on all future-- requests token = nilpath = "/authenticate" request = function() return wrk.format("GET", path)end response = function(status, headers, body) if not token and status == 200 then token = headers["X-Token"] path = "/resource" wrk.headers["X-Token"] = token endendwrk -c 1000 -t 2 -s auth.lua http://192.168.0.30/ DNS slow A record, used to translate the domain name into an IP address; CNAME record for creating aliases; The NS record indicates the name server address corresponding to the domain name. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748root@ubuntu:/home/feiyang/wrk# dig +trace +nodnssec feiyang233.clubroot@ubuntu:/home/feiyang/wrk# nslookup feiyang233.clubServer: 1.1.1.1Address: 1.1.1.1#53Non-authoritative answer:feiyang233.club canonical name = fainyang.github.io.Name: fainyang.github.ioAddress: 185.199.109.153Name: fainyang.github.ioAddress: 185.199.110.153Name: fainyang.github.ioAddress: 185.199.111.153Name: fainyang.github.ioAddress: 185.199.108.153root@ubuntu:/home/feiyang/wrk# dig +trace +nodnssec feiyang233.club; &lt;&lt;&gt;&gt; DiG 9.11.3-1ubuntu1.11-Ubuntu &lt;&lt;&gt;&gt; +trace +nodnssec feiyang233.club;; global options: +cmd. 7974 IN NS i.root-servers.net.. 7974 IN NS j.root-servers.net.. 7974 IN NS k.root-servers.net.. 7974 IN NS l.root-servers.net.. 7974 IN NS m.root-servers.net.. 7974 IN NS a.root-servers.net.. 7974 IN NS b.root-servers.net.. 7974 IN NS c.root-servers.net.. 7974 IN NS d.root-servers.net.. 7974 IN NS e.root-servers.net.. 7974 IN NS f.root-servers.net.. 7974 IN NS g.root-servers.net.. 7974 IN NS h.root-servers.net.;; Received 431 bytes from 1.1.1.1#53(1.1.1.1) in 8 msclub. 172800 IN NS ns4.dns.nic.club.club. 172800 IN NS ns1.dns.nic.club.club. 172800 IN NS ns6.dns.nic.club.club. 172800 IN NS ns2.dns.nic.club.club. 172800 IN NS ns3.dns.nic.club.club. 172800 IN NS ns5.dns.nic.club.;; Received 456 bytes from 192.36.148.17#53(i.root-servers.net) in 104 msfeiyang233.club. 3600 IN NS f1g1ns1.dnspod.net.feiyang233.club. 3600 IN NS f1g1ns2.dnspod.net.;; Received 98 bytes from 156.154.144.215#53(ns1.dns.nic.club) in 189 ms;; Received 44 bytes from 182.140.167.166#53(f1g1ns1.dnspod.net) in 404 ms no /etc/resolv.conf 12345nslookup feiyang233.club;; connection timed out; no servers could be reached# add DNSecho "nameserver 1.1.1.1" &gt; /etc/resolv.conf DNS unstable 12345678910111213141516171819time nslookup time.geekbang.orgServer: 8.8.8.8Address: 8.8.8.8#53 Non-authoritative answer:Name: time.geekbang.orgAddress: 39.106.233.176 real 0m10.349suser 0m0.004ssys 0m0.0time nslookup time.geekbang.org;; connection timed out; no servers could be reached real 0m15.011suser 0m0.006ssys 0m0.006s The DNS server itself has problems, the response is slow and unstable; The network delay from the client to the DNS server is relatively large; DNS request or response packets are lost by network devices on the link in some cases. 123456789101112131415161718192021222324252627282930313233ping -c3 8.8.8.8PING 8.8.8.8 (8.8.8.8): 56 data bytes64 bytes from 8.8.8.8: icmp_seq=0 ttl=31 time=137.637 ms64 bytes from 8.8.8.8: icmp_seq=1 ttl=31 time=144.743 ms64 bytes from 8.8.8.8: icmp_seq=2 ttl=31 time=138.576 ms--- 8.8.8.8 ping statistics ---3 packets transmitted, 3 packets received, 0% packet lossround-trip min/avg/max/stddev = 137.637/140.319/144.743/3.152 ms# try other DNSping -c3 114.114.114.114PING 114.114.114.114 (114.114.114.114): 56 data bytes64 bytes from 114.114.114.114: icmp_seq=0 ttl=67 time=31.130 ms64 bytes from 114.114.114.114: icmp_seq=1 ttl=56 time=31.302 ms64 bytes from 114.114.114.114: icmp_seq=2 ttl=56 time=31.250 ms--- 114.114.114.114 ping statistics ---3 packets transmitted, 3 packets received, 0% packet lossround-trip min/avg/max/stddev = 31.130/31.227/31.302/0.072 msecho nameserver 114.114.114.114 &gt; /etc/resolv.conftime nslookup time.geekbang.orgServer: 114.114.114.114Address: 114.114.114.114#53 Non-authoritative answer:Name: time.geekbang.orgAddress: 39.106.233.176 real 0m0.064suser 0m0.007ssys 0m0.006s dump traffic123456789101112131415161718192021# Ubuntuapt-get install tcpdump wireshark # CentOSyum install -y tcpdump wiresharktcpdump -nn udp port 53 or host 35.190.27.188tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes14:02:31.100564 IP 172.16.3.4.56669 &gt; 114.114.114.114.53: 36909+ A? geektime.org. (30)14:02:31.507699 IP 114.114.114.114.53 &gt; 172.16.3.4.56669: 36909 1/0/0 A 35.190.27.188 (46)14:02:31.508164 IP 172.16.3.4 &gt; 35.190.27.188: ICMP echo request, id 4356, seq 1, length 6414:02:31.539667 IP 35.190.27.188 &gt; 172.16.3.4: ICMP echo reply, id 4356, seq 1, length 6414:02:31.539995 IP 172.16.3.4.60254 &gt; 114.114.114.114.53: 49932+ PTR? 188.27.190.35.in-addr.arpa. (44)14:02:36.545104 IP 172.16.3.4.60254 &gt; 114.114.114.114.53: 49932+ PTR? 188.27.190.35.in-addr.arpa. (44)14:02:41.551284 IP 172.16.3.4 &gt; 35.190.27.188: ICMP echo request, id 4356, seq 2, length 6414:02:41.582363 IP 35.190.27.188 &gt; 172.16.3.4: ICMP echo reply, id 4356, seq 2, length 6414:02:42.552506 IP 172.16.3.4 &gt; 35.190.27.188: ICMP echo request, id 4356, seq 3, length 6414:02:42.583646 IP 35.190.27.188 &gt; 172.16.3.4: ICMP echo reply, id 4356, seq 3, length 64 The purpose of PTR reverse address resolution is to find out the domain name from the IP address, but in fact, not all IP addresses will define PTR records, so PTR queries are likely to fail. 12345678910111213141516# check PTRnslookup -type=PTR 35.190.27.188 8.8.8.8Server: 8.8.8.8Address: 8.8.8.8#53Non-authoritative answer:188.27.190.35.in-addr.arpa name = 188.27.190.35.bc.googleusercontent.com.Authoritative answers can be found from:# testdig +short example.com93.184.216.34tcpdump -nn host 93.184.216.34 -w web.pcap# in another ttycurl http://example.com 123#tcpdump output formatTimestamp Protocol Source Address Source Port&gt; Destination Address Destination Port Network Packet Details时间戳 协议 源地址 源端口 &gt; 目的地址 目的端口 网络包详细信息 anti-DDoSDDoS（Distributed Denial of Service） Running out of bandwidth Running out of operating system resources Running out of application resources 123456789101112131415161718192021222324252627282930# -S set TCP SYN，-p port 80# -i u10 10us intervalhping3 -S -p 80 -i u10 192.168.0.30# -w only output HTTP status total time -o redirect to /dev/nullcurl -s -w 'Http code: %&#123;http_code&#125;\nTotal time:%&#123;time_total&#125;s\n' -o /dev/null http://192.168.0.30/...Http code: 200Total time:0.002ssar -n DEV 108:55:49 IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s %ifutil08:55:50 docker0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0008:55:50 eth0 22274.00 629.00 1174.64 37.78 0.00 0.00 0.00 0.0208:55:50 lo 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00# small package 54B （1174*1024/22274=54）# -i eth0 nic，-n Don't convert addresses (i.e., host addresses, port numbers, etc.) to names.# tcp port 80 $ tcpdump -i eth0 -n tcp port 8009:15:48.287047 IP 192.168.0.2.27095 &gt; 192.168.0.30: Flags [S], seq 1288268370, win 512, length 009:15:48.287050 IP 192.168.0.2.27131 &gt; 192.168.0.30: Flags [S], seq 2084255254, win 512, length 009:15:48.287052 IP 192.168.0.2.27116 &gt; 192.168.0.30: Flags [S], seq 677393791, win 512, length 009:15:48.287055 IP 192.168.0.2.27141 &gt; 192.168.0.30: Flags [S], seq 1276451587, win 512, length 009:15:48.287068 IP 192.168.0.2.27154 &gt; 192.168.0.30: Flags [S], seq 1851495339, win 512, length 0... 1234567891011121314151617181920212223242526272829303132333435363738# check SYN_RECnetstat -n -p | grep SYN_RECtcp 0 0 192.168.0.30:80 192.168.0.2:12503 SYN_RECV -tcp 0 0 192.168.0.30:80 192.168.0.2:13502 SYN_RECV -tcp 0 0 192.168.0.30:80 192.168.0.2:15256 SYN_RECV -tcp 0 0 192.168.0.30:80 192.168.0.2:18117 SYN_RECV -...netstat -n -p | grep SYN_REC | wc -l193# method for SYN DDoS# limit SYN 1/s$ iptables -A INPUT -p tcp --syn -m limit --limit 1/s -j ACCEPT # limit every IP only establish 10 connections during 60s$ iptables -I INPUT -p tcp --dport 80 --syn -m recent --name SYN_FLOOD --update --seconds 60 --hitcount 10 -j REJECTsysctl net.ipv4.tcp_max_syn_backlognet.ipv4.tcp_max_syn_backlog = 256# increase maxsysctl -w net.ipv4.tcp_max_syn_backlog=1024net.ipv4.tcp_max_syn_backlog = 1024# decrease retry time from default 5 to 1sysctl -w net.ipv4.tcp_synack_retries=1net.ipv4.tcp_synack_retries = 1# enable TCP SYN Cookiessysctl -w net.ipv4.tcp_syncookies=1net.ipv4.tcp_syncookies = 1# above set is temporary, for persistentcat /etc/sysctl.confnet.ipv4.tcp_syncookies = 1net.ipv4.tcp_synack_retries = 1net.ipv4.tcp_max_syn_backlog = 1024 In a Linux server, you can increase the anti-attack capability of the server and reduce the impact of DDoS on normal services through various methods such as kernel tuning, DPDK, and XDP. In the application, you can use various levels of caching, WAF, CDN and other methods to mitigate the impact of DDoS on the application. network slow123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106traceroute --tcp -p 80 -n baidu.comtraceroute to baidu.com (123.125.115.110), 30 hops max, 60 byte packets 1 * * * 2 * * * 3 * * * 4 * * * 5 * * * 6 * * * 7 * * * 8 * * * 9 * * *10 * * *11 * * *12 * * *13 * * *14 123.125.115.110 20.684 ms * 20.798 ms# install wrkgit clone https://github.com/wg/wrkcd wrkapt-get install build-essential -ymakesudo cp wrk /usr/local/bin/# create 2 nginxdocker run --network=host --name=good -itd nginxdocker run --name nginx --network=host -itd feisky/nginx:latency# curl port 80 and 8080# 80 okcurl http://192.168.0.30&lt;!DOCTYPE html&gt;&lt;html&gt;...&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; # 8080 okcurl http://192.168.0.30:8080...&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;# 80 ok hping3 -c 3 -S -p 80 192.168.0.30HPING 192.168.0.30 (eth0 192.168.0.30): S set, 40 headers + 0 data byteslen=44 ip=192.168.0.30 ttl=64 DF id=0 sport=80 flags=SA seq=0 win=29200 rtt=7.8 mslen=44 ip=192.168.0.30 ttl=64 DF id=0 sport=80 flags=SA seq=1 win=29200 rtt=7.7 mslen=44 ip=192.168.0.30 ttl=64 DF id=0 sport=80 flags=SA seq=2 win=29200 rtt=7.6 ms --- 192.168.0.30 hping statistic ---3 packets transmitted, 3 packets received, 0% packet lossround-trip min/avg/max = 7.6/7.7/7.8 ms# 8080 okhping3 -c 3 -S -p 8080 192.168.0.30HPING 192.168.0.30 (eth0 192.168.0.30): S set, 40 headers + 0 data byteslen=44 ip=192.168.0.30 ttl=64 DF id=0 sport=8080 flags=SA seq=0 win=29200 rtt=7.7 mslen=44 ip=192.168.0.30 ttl=64 DF id=0 sport=8080 flags=SA seq=1 win=29200 rtt=7.6 mslen=44 ip=192.168.0.30 ttl=64 DF id=0 sport=8080 flags=SA seq=2 win=29200 rtt=7.3 ms --- 192.168.0.30 hping statistic ---3 packets transmitted, 3 packets received, 0% packet lossround-trip min/avg/max = 7.3/7.6/7.7 ms# test 80 测试 80 端口性能$ # wrk --latency -c 100 -t 2 --timeout 2 http://192.168.0.30/Running 10s test @ http://192.168.0.30/ 2 threads and 100 connections Thread Stats Avg Stdev Max +/- Stdev Latency 9.19ms 12.32ms 319.61ms 97.80% Req/Sec 6.20k 426.80 8.25k 85.50% Latency Distribution 50% 7.78ms 75% 8.22ms 90% 9.14ms 99% 50.53ms 123558 requests in 10.01s, 100.15MB readRequests/sec: 12340.91Transfer/sec: 10.00MB# test 8080, this nginx is very slowwrk --latency -c 100 -t 2 --timeout 2 http://192.168.0.30:8080/Running 10s test @ http://192.168.0.30:8080/ 2 threads and 100 connections Thread Stats Avg Stdev Max +/- Stdev Latency 43.60ms 6.41ms 56.58ms 97.06% Req/Sec 1.15k 120.29 1.92k 88.50% Latency Distribution 50% 44.02ms 75% 44.33ms 90% 47.62ms 99% 48.88ms 22853 requests in 10.01s, 18.55MB readRequests/sec: 2283.31Transfer/sec: 1.85MB# dump the traffictcpdump -nn tcp port 8080 -w nginx.pcapwrk --latency -c 100 -t 2 --timeout 2 http://192.168.0.30:8080/ Open this nginx.pcap in Wireshark, Statics -&gt; Flow Graph，select “Limit to display filter” and setup Flow type to “TCP Flows”：Blue area is very slow costs 40ms, 40ms is minimum timeout for TCP delayed acknowledgement (Delayed ACK).An optimization mechanism for TCP ACK, that is, instead of sending an ACK for each request, you wait for a while (such as 40ms). If there are other packets that need to be sent during this period, send them with the ACK. Of course, if you can’t wait for other packets, then send ACK separately after timeout. 1234567# TCP_QUICKACK (since Linux 2.4.4)# Enable quickack mode if set or disable quickack mode if cleared. In quickack mode, # acks are sent imme‐diately, rather than delayed if needed in accordance to normal TCP operation.# This flag is not perma‐nent, it only enables a switch to or from quickack mode. # Subsequent operation of the TCP protocol will once again enter/leave quickack mode# depending on internal protocol processing and factors such as delayed ack timeouts occurring # and data transfer. This option should not be used in code intended to be portable. The Nagle algorithm is an optimization algorithm used in the TCP protocol to reduce the number of small packets sent, in order to improve the utilization of the actual bandwidth. The Nagle algorithm specifies that there can be at most one unconfirmed outstanding packet on a TCP connection; no other packets are sent until an ACK for this packet is received. 1234567891011#TCP_NODELAY# If set, disable the Nagle algorithm. This means that segments are always sent as soon as possible,# even if there is only a small amount of data. When not set, data is buffered until there is a # sufficient amount to send out, thereby avoiding the frequent sending of small packets, # which results in poor uti‐lization of the network. # This option is overridden by TCP_CORK; however, setting this option forces an explicit flush of# pending output, even if TCP_CORK is currently set.# check tcp_nodelaycat /etc/nginx/nginx.conf | grep tcp_nodelay tcp_nodelay off; NATNetwork Address and Port Translation123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475# SNAT# MASQUERADE, change out ip to Linux wan ipiptables -t nat -A POSTROUTING -s 192.168.0.0/16 -j MASQUERADEiptables -t nat -A POSTROUTING -s 192.168.0.2 -j SNAT --to-source 100.100.100.100# DNATiptables -t nat -A PREROUTING -d 100.100.100.100 -j DNAT --to-destination 192.168.0.2iptables -t nat -A POSTROUTING -s 192.168.0.2 -j SNAT --to-source 100.100.100.100iptables -t nat -A PREROUTING -d 100.100.100.100 -j DNAT --to-destination 192.168.0.2# enable forward function# sysctl net.ipv4.ip_forward# net.ipv4.ip_forward = 1sysctl -w net.ipv4.ip_forward=1# open filesulimit -n1024# increase to 66535ulimit -n 65536ab -c 5000 -n 100000 -r -s 2 http://192.168.0.30/...Requests per second: 6576.21 [#/sec] (mean)Time per request: 760.317 [ms] (mean)Time per request: 0.152 [ms] (mean, across all concurrent requests)Transfer rate: 5390.19 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median maxConnect: 0 177 714.3 9 7338Processing: 0 27 39.8 19 961Waiting: 0 23 39.5 16 951Total: 1 204 716.3 28 7349# run new test containerdocker run --name nginx --privileged -p 8080:8080 -itd feisky/nginx:natiptables -nL -t natChain PREROUTING (policy ACCEPT)target prot opt source destinationDOCKER all -- 0.0.0.0/0 0.0.0.0/0 ADDRTYPE match dst-type LOCAL ... Chain DOCKER (2 references)target prot opt source destinationRETURN all -- 0.0.0.0/0 0.0.0.0/0DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:8080 to:172.17.0.2:8080# test againab -c 5000 -n 100000 -r -s 2 http://192.168.0.30:8080/...apr_pollset_poll: The timeout specified has expired (70007)Total of 5602 requests completed# set timeout is 30sab -c 5000 -n 10000 -r -s 30 http://192.168.0.30:8080/...Requests per second: 76.47 [#/sec] (mean)Time per request: 65380.868 [ms] (mean)Time per request: 13.076 [ms] (mean, across all concurrent requests)Transfer rate: 44.79 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median maxConnect: 0 1300 5578.0 1 65184Processing: 0 37916 59283.2 1 130682Waiting: 0 2 8.7 1 414Total: 1 39216 58711.6 1021 130682... we create a script to follow this iptable path1234567891011121314151617181920212223242526272829#! /usr/bin/env stap ############################################################# Dropwatch.stp# Author: Neil Horman &lt;nhorman@redhat.com&gt;# An example script to mimic the behavior of the dropwatch utility# http://fedorahosted.org/dropwatch############################################################ # Array to hold the list of drop points we findglobal locations # Note when we turn the monitor on and offprobe begin &#123; printf("Monitoring for dropped packets\n") &#125;probe end &#123; printf("Stopping dropped packet monitor\n") &#125; # increment a drop counter for every location we drop atprobe kernel.trace("kfree_skb") &#123; locations[$location] &lt;&lt;&lt; 1 &#125; # Every 5 seconds report our drop locationsprobe timer.sec(5)&#123; printf("\n") foreach (l in locations-) &#123; printf("%d packets dropped at %s\n", @count(locations[l]), symname(l)) &#125; delete locations&#125; run this script1234567891011121314151617stap --all-modules dropwatch.stpMonitoring for dropped packets10031 packets dropped at nf_hook_slow676 packets dropped at tcp_v4_rcv 7284 packets dropped at nf_hook_slow268 packets dropped at tcp_v4_rcv# use perf to check # record 30s crtl + c$ perf record -a -g -- sleep 30 # print report$ perf report -g graph,0 Slow in 3 point ipv4_conntrack_in br_nf_pre_routing iptable_nat_ipv4_in 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# check conntracksysctl -a | grep conntracknet.netfilter.nf_conntrack_count = 180net.netfilter.nf_conntrack_max = 1000net.netfilter.nf_conntrack_buckets = 65536net.netfilter.nf_conntrack_tcp_timeout_syn_recv = 60net.netfilter.nf_conntrack_tcp_timeout_syn_sent = 120net.netfilter.nf_conntrack_tcp_timeout_time_wait = 120...# net.netfilter.nf_conntrack_max = 1000 is to smalldmesg | tail[104235.156774] nf_conntrack: nf_conntrack: table full, dropping packet[104243.800401] net_ratelimit: 3939 callbacks suppressed[104243.800401] nf_conntrack: nf_conntrack: table full, dropping packet[104262.962157] nf_conntrack: nf_conntrack: table full, dropping packetThe connection tracking object size is 376, and the list item size is 16nf_conntrack_max * connection tracking object size + nf_conntrack_buckets * list item size= 1000 * 376 + 65536 * 16 B= 1.4 MB# increase conntracksysctl -w net.netfilter.nf_conntrack_max=131072sysctl -w net.netfilter.nf_conntrack_buckets=65536# test again, now delay is okab -c 5000 -n 100000 -r -s 2 http://192.168.0.30:8080/...Requests per second: 6315.99 [#/sec] (mean)Time per request: 791.641 [ms] (mean)Time per request: 0.158 [ms] (mean, across all concurrent requests)Transfer rate: 4985.15 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median maxConnect: 0 355 793.7 29 7352Processing: 8 311 855.9 51 14481Waiting: 0 292 851.5 36 14481Total: 15 666 1216.3 148 14645conntrack -L -o extended | headipv4 2 tcp 6 7 TIME_WAIT src=192.168.0.2 dst=192.168.0.96 sport=51744 dport=8080 src=172.17.0.2 dst=192.168.0.2 sport=8080 dport=51744 [ASSURED] mark=0 use=1ipv4 2 tcp 6 6 TIME_WAIT src=192.168.0.2 dst=192.168.0.96 sport=51524 dport=8080 src=172.17.0.2 dst=192.168.0.2 sport=8080 dport=51524 [ASSURED] mark=0 use=1conntrack -L -o extended | wc -l14289# collecte all statesconntrack -L -o extended | awk '/^.*tcp.*$/ &#123;sum[$6]++&#125; END &#123;for(i in sum) print i, sum[i]&#125;'SYN_RECV 4CLOSE_WAIT 9ESTABLISHED 2877FIN_WAIT 3SYN_SENT 2113TIME_WAIT 9283# collect each source IPconntrack -L -o extended | awk '&#123;print $7&#125;' | cut -d "=" -f 2 | sort | uniq -c | sort -nr | head -n 10 14116 192.168.0.2 172 192.168.0.96 important123# tcp time_wait settings check sysctl net.netfilter.nf_conntrack_tcp_timeout_time_waitnet.netfilter.nf_conntrack_tcp_timeout_time_wait = 120 summaryAfter setting TCP_NODELAY for the TCP connection, you can disable the Nagle algorithm; After TCP_CORK is enabled for a TCP connection, small packets can be aggregated into large packets before being sent (note that it will block the sending of small packets); With SO_SNDBUF and SO_RCVBUF, you can adjust the size of the socket send buffer and receive buffer, respectively.The three values of tcp_rmem and tcp_wmem are min, default, and max respectively. The system will automatically adjust the size of the TCP receive / send buffer according to these settings. The three values of udp_mem are min, pressure, max. The system will automatically adjust the size of the UDP send buffer according to these settings. Best Practicedocker12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091# check running dockerdocker ps# check logsdocker logs -f [container_id]# check container config in json formatdocker inspect [container_id] -f '&#123;&#123;json .State&#125;&#125;' | jq&#123; "Status": "exited", "Running": false, "Paused": false, "Restarting": false, "OOMKilled": true, "Dead": false, "Pid": 0, "ExitCode": 137, "Error": "", ...&#125;# OMM Kill, we can check in hostdmesg[193038.106393] java invoked oom-killer: gfp_mask=0x14000c0(GFP_KERNEL), nodemask=(null), order=0, oom_score_adj=0[193038.106396] java cpuset=0f2b3fcdd2578165ea77266cdc7b1ad43e75877b0ac1889ecda30a78cb78bd53 mems_allowed=0[193038.106402] CPU: 0 PID: 27424 Comm: java Tainted: G OE 4.15.0-1037 #39-Ubuntu[193038.106404] Hardware name: Microsoft Corporation Virtual Machine/Virtual Machine, BIOS 090007 06/02/2017[193038.106405] Call Trace:[193038.106414] dump_stack+0x63/0x89[193038.106419] dump_header+0x71/0x285[193038.106422] oom_kill_process+0x220/0x440[193038.106424] out_of_memory+0x2d1/0x4f0[193038.106429] mem_cgroup_out_of_memory+0x4b/0x80[193038.106432] mem_cgroup_oom_synchronize+0x2e8/0x320[193038.106435] ? mem_cgroup_css_online+0x40/0x40[193038.106437] pagefault_out_of_memory+0x36/0x7b[193038.106443] mm_fault_error+0x90/0x180[193038.106445] __do_page_fault+0x4a5/0x4d0[193038.106448] do_page_fault+0x2e/0xe0[193038.106454] ? page_fault+0x2f/0x50[193038.106456] page_fault+0x45/0x50[193038.106459] RIP: 0033:0x7fa053e5a20d[193038.106460] RSP: 002b:00007fa0060159e8 EFLAGS: 00010206[193038.106462] RAX: 0000000000000000 RBX: 00007fa04c4b3000 RCX: 0000000009187440[193038.106463] RDX: 00000000943aa440 RSI: 0000000000000000 RDI: 000000009b223000[193038.106464] RBP: 00007fa006015a60 R08: 0000000002000002 R09: 00007fa053d0a8a1[193038.106465] R10: 00007fa04c018b80 R11: 0000000000000206 R12: 0000000100000768[193038.106466] R13: 00007fa04c4b3000 R14: 0000000100000768 R15: 0000000010000000[193038.106468] Task in /docker/0f2b3fcdd2578165ea77266cdc7b1ad43e75877b0ac1889ecda30a78cb78bd53killed as a result of limit of /docker/0f2b3fcdd2578165ea77266cdc7b1ad43e75877b0ac1889ecda30a78cb78bd53[193038.106478] memory: usage 524288kB, limit 524288kB, failcnt 77[193038.106480] memory+swap: usage 0kB, limit 9007199254740988kB, failcnt 0[193038.106481] kmem: usage 3708kB, limit 9007199254740988kB, failcnt 0[193038.106481] Memory cgroup stats for /docker/0f2b3fcdd2578165ea77266cdc7b1ad43e75877b0ac1889ecda30a78cb78bd53: cache:0KB rss:520580KB rss_huge:450560KB shmem:0KB mapped_file:0KB dirty:0KB writeback:0KB inactive_anon:0KB active_anon:520580KB inactive_file:0KB active_file:0KB unevictable:0KB[193038.106494] [ pid ] uid tgid total_vm rss pgtables_bytes swapents oom_score_adj name[193038.106571] [27281] 0 27281 1153302 134371 1466368 0 0 java[193038.106574] Memory cgroup out of memory: Kill process 27281 (java) score 1027 or sacrifice child[193038.148334] Killed process 27281 (java) total-vm:4613208kB, anon-rss:517316kB, file-rss:20168kB, shmem-rss:0kB[193039.607503] oom_reaper: reaped process 27281 (java), now anon-rss:0kB, file-rss:0kB, shmem-rss:0kB# add memory limitdocker run --name tomcat --cpus 0.1 -m 512M -p 8080:8080 -itd feisky/tomcat:8docker exec tomcat java -XX:+PrintFlagsFinal -version | grep HeapSizeuintx ErgoHeapSizeLimit = 0 &#123;product&#125;uintx HeapSizePerGCThread = 87241520 &#123;product&#125;uintx InitialHeapSize := 132120576 &#123;product&#125;uintx LargePageHeapSizeThreshold = 134217728 &#123;product&#125;uintx MaxHeapSize := 2092957696 # set env to limit memorydocker run --name tomcat --cpus 0.1 -m 512M -e JAVA_OPTS='-Xmx512m -Xms512m' -p 8080:8080 -itd feisky/tomcat:8# check thread PID=$(docker inspect [container_id] -f '&#123;&#123;.State.Pid&#125;&#125;')# run pidstat$ pidstat -t -p $PID 112:59:28 UID TGID TID %usr %system %guest %wait %CPU CPU Command12:59:29 0 29850 - 10.00 0.00 0.00 0.00 10.00 0 java12:59:29 0 - 29850 0.00 0.00 0.00 0.00 0.00 0 |__java12:59:29 0 - 29897 5.00 1.00 0.00 86.00 6.00 1 |__java...12:59:29 0 - 29905 3.00 0.00 0.00 97.00 3.00 0 |__java12:59:29 0 - 29906 2.00 0.00 0.00 49.00 2.00 1 |__java12:59:29 0 - 29908 0.00 0.00 0.00 45.00 0.00 0 |__java# io_wait is high packet loss1234567891011121314151617181920212223242526# check max connectionsysctl net.netfilter.nf_conntrack_maxnet.netfilter.nf_conntrack_max = 262144sysctl net.netfilter.nf_conntrack_countnet.netfilter.nf_conntrack_count = 182# execute in containerroot@nginx:/ iptables -t filter -nvLChain INPUT (policy ACCEPT 25 packets, 1000 bytes) pkts bytes target prot opt in out source destination 6 240 DROP all -- * * 0.0.0.0/0 0.0.0.0/0 statistic mode random probability 0.29999999981 Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 15 packets, 660 bytes) pkts bytes target prot opt in out source destination 6 264 DROP all -- * * 0.0.0.0/0 0.0.0.0/0 statistic mode random probability 0.29999999981# check MTUnetstat -iKernel Interface tableIface MTU RX-OK RX-ERR RX-DRP RX-OVR TX-OK TX-ERR TX-DRP TX-OVR Flgeth0 100 157 0 344 0 94 0 0 0 BMRUlo 65536 0 0 0 0 0 0 0 0 LRU During Linux startup, there are three special processes, that is, the three processes with the smallest PID numbers. Process 0 is an idle process. This is also the first process created by the system. After initializing processes 1 and 2, it becomes an idle task. It runs when no other tasks are executing on the CPU. Process 1 is the init process, which is usually the systemd process. It runs in user mode and is used to manage other user mode processes. Process 2 is a kthreadd process, which runs in kernel mode and is used to manage kernel threads. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758ps -f --ppid 2 -p 2UID PID PPID C STIME TTY TIME CMDroot 2 0 0 12:02 ? 00:00:01 [kthreadd]root 9 2 0 12:02 ? 00:00:21 [ksoftirqd/0]root 10 2 0 12:02 ? 00:11:47 [rcu_sched]root 11 2 0 12:02 ? 00:00:18 [migration/0]...root 11094 2 0 14:20 ? 00:00:00 [kworker/1:0-eve]root 11647 2 0 14:27 ? 00:00:00 [kworker/0:2-cgr]# Kernel thread names (CMD) are in square brackets []ps -ef | grep "\[.*\]"root 2 0 0 08:14 ? 00:00:00 [kthreadd]root 3 2 0 08:14 ? 00:00:00 [rcu_gp]root 4 2 0 08:14 ? 00:00:00 [rcu_par_gp]...# test -S: SYN -p port, -i interval 10ushping3 -S -p 80 -i u10 192.168.0.30# toptoptop - 08:31:43 up 17 min, 1 user, load average: 0.00, 0.00, 0.02Tasks: 128 total, 1 running, 69 sleeping, 0 stopped, 0 zombie%Cpu0 : 0.3 us, 0.3 sy, 0.0 ni, 66.8 id, 0.3 wa, 0.0 hi, 32.4 si, 0.0 st%Cpu1 : 0.0 us, 0.3 sy, 0.0 ni, 65.2 id, 0.0 wa, 0.0 hi, 34.5 si, 0.0 stKiB Mem : 8167040 total, 7234236 free, 358976 used, 573828 buff/cacheKiB Swap: 0 total, 0 free, 0 used. 7560460 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 9 root 20 0 0 0 0 S 7.0 0.0 0:00.48 ksoftirqd/0 18 root 20 0 0 0 0 S 6.9 0.0 0:00.56 ksoftirqd/1 2489 root 20 0 876896 38408 21520 S 0.3 0.5 0:01.50 docker-containe 3008 root 20 0 44536 3936 3304 R 0.3 0.0 0:00.09 top 1 root 20 0 78116 9000 6432 S 0.0 0.1 0:11.77 systemd# check ksoftirqd pid is 9pstack 9Could not attach to target 9: Operation not permitted.detach: No such processcat /proc/9/stack[&lt;0&gt;] smpboot_thread_fn+0x166/0x170[&lt;0&gt;] kthread+0x121/0x140[&lt;0&gt;] ret_from_fork+0x35/0x40[&lt;0&gt;] 0xffffffffffffffff# perf perf record -a -g -p 9 -- sleep 30perf report# install flamegraphgit clone https://github.com/brendangregg/FlameGraphcd FlameGraphperf script -i /root/perf.data | ./stackcollapse-perf.pl --all | ./flamegraph.pl &gt; ksoftirqd.svg Dynamic Tracing12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576cd /sys/kernel/debug/tracing# if not exist this directorymount -t debugfs nodev /sys/kernel/debugcat available_tracershwlat blk mmiotrace function_graph wakeup_dl wakeup_rt wakeup function nopcat available_filter_functionscat available_events# testecho do_sys_open &gt; set_graph_functionecho function_graph &gt; current_tracerecho funcgraph-proc &gt; trace_options# enable traceecho 1 &gt; tracing_onls# disable traceecho 0 &gt; tracing_oncat trace# tracer: function_graph## CPU TASK/PID DURATION FUNCTION CALLS# | | | | | | | | | 0) ls-12276 | | do_sys_open() &#123; 0) ls-12276 | | getname() &#123; 0) ls-12276 | | getname_flags() &#123; 0) ls-12276 | | kmem_cache_alloc() &#123; 0) ls-12276 | | _cond_resched() &#123; 0) ls-12276 | 0.049 us | rcu_all_qs(); 0) ls-12276 | 0.791 us | &#125; 0) ls-12276 | 0.041 us | should_failslab(); 0) ls-12276 | 0.040 us | prefetch_freepointer(); 0) ls-12276 | 0.039 us | memcg_kmem_put_cache(); 0) ls-12276 | 2.895 us | &#125; 0) ls-12276 | | __check_object_size() &#123; 0) ls-12276 | 0.067 us | __virt_addr_valid(); 0) ls-12276 | 0.044 us | __check_heap_object(); 0) ls-12276 | 0.039 us | check_stack_object(); 0) ls-12276 | 1.570 us | &#125; 0) ls-12276 | 5.790 us | &#125; 0) ls-12276 | 6.325 us | &#125;...# Ubuntuapt-get install trace-cmd# CentOSyum install trace-cmd# above 5 step simplify to follow 1 steptrace-cmd record -p function_graph -g do_sys_open -O funcgraph-proc ls$ trace-cmd report... ls-12418 [000] 85558.075341: funcgraph_entry: | do_sys_open() &#123; ls-12418 [000] 85558.075363: funcgraph_entry: | getname() &#123; ls-12418 [000] 85558.075364: funcgraph_entry: | getname_flags() &#123; ls-12418 [000] 85558.075364: funcgraph_entry: | kmem_cache_alloc() &#123; ls-12418 [000] 85558.075365: funcgraph_entry: | _cond_resched() &#123; ls-12418 [000] 85558.075365: funcgraph_entry: 0.074 us | rcu_all_qs(); ls-12418 [000] 85558.075366: funcgraph_exit: 1.143 us | &#125; ls-12418 [000] 85558.075366: funcgraph_entry: 0.064 us | should_failslab(); ls-12418 [000] 85558.075367: funcgraph_entry: 0.075 us | prefetch_freepointer(); ls-12418 [000] 85558.075368: funcgraph_entry: 0.085 us | memcg_kmem_put_cache(); ls-12418 [000] 85558.075369: funcgraph_exit: 4.447 us | &#125; ls-12418 [000] 85558.075369: funcgraph_entry: | __check_object_size() &#123; ls-12418 [000] 85558.075370: funcgraph_entry: 0.132 us | __virt_addr_valid(); ls-12418 [000] 85558.075370: funcgraph_entry: 0.093 us | __check_heap_object(); ls-12418 [000] 85558.075371: funcgraph_entry: 0.059 us | check_stack_object(); ls-12418 [000] 85558.075372: funcgraph_exit: 2.323 us | &#125; ls-12418 [000] 85558.075372: funcgraph_exit: 8.411 us | &#125; ls-12418 [000] 85558.075373: funcgraph_exit: 9.195 us | &#125;... 12345678910111213141516171819202122232425262728293031323334perf probe --add do_sys_openAdded new event: probe:do_sys_open (on do_sys_open)You can now use it in all perf tools, such as: perf record -e probe:do_sys_open -aR sleep 1perf probe -V do_sys_openAvailable variables at do_sys_open @&lt;do_sys_open+0&gt; char* filename int dfd int flags struct open_flags op umode_t mode# delete probeperf probe --del probe:do_sys_openperf probe --add 'do_sys_open filename:string'Added new event: probe:do_sys_open (on do_sys_open with filename:string)You can now use it in all perf tools, such as: perf record -e probe:do_sys_open -aR sleep 1# runperf record -e probe:do_sys_open -aR ls# resultperf script perf 13593 [000] 91846.053622: probe:do_sys_open: (ffffffffa807b290) filename_string="/proc/13596/status" ls 13596 [000] 91846.053995: probe:do_sys_open: (ffffffffa807b290) filename_string="/etc/ld.so.cache" ls 13596 [000] 91846.054011: probe:do_sys_open: (ffffffffa807b290) filename_string="/lib/x86_64-linux-gnu/libselinux.so.1" ls 13596 [000] 91846.054066: probe:do_sys_open: (ffffffffa807b290) filename_string="/lib/x86_64-linux-gnu/libc.so.6” 1234567891011121314151617181920212223242526# delete probe before leaveperf probe --del probe:do_sys_open# starce is based on ptracestrace ls...access("/etc/ld.so.nohwcap", F_OK) = -1 ENOENT (No such file or directory)access("/etc/ld.so.preload", R_OK) = -1 ENOENT (No such file or directory)openat(AT_FDCWD, "/etc/ld.so.cache", O_RDONLY|O_CLOEXEC) = 3...access("/etc/ld.so.nohwcap", F_OK) = -1 ENOENT (No such file or directory)openat(AT_FDCWD, "/lib/x86_64-linux-gnu/libselinux.so.1", O_RDONLY|O_CLOEXEC) = 3...# perf trace is lightperf trace ls ? ( ): ls/14234 ... [continued]: execve()) = 0 0.177 ( 0.013 ms): ls/14234 brk( ) = 0x555d96be7000 0.224 ( 0.014 ms): ls/14234 access(filename: 0xad98082 ) = -1 ENOENT No such file or directory 0.248 ( 0.009 ms): ls/14234 access(filename: 0xad9add0, mode: R ) = -1 ENOENT No such file or directory 0.267 ( 0.012 ms): ls/14234 openat(dfd: CWD, filename: 0xad98428, flags: CLOEXEC ) = 3 0.288 ( 0.009 ms): ls/14234 fstat(fd: 3&lt;/usr/lib/locale/C.UTF-8/LC_NAME&gt;, statbuf: 0x7ffd2015f230 ) = 0 0.305 ( 0.011 ms): ls/14234 mmap(len: 45560, prot: READ, flags: PRIVATE, fd: 3 ) = 0x7efe0af92000 0.324 Dockerfile test.sh( 0.008 ms): ls/14234 close(fd: 3&lt;/usr/lib/locale/C.UTF-8/LC_NAME&gt; ) = 0 ... socket1234567891011121314151617181920212223242526netstat -s | grep socket 73 resets received for embryonic SYN_RECV sockets 308582 TCP sockets finished time wait in fast timer 8 delayed acks further delayed because of locked socket 290566 times the listen queue of a socket overflowed 290566 SYNs to LISTEN sockets droppedss -ltnpState Recv-Q Send-Q Local Address:Port Peer Address:PortLISTEN 10 10 0.0.0.0:80 0.0.0.0:* users:(("nginx",pid=10491,fd=6),("nginx",pid=10490,fd=6),("nginx",pid=10487,fd=6))LISTEN 7 10 *:9000 *:* users:(("php-fpm",pid=11084,fd=9),...,("php-fpm",pid=10529,fd=7))sysctl net.ipv4.ip_local_port_rangenet.ipv4.ip_local_port_range=20000 20050sysctl -w net.ipv4.ip_local_port_range="10000 65535"net.ipv4.ip_local_port_range = 10000 65535# timewait is still use port, can decrase timewait time or port reusess -sTCP: 32775 (estab 1, closed 32768, orphaned 0, synrecv 0, timewait 32768/0), ports 0sysctl net.ipv4.tcp_tw_reusenet.ipv4.tcp_tw_reuse = 0... monitor learn kernel kernel eBPFAdditions]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据学习笔记]]></title>
    <url>%2Fpost%2Fbigdata%2F</url>
    <content type="text"><![CDATA[大数据技术主要是要解决大规模数据的计算处理问题，但是我们要想对数据进行计算，首先要解决的其实是大规模数据的存储问题。 RAID大规模数据存储都需要解决 3 个核心问题: 1.数据存储容量的问题 2.数据读写速度的问题 3.数据可靠性的问题 RAID（独立磁盘冗余阵列）技术是将多块普通磁盘组成一个阵列，共同对外提供服务。主要是为了改善磁盘的存储容量、读写速度，增强磁盘的可用性和容错能力。目前服务器级别的计算机都支持插入多块磁盘（8 块或者更多），通过使用 RAID 技术，实现数据在多块磁盘上的并发读写和数据备份。首先，我们先假设服务器有 N 块磁盘，RAID 0是数据在从内存缓冲区写入磁盘时，根据磁盘数量将数据分成 N 份，这些数据同时并发写入 N 块磁盘，使得数据整体写入速度是一块磁盘的 N 倍；读取的时候也一样，因此 RAID 0 具有极快的数据读写速度。但是 RAID 0 不做数据备份，N 块磁盘中只要有一块损坏，数据完整性就被破坏，其他磁盘的数据也都无法使用了。 RAID 1是数据在写入磁盘时，将一份数据同时写入两块磁盘，这样任何一块磁盘损坏都不会导致数据丢失，插入一块新磁盘就可以通过复制数据的方式自动修复，具有极高的可靠性。 结合 RAID 0 和 RAID 1 两种方案构成了RAID 10，它是将所有磁盘 N 平均分成两份，数据同时在两份磁盘写入，相当于 RAID 1；但是平分成两份，在每一份磁盘（也就是 N/2 块磁盘）里面，利用 RAID 0 技术并发读写，这样既提高可靠性又改善性能。不过 RAID 10 的磁盘利用率较低，有一半的磁盘用来写备份数据。 一般情况下，一台服务器上很少出现同时损坏两块磁盘的情况，在只损坏一块磁盘的情况下，如果能利用其他磁盘的数据恢复损坏磁盘的数据，这样在保证可靠性和性能的同时，磁盘利用率也得到大幅提升。 顺着这个思路，RAID 3可以在数据写入磁盘的时候，将数据分成 N-1 份，并发写入 N-1 块磁盘，并在第 N 块磁盘记录校验数据，这样任何一块磁盘损坏（包括校验数据磁盘），都可以利用其他 N-1 块磁盘的数据修复。 但是在数据修改较多的场景中，任何磁盘数据的修改，都会导致第 N 块磁盘重写校验数据。频繁写入的后果是第 N 块磁盘比其他磁盘更容易损坏，需要频繁更换，所以 RAID 3 很少在实践中使用，因此在上面图中也就没有单独列出。 相比 RAID 3，RAID 5是使用更多的方案。RAID 5 和 RAID 3 很相似，但是校验数据不是写入第 N 块磁盘，而是螺旋式地写入所有磁盘中。这样校验数据的修改也被平均到所有磁盘上，避免 RAID 3 频繁写坏一块磁盘的情况。 如果数据需要很高的可靠性，在出现同时损坏两块磁盘的情况下（或者运维管理水平比较落后，坏了一块磁盘但是迟迟没有更换，导致又坏了一块磁盘），仍然需要修复数据，这时候可以使用RAID 6。 RAID 6 和 RAID 5 类似，但是数据只写入 N-2 块磁盘，并螺旋式地在两块磁盘中写入校验信息（使用不同算法生成）。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>bigdata</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Algorithm]]></title>
    <url>%2Fpost%2Falgo%2F</url>
    <content type="text"><![CDATA[Record the learning algorithm experience. Math Reverse Integer 12345678910111213func reverse(x int) int &#123; //math.MinInt32 = -2147483648 //math.MaxInt32 = 2147483647 var result int for x!=0&#123; result=result*10+x%10 if result &gt; 2147483647 || result &lt; -2147483648&#123; return 0 &#125; x/=10 &#125; return result&#125; quickSort Based on recursive 123456789101112131415161718192021222324252627282930313233343536package mainimport "fmt"func quickSort(arr []int, startIndex, endIndex int ) &#123; if startIndex &gt;= endIndex &#123; return &#125; pivotIndex:= partition(arr,startIndex,endIndex) quickSort(arr,startIndex,pivotIndex-1) quickSort(arr,pivotIndex+1,endIndex)&#125;func partition(arr []int, startIndex, endIndex int ) int &#123; pivot:=arr[startIndex] mark:=startIndex for i:=startIndex+1; i&lt;= endIndex; i++&#123; if arr[i]&lt;pivot &#123; mark++ // numbers that smaller pivot ++ arr[i],arr[mark] = arr[mark], arr[i] &#125; &#125; arr[startIndex] = arr [mark] arr[mark] = pivot return mark&#125;func main() &#123; var arr = []int&#123;4,4,6,5,3,2,8,1&#125; quickSort(arr,0, len(arr)-1) fmt.Print(arr)&#125; Use stack 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package mainimport "fmt"import "github.com/golang-collections/collections/stack"func quickSort(arr []int, startIndex, endIndex int ) &#123; quickSortStack := stack.New() rootParam := make(map[string]int) rootParam["startIndex"] = startIndex rootParam["endIndex"] = endIndex quickSortStack.Push(rootParam) for quickSortStack.Len() &gt; 0 &#123; param := quickSortStack.Pop().(map[string]int) pivotIndex := partition(arr, param["startIndex"], param["endIndex"] ) if param["startIndex"] &lt; pivotIndex-1 &#123; leftParam := make(map[string]int) leftParam["startIndex"] = param["startIndex"] leftParam["endIndex"] = pivotIndex-1 quickSortStack.Push(leftParam) &#125; if pivotIndex+1 &lt; param["endIndex"] &#123; rightParam := make(map[string]int) rightParam["startIndex"] = pivotIndex+1 rightParam["endIndex"] = param["endIndex"] quickSortStack.Push(rightParam) &#125; &#125;&#125;func partition(arr []int, startIndex, endIndex int ) int &#123; pivot:=arr[startIndex] mark:=startIndex for i:=startIndex+1; i&lt;= endIndex; i++&#123; if arr[i]&lt;pivot &#123; mark++ // smaller pivot numbers ++ arr[i],arr[mark] = arr[mark], arr[i] &#125; &#125; arr[startIndex] = arr [mark] arr[mark] = pivot return mark&#125;func main() &#123; var arr = []int&#123;4,4,6,5,3,2,8,1&#125; quickSort(arr,0, len(arr)-1) fmt.Print(arr)&#125; Hash Table Two Sum 1234567891011121314func twoSum(nums []int, target int) []int &#123; dic := make(map[int]int) for i, v := range nums &#123; k,ok := dic[target-v] if ok &#123; return []int&#123;k, i&#125; &#125; dic[v]=i &#125; return nil &#125; least recently used]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go 学习笔记]]></title>
    <url>%2Fpost%2Fgolang%2F</url>
    <content type="text"><![CDATA[本文初始化于 2019-11-11 02:12:12，深夜开启学习 golang 的道路。go 是静态类型的语言，入门推荐Go 编程语言指南还有一个极客时间的课程示例代码 GOROOT GOPATH GOROOT：Go 语言安装根目录的路径，也就是 GO 语言的安装路径。 GOPATH：若干工作区目录的路径。是我们自己定义的工作空间。 官方解释 GOBIN：GO 程序生成的可执行文件（executable file）的路径。 命令源码文件命令源码文件的用途是什么，怎样编写它？命令源码文件是程序的运行入口，是每个可独立运行的程序必须拥有的。我们可以通过构建或安装，生成与其对应的可执行文件，后者一般会与该命令源码文件的直接父目录同名。如果一个源码文件声明属于main包，并且包含一个无参数声明且无结果声明的main函数，那么它就是命令源码文件。在同一个目录下的源码文件都需要被声明为属于同一个代码包。源码文件声明的包名可以与其所在目录的名称不同，只要这些文件声明的包名一致就可以。 库源码文件库源码文件是不能被直接运行的源码文件，它仅用于存放程序实体，这些程序实体可以被其他代码使用（只要遵从 Go 语言规范的话）。第一条规则，同目录下的源码文件的代码包声明语句要一致。也就是说，它们要同属于一个代码包。这对于所有源码文件都是适用的。如果目录中有命令源码文件，那么其他种类的源码文件也应该声明属于main包。这也是我们能够成功构建和运行它们的前提。第二条规则，源码文件声明的代码包的名称可以与其所在的目录的名称不同。在针对代码包进行构建时，生成的结果文件的主名称与其父目录的名称一致。对于命令源码文件而言，构建生成的可执行文件的主名称会与其父目录的名称相同。 数组 array数组是连续存储在内存中的，每一个切片的底层实现都是绑定着一个数组。当切面的值改变时，底层数组也会跟着改变。当切片的 capacity 增大超过当前数组长度时，go 会自动产生一个新的底层数组，长度为以前的 2 倍，然后再绑定到切片上。当切片 capacity 大于 1000 时，底层数组的增长因子就会有 2 变为 1.25。数组的容量永远等于其长度，都是不可变的。 切片 sliceGo 语言的切片类型属于引用类型，同属引用类型的还有字典类型、通道类型、函数类型等；而 Go 语言的数组类型则属于值类型，同属值类型的有基础数据类型以及结构体类型。注意，Go 语言里不存在像 Java 等编程语言中令人困惑的“传值或传引用”问题。在 Go 语言中，我们判断所谓的“传值”或者“传引用”只要看被传递的值的类型就好了。如果传递的值是引用类型的，那么就是“传引用”。如果传递的值是值类型的，那么就是“传值”。从传递成本的角度讲，引用类型的值往往要比值类型的值低很多。我们在数组和切片之上都可以应用索引表达式，得到的都会是某个元素。我们在它们之上也都可以应用切片表达式，也都会得到一个新的切片。切片的容量却不是这样，并且它的变化是有规律可寻的。用make函数初始化切片时，如果不指明其容量，那么它就会和长度一致。如果在初始化时指明了容量，那么切片的实际容量也就是它了。 切片代表的窗口是无法向左扩展的。slice := make([]int, len, cap) Go 语言经典知识总结123456789101112131415161718192021222324252627282930313233基于混合线程的并发编程模型自然不必多说。在数据类型方面有：基于底层数组的切片；用来传递数据的通道；作为一等类型的函数；可实现面向对象的结构体；能无侵入实现的接口等。在语法方面有：异步编程神器go语句；函数的最后关卡defer语句；可做类型判断的switch语句；多通道操作利器select语句；非常有特色的异常处理函数panic和recover。测试 Go 程序的主要方式。这涉及了 Go 语言自带的程序测试套件，相关的概念和工具包括：独立的测试源码文件；三种功用不同的测试函数；专用的testing代码包；功能强大的go test命令。Go 语言提供的那些同步工具。它们也是 Go 语言并发编程工具箱中不可或缺的一部分。这包括了：经典的互斥锁；读写锁；条件变量；原子操作。以及Go 语言特有的一些数据类型，即：单次执行小助手sync.Once；临时对象池sync.Pool；帮助我们实现多 goroutine 协作流程的sync.WaitGroup、context.Context；一种高效的并发安全字典sync.Map。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[那些年在新加坡吃过的店]]></title>
    <url>%2Fpost%2FSG-food%2F</url>
    <content type="text"><![CDATA[民以食为天作为一个在新加坡搬砖的四川吃货，唯有美食不可辜负。坡县虽小，但各式各样的菜系都可以吃到。由于在四川长大，比较喜欢川菜火锅串串香锅烤鱼等。偶尔也跟着朋友们尝试一下其他的菜系。 川菜NTU Canteen 11 川菜 地址：20 Nanyang Ave, Singapore 639809 点评：目前吃过的新加坡性价比最高的川菜，味道好，价格最便宜，随便点还没有踩雷过。唯一的缺点就是地理位置太偏远了。 上图: 牛车水食阁 川味园 地址：32 New Market Rd, Singapore 050032 点评：德阳大叔开的店，味道非常的四川，价格实惠，分量也很足。只是在食阁里比较吵杂，还有点热。沸腾鱼我觉得很一般，不是很入味。据说旁边那家日日红麻辣香锅很不错，还没去吃过。 上图: 厦门街 成都 地址：74 Amoy St, Singapore 069893 点评：新加坡的四川同学安利的，水煮鱼，牛蛙强推，干锅肥肠也不错(但我觉得老成都的干锅肥肠更好吃)。辣子鸡不要点，辣鸡。 上图: 牛车水 老成都 地址：80 Pagoda St, Singapore 059239 点评：曾经我以为这家是热门景点宰客的店，虽然价格是贵，但人家的味道对得起老成都这个名字。目前吃过干锅肥肠，豆花牛肉，炒时蔬，nice！ 无图：吃的时候没有拍照，O(∩_∩)O哈哈~ 金文泰食阁 四川厨子 地址：450 Clementi Ave 3, #01-271, Singapore 120450 点评：窗口那个姐姐是四川邛崃的，目前只吃过她家的烧鸡公一次，味道不错。川菜水煮鱼不错，孜然排骨太干了，不行。 上图: NUS PGP食堂 川菜 地址：23 Prince George’s Park, Singapore 118422 点评：四川厨师，在蜀香吃过一次干煸肥肠，有点干煸过头。2019-03-27 吃了一次水煮肉片，味道很棒。有空调那家卖面条的姐姐是四川大邑的(好像又换人了)，没有空调卖面条的那个锅锅是四川泸州的。没有空调最里面那家我也吃过川菜，味道还行吧。 上图: NUS Utown 川菜 地址：2 College Ave West, Level 2 Stephen Riady Centre, Singapore 138607 点评：东北厨师，配菜是德阳小姐姐。分量是真的足，但味道就很一般了，学校食堂比较便宜。小炒肉、夫妻肺片还不错吧，水煮鱼 鱼香茄子 宫保鸡丁打扰了。 上图: 蜀香添一点 地址：721 Clementi West Street 2, Singapore 120721 点评：以前读书的时候住在附近，经常和学长去这家吃饭。价格还行，有的菜味道还可以。口水鸡，夫妻肺片凉菜不错。小炒肉，鱼香肉丝也行。大盘鸡 辣子鸡 水煮肉片 再见！ 上图: 天府川菜 地址：3151 Commonwealth Ave W, #01-17/18, Singapore 129581 点评：去吃过一次，我觉得酸菜鱼很一般，那个炒莲白更是不行啊。感觉辣椒没有几个，味道非常辣，像是放了辣椒素的感觉。 上图: 四川豆花饭庄 地址：7500 Beach Rd, Singapore 199591 点评：某次志愿者活动结束后去吃过一次。饭店的环境不错，豆花可以但分量超级小，其余的菜就很一般啦，我一点印象都没得咯。 上图: 老四川 看网红博主的测评视频 思味冒菜 地址: 33 Mosque St, Singapore 059511 点评： 我真的佛了，就成都gai上随便吃个冒菜，都比他家的好吃。真的太淡了，除了油，不辣不香，吃了想打人。泡椒牛蛙反而还不错的。 上图： A ONE 地址：23 Serangoon Central, #B1-73/74, Singapore 556083 serangoon 地铁站旁 点评：其实是一家本地的店，居然也有麻辣系列。但分量较小，味道还行。水煮鱼的鱼肉不行，肉质差。水煮肉片的淀粉少了，不够嫩。人均 30 新吧 上图： Birds of a Feather 地址: 115 Amoy St, #01-01, Singapore 069935 点评：一家精致的川菜，分量超级小，有点西式料理的味道。人少的话可以来试一试。就在 厦门街 成都 那一条街上。 上图： 小渔村柴火鸡 地址: 201 Upper Paya Lebar Rd, Singapore 534876 点评：柴火鸡的味道非常不错，还可以往锅里面加菜，越到后面味道越浓。老板还是一位马拉松爱好者，店里面全是马拉松的奖牌。人均 20+ 新币，地理位置稍微偏了一点点，其他的挺不错的。 软文 上图： 火锅小龙坎 地址：牛车水或者武吉士 点评： 每年愚人节的那个周末，全球的小龙坎全场半价，抓住机会。平时人均 60 新，味道不错。 上图： 大龙燚 地址：乌节路，somerset 楼上 181 Orchard Rd, #08-08, Singapore 238896 点评：也是又贵又好吃的那种火锅。贫穷限制了我吃火锅。 上图： 同心如意火锅 地址：克拉码头 6 Lor Telok, Singapore 049019 点评：强烈安利了，真的好吃又不贵，环境还优雅，免费的西瓜深受吃瓜群众的喜爱。人均 30-40 新币 上图： 海底捞 地址: 全岛分店太多了，我去的是 vivo city 那家 点评：服务好，价格贵。食材新鲜，就是有时候排队很长。人均 60+，一不小心吃上到了人均 80 新币 上图： 重庆小木凳火锅 地址： 牛车水 279 New Bridge Rd, Singapore 088752 点评： 小组聚餐吃过一次，人均 30-35 新，味道还是非常的不错，环境也还是可以的。 上图： 居然无图，忘了拍照。。。。食间火锅 地址：jurong east 和 Suntec City 点评：老板小武哥哥是 MIT 毕业的重庆大佬，食材新鲜，味道不错。Suntec City 还有自助火锅，人均在 30 新左右。 上图： 优品火锅 地址： 全岛有几家，我吃的是 west coast plaza 那一家 154 West Coast Rd, #01-02, Singapore 127371 点评： 出乎意料的火锅，以为是一家不知名的小店，结果味道很不错。人均 40+ 新 上图： 食立方火锅 地址：全岛连锁,我吃的是 west coast plaza 那一家 154 West Coast Road 127371, 02-24 West Coast Rd, Singapore 127447 点评：吃火锅送公仔娃娃是这家店最大的特色。味道不辣，适合口味淡的朋友们。人均 30 新左右。 上图： 潮汕牛肉火锅 地址: 195 E Coast Rd, Singapore 428900 点评：地理位置有点偏，但味道还不错，一共吃了三次。他家的牛肉粉真是一绝，但我们在吃第三次的时候，有的牛肉居然是冷冻拿出来的，反应给老板。老板态度良好，道歉还打了8折。希望他们越办越好吧。 上图： 满族火锅 地址: 350 Jurong East Ave 1, #01-1231 Singapore Singapore Region, Singapore 600350 点评：裕华园地铁出来，食阁附近的一家小店。那一排的店家都是这个风格，可能是地理位置偏远，价格非常便宜，味道还行。 上图： 锦门大院火锅 地址: 22 Mosque St, Singapore 059502 点评：一家新开的自助火锅店，装修还行。但这个铁锅，不敢恭维。味道还不错，也算便宜，地理位置也方便，安利一下。 新加坡眼的软文, 我觉得夸张了，哈哈 上图： 串串嘿串串 地址： 291 South Bridge Rd, Singapore 058836 点评： 可以做游戏打折，比如立定跳远。还有自助串串，我觉得一般般。感觉好像味道下降了？ 上图： 重庆李记串串 地址： 295 South Bridge Rd, Singapore 058838 South Bridge Rd, Singapore 058838，巧了就在嘿串串旁边 点评： 一年前吃过一次，味道还不错，感觉性价比 比嘿串串高，但店面环境没有嘿串串好。 上图： 香锅Timbre大海麻辣香锅 地址：73A Ayer Rajah Crescent, JTC Launchpad, Singapore 139957 one north 地铁站出来 Timbre+ 食阁里 点评：非常温和的一家香锅，记得喊老板多放油。毛肚不错哦，还可以送外卖的。人均 8-15 新 上图： OneNorth口福麻辣香锅 地址：1 Fusionopolis Way, Singapore 138577 One North 口福 点评：他们家放的芝麻可是真的比较多，真香！ 上图： Galaxis二楼麻辣香锅 地址： 1 Fusionopolis Pl, Singapore 138522 Galaxis 二楼食阁 点评： 有一个绵阳的哥哥在这里，是我看到过的第一家荤素菜不分开计算重量的麻辣香锅。所以也就是最便宜的麻辣香锅。 上图： 宽宽干锅 地址：38 Mosque St, Singapore 059516 点评：是伟翔锅锅请我这个小弟吃的，味道不错，和国内的干锅比较相似。鸭掌还不够糯，比起成都的销魂掌还是有点差距的。 上图： 美蛙烤鱼探鱼 地址：313 Orchard Rd, Singapore 238895 点评：新加坡吃过最好吃的烤鱼，肉质不是油炸的那种烤鱼。推荐鲜青椒烤鱼，重庆豆花烤鱼。人均 30-40 新 上图: 蛙功夫 地址: 牛车水店 470 North Bridge Rd, Singapore 188735 点评：微信提前预定打九折。新加坡第一蛙了，怪椒味和姜辣味真的好辣啊，四川人都遭不住了。 人均 30-40 新，最新发现蛙功夫在 eatigo 上有打折，最低有半价哦。注册时可填写我的邀请码【 eati1avcj 】 上图： 齐来丰酸菜鱼 地址: 133 New Bridge Road B1-14/15 Chinatown Point S059413 点评： 新加坡眼推荐 味道还不错，价格还可以接受，可以试一试。 上图： 尝相思 重庆烤鱼 地址: Singapore 419779 点评：地理位置有点偏僻，在 eunos 地铁站出来步行 12 分钟左右。老板是本地人，老板娘是重庆人。他们家的烤鱼是油炸的，算是在新加坡油炸的烤鱼中非常不错的了。而且价格比较的实惠。推荐麻辣味道 上图: 午阳烤鱼 NUS YIH 地址: 31 Lower Kent Ridge Rd, Singapore 119078 点评： NUS YIH 学校食堂的烤鱼，一份才 12 新币。油炸的鱼，再配以佐料，油汤煮一下即可。味道还不错，油放的超级多（四川人表示很赞），辣度适中，这个价格性价比很高。 上图： 螃蟹Long Seafood 地址： Ang Mo Kio Avenue 3, #01-1222,Block 232, Singapore 560232 龙海鲜螃蟹王宏茂桥店 点评： 在一个食阁的一楼，店面很大。米粉螃蟹不错，人均 70 新币 上图： Mellben Signature 地址：7 Tanjong Pagar Plaza, #01-105, Singapore 081007 点评： 人均 60 新币，感觉螃蟹小一点点，没有宏茂桥那家好吃，米粉螃蟹的话。 上图： Por Kee Eating House 1996 地址: 69 Seng Poh Ln, #01-02, Singapore 160069 点评: 非常 local 的一家本地菜系, 需要提前打电话预定，味道还不错，人均 50 左右 上图: 烤肉烤串Supulae 李加绒同学倾情推荐，还没有来得及去吃。Super Star K 地址: 75 Tg Pagar Rd, Singapore 088496 点评：吃过3次了，那一条街大部分店面都是晚上营业，他家是少有的中午也开门。味道不错，分量也很足。服务员还会帮忙烤肉，态度也很好的。人均 40 新左右，推荐。 上图： GO! K - BBQ 地址: 76 Amoy St, Singapore 069895 就在川菜《成都》店旁边，都在厦门街 点评：晚上才开，人很多，最好提前预定。他家的肉是腌制过后的肉，味道更加的美味。但感觉服务员少了，有点忙不过来。肉的分量稍微小点，价格也就偏贵。 上图： 烧肉王子 地址: 321 Clementi Ave 3, #01-01, Singapore 129905 点评： 日式的自助烤肉，分为三个等级：只有鸡肉，有猪肉牛肉，顶级和牛。我们当时吃的是中间那个档次，人均 32 新币。 上图： 大胡子烧烤 地址: 70 Lor 25A Geylang, Singapore 388255 点评： 就在 Aljunied 地铁站出来，烤串是非常不错。干锅肥肠还需要煎的更焦一点。 店里有点炒闹，吃饭时感觉回到了国内小城市的街边小店。 上图： 香港小厨 地址: 24 Clementi Rd, Singapore 129753 点评：就在 NUS 后街，烤串还是不错的，烤羊排不行，少的太真实了。烤韭菜，一定要让老板少放点盐啊！ 上图: 王大爷烧烤 地址: 16 Mosque St, Singapore 059496 点评：在牛车水摩士街，有点小贵。店面环境还不错，味道一般般，那个柴火鸡一定不要点，比起成都的差太多了，吃了想打人。 上图： HANSSIK 地址: 3155 Commonwealth Ave W, #05-17/18, Singapore 129588 点评： 金文泰商场 5 楼，一家自助烤肉，味道还行吧，人均 33 新。吃到最后有点闷了。。。。 上图： 鸡饭了凡香港油鸡饭 地址：78 Smith St, Singapore 058972 点评： 米其林一星的鸡饭，我觉得可以。 上图： 天天海南鸡饭待吃荣亮阁 地址: #01-192, 725 Clementi West Street 2, Block 725, Singapore 120725 点评：家附近的一家鸡饭店，本地人经常去吃，物美价廉。 上图： 面食大华猪肉裸条面 地址: 466 Crawford Ln, #01-12, Singapore 190466 点评: 就在 ICA 移民局背后的组屋群，一楼的食阁窗口，号称米其林一星。味道还不错，料很足的。但是价格也比一般的店铺贵一点，还是值得一吃。 上图: 舌尖尖兰州拉面 地址：有几家分店 302 Tiong Bahru Road, Tiong Bahru Plaza,#02-107 点评：在新加坡非常火的一家兰州拉面，味道还是不错的。 上图： jurong east 食阁 地址：10 Jurong East Street 12, Singapore 609690 点评：在 jurong east 地铁出来，下一层电梯。 1 楼的食阁里，有一家西安的面食，四合一，油泼面都非常不错，价格也实惠，唯一就是没有空调有点热。 上图： O.BBa Jjajang 地址: 77 Tg Pagar Rd, Singapore 088498 点评： 他家的炸酱面不错，大猪蹄子也还可以。每次都需要排队一小会，中午也开门，旁边就是那家 Super Star K 上图： 其他美食莆田 地址: 1 HarbourFront Walk #02-131/132 点评： 一家非常不错的福建菜。空调有点冷，九转粉肠非常的好吃，虾苗拌紫菜也好吃，蛏子也很大只的。百秒黄花鱼也是很嫩。扁肉汤的馄饨好吃，汤一般般。蔬菜豆腐汤不错，很好喝推荐。脆皮蒜香鸡不行，不推荐。 上图： 汕头海鲜 地址: Blk 181 Lorong 4 Toa Payoh #02-602 有两家店 点评：我们当时是去吃的自助下午茶，会有阿姨推车过，然后选择需要的茶点。环境还不错，菜品也丰富。可以早点去 2:30 到店里。 上图： Sushi Tei 地址: 新加坡全岛有 13 家分店，我是在家附近 Raffles Holland V 吃的 点评：猪肉饭非常的不错，鳗鱼寿司也可以。金针菇牛肉卷感觉一般，刺身拼盘也可以。官网菜单 上图： 蟹老宋 地址: 16 Smith St, Singapore 058930 点评：本来去是想吃大闸蟹的，结果只剩大个头的蟹蟹，48 新币一只，打扰了。然后就吃了他家的其他一些菜，小龙虾和烤鸭都还不错的，酸辣鸡杂也不错。 上图:]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>美食</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Prometheus 指南]]></title>
    <url>%2Fpost%2Fprometheus%2F</url>
    <content type="text"><![CDATA[Prometheus 作为业务级监控告警工具，再配合上可视化工具 Grafana，运维人员能方便的监控所需的指标。本文记录了小白入门学习的过程 学习资料 Prometheus 中文文档 官方文档地址 入门体验区快速安装快速安装应用首选 docker 方式，不需要配置复杂的环境。当我们已经非常熟悉如何使用 prometheus 的时候，再返回来使用普通安装。 12345678# 只监听在本地docker run --name prometheus -d -p 127.0.0.1:9090:9090 prom/prometheus# 将配置文件挂载到容器中，方便修改docker run -p 9090:9090 -v /tmp/prometheus.yml:/etc/prometheus/prometheus.yml prom/prometheus# 使用额外的数据卷挂载配置文件：docker run -p 9090:9090 -v /prometheus-data prom/prometheus --config.file=/prometheus-data/prometheus.yml 安装完成后，即可访问 http://localhost:9090 可以在 Graph 查询监控项，在 Status 查看监控了哪些机器，配置文件等。 修改配置文件[配置介绍官方文档]https://prometheus.io/docs/prometheus/latest/configuration/configuration/)我是在 Mac 电脑上 docker 安装 prometheus。由于我没有把配置文件挂载出来，只能进入容器去修改。123docker exec -u root -it 51ae3954e880 shvi /etc/prometheus/prometheus.yml 配置文件如下所示123456789101112131415161718192021222324252627282930313233343536# my global configglobal: scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute. # scrape_timeout is set to the global default (10s).# Alertmanager configurationalerting: alertmanagers: - static_configs: - targets: # - alertmanager:9093# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.rule_files: # - "first_rules.yml" # - "second_rules.yml"# A scrape configuration containing exactly one endpoint to scrape:# Here it's Prometheus itself.scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. - job_name: 'prometheus' # metrics_path defaults to '/metrics' # scheme defaults to 'http'. static_configs: - targets: ['localhost:9090'] - job_name: 'feiy' scrape_interval: 5s static_configs: - targets: ['host.docker.internal:8000'] labels: group: 'production' 完整的配置例子，请点击这里 检查结果打开 http://localhost:9090/targets 我们可以看到如下的结果 图中两个 target，第一个 feiy 是我自己写的一个 django 小程序，暴露出来了我关心的指标，用了Prometheus Python Client。第二个则是 Prometheus 服务器自带的监控数据。 123456789101112131415161718192021222324252627282930313233343536# feiy 暴露出来的数据# HELP python_gc_objects_collected_total Objects collected during gc# TYPE python_gc_objects_collected_total counterpython_gc_objects_collected_total&#123;generation="0"&#125; 17999.0python_gc_objects_collected_total&#123;generation="1"&#125; 2384.0python_gc_objects_collected_total&#123;generation="2"&#125; 833.0# HELP python_gc_objects_uncollectable_total Uncollectable object found during GC# TYPE python_gc_objects_uncollectable_total counterpython_gc_objects_uncollectable_total&#123;generation="0"&#125; 0.0python_gc_objects_uncollectable_total&#123;generation="1"&#125; 0.0python_gc_objects_uncollectable_total&#123;generation="2"&#125; 0.0# HELP python_gc_collections_total Number of times this generation was collected# TYPE python_gc_collections_total counterpython_gc_collections_total&#123;generation="0"&#125; 255.0python_gc_collections_total&#123;generation="1"&#125; 23.0python_gc_collections_total&#123;generation="2"&#125; 2.0# HELP python_info Python platform information# TYPE python_info gaugepython_info&#123;implementation="CPython",major="3",minor="7",patchlevel="1",version="3.7.1"&#125; 1.0# HELP request_processing_seconds Time spent processing request# TYPE request_processing_seconds summaryrequest_processing_seconds_count&#123;endpoint="/metrics/",method="GET",status_code="200"&#125; 3.0request_processing_seconds_sum&#123;endpoint="/metrics/",method="GET",status_code="200"&#125; 0.0# TYPE request_processing_seconds_created gaugerequest_processing_seconds_created&#123;endpoint="/metrics/",method="GET",status_code="200"&#125; 1.570089497964517e+09# HELP request_byte_sum_total Total request byte sum# TYPE request_byte_sum_total counterrequest_byte_sum_total&#123;endpoint="/metrics/",method="GET",status_code="200"&#125; 0.0# TYPE request_byte_sum_created gaugerequest_byte_sum_created&#123;endpoint="/metrics/",method="GET",status_code="200"&#125; 1.570089497964633e+09# HELP response_byte_sum_total Total response byte sum# TYPE response_byte_sum_total counterresponse_byte_sum_total&#123;endpoint="/metrics/",method="GET",status_code="200"&#125; 5533.0# TYPE response_byte_sum_created gaugeresponse_byte_sum_created&#123;endpoint="/metrics/",method="GET",status_code="200"&#125; 1.5700894979645782e+09 我们可以在 Graph 查询结果，比如查询 python_info{implementation=”CPython”,major=”3”,minor=”7”,patchlevel=”1”,version=”3.7.1”} 1.0得到了如下的结果 进阶区暴露数据 node exporter 监控 Linux 机器 监控容器 metric 官方文档 详细解读 Prometheus 的指标类型 一文搞懂 Prometheus 的直方图 rules 官方文档123# 如果有错的话，则那一条规则不会更新。go get github.com/prometheus/prometheus/cmd/promtoolpromtool check rules /path/to/example.rules.yml 采坑区]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac 小技巧]]></title>
    <url>%2Fpost%2Fmac%2F</url>
    <content type="text"><![CDATA[苹果电脑小技巧。 命令行技巧 移动到行首 ctrl+A 移动当行尾 ctrl+E 往左移一个单词 esc+B 往右移一个单词 esc+F 删除光标前一个单词 ctrl+W 删除光标前所有 ctrl+U 上传rz 下载sz 首先下载 iTerm2 因为自带的 terminal 不行啊 安装 lrzsz brew install lrzsz 下载两个脚本 send-zmodem.sh recv-zmodem.sh1234cd /usr/local/bin sudo wget https://gist.githubusercontent.com/sy-records/1b3010b566af42f57fa6fa38138dd22a/raw/2bfe590665d3b0e6c8223623922474361058920c/iterm2-send-zmodem.sh sudo wget https://gist.githubusercontent.com/sy-records/40f4ba22e3fbdeedf58463b067798962/raw/b32d2f7ac3fa54acca81be3664797cebb724690f/iterm2-recv-zmodem.shsudo chmod 755 /usr/local/bin/iterm2-* 如果链接失败，可以复制以下文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#vim /usr/local/bin/iterm2-send-zmodem.sh#!/bin/bash# Author: Matt Mastracci (matthew@mastracci.com)# AppleScript from http://stackoverflow.com/questions/4309087/cancel-button-on-osascript-in-a-bash-script# licensed under cc-wiki with attribution required # Remainder of script public domainosascript -e 'tell application "iTerm2" to version' &gt; /dev/null 2&gt;&amp;1 &amp;&amp; NAME=iTerm2 || NAME=iTermif [[ $NAME = "iTerm" ]]; then FILE=`osascript -e 'tell application "iTerm" to activate' -e 'tell application "iTerm" to set thefile to choose file with prompt "Choose a file to send"' -e "do shell script (\"echo \"&amp;(quoted form of POSIX path of thefile as Unicode text)&amp;\"\")"`else FILE=`osascript -e 'tell application "iTerm2" to activate' -e 'tell application "iTerm2" to set thefile to choose file with prompt "Choose a file to send"' -e "do shell script (\"echo \"&amp;(quoted form of POSIX path of thefile as Unicode text)&amp;\"\")"`fiif [[ $FILE = "" ]]; then echo Cancelled. # Send ZModem cancel echo -e \\x18\\x18\\x18\\x18\\x18 sleep 1 echo echo \# Cancelled transferelse /usr/local/bin/sz "$FILE" -e -b sleep 1 echo echo \# Received $FILEfi#vim /usr/local/bin/iterm2-recv-zmodem.sh#!/bin/bash# Author: Matt Mastracci (matthew@mastracci.com)# AppleScript from http://stackoverflow.com/questions/4309087/cancel-button-on-osascript-in-a-bash-script# licensed under cc-wiki with attribution required # Remainder of script public domainosascript -e 'tell application "iTerm2" to version' &gt; /dev/null 2&gt;&amp;1 &amp;&amp; NAME=iTerm2 || NAME=iTermif [[ $NAME = "iTerm" ]]; then FILE=`osascript -e 'tell application "iTerm" to activate' -e 'tell application "iTerm" to set thefile to choose folder with prompt "Choose a folder to place received files in"' -e "do shell script (\"echo \"&amp;(quoted form of POSIX path of thefile as Unicode text)&amp;\"\")"`else FILE=`osascript -e 'tell application "iTerm2" to activate' -e 'tell application "iTerm2" to set thefile to choose folder with prompt "Choose a folder to place received files in"' -e "do shell script (\"echo \"&amp;(quoted form of POSIX path of thefile as Unicode text)&amp;\"\")"`fiif [[ $FILE = "" ]]; then echo Cancelled. # Send ZModem cancel echo -e \\x18\\x18\\x18\\x18\\x18 sleep 1 echo echo \# Cancelled transferelse cd "$FILE" /usr/local/bin/rz -E -e -b sleep 1 echo echo echo \# Sent \-\&gt; $FILEfi iTerm2 的配置点击 iTerm2 的设置界面 Perference -&gt; Profiles -&gt; Default -&gt; Advanced -&gt; Triggers 的 Edit 按钮点击+号，添加如下的参数123456789Regular expression: \*\*B0100 Action: Run Silent Coprocess Parameters: /usr/local/bin/iterm2-send-zmodem.sh Instant: checkedRegular expression: \*\*B00000000000000 Action: Run Silent Coprocess Parameters: /usr/local/bin/iterm2-recv-zmodem.sh Instant: checked 添加完成如下图所示简单的用法，当 ssh 登录后12345678910# 下载一个文件： sz filename # 下载多个文件： sz filename1 filename2# 下载dir目录下的所有文件，不包含dir下的文件夹： sz dir/*# 直接键入rz命令即可rz# 在弹出的文件窗口，选择需要上传的文件]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Google Script 学习]]></title>
    <url>%2Fpost%2Fgoogle-script%2F</url>
    <content type="text"><![CDATA[本文记录用Google Script 处理数据，发送每日邮件的过程。 创建 Sheet 和 Script首先用自己的谷歌账号创建一个新的 Google Sheet, 然后点击菜单栏的 Tools &gt; Script editor 就可以创建脚本。其语法和 JavaScript 相似的。1234567891011/** * Sends emails with data from the current spreadsheet. */// 首先添加一个发送按钮名称为 Action，绑定到函数 SendEmailfunction onOpen() &#123; SpreadsheetApp.getUi() .createMenu('Action') .addItem('Send Daily Report', 'SendEmail') .addToUi();&#125; 学会 debug可以调用函数 Logger.log(); 打印结果到后台，然后在 View &gt; Logs 查看结果。举一个简单的例子：markdown 语法注意，使用时发现，表格的语句上一行必须为空行，不然表格不生效。比如你的表是这个样子的 A B 1 A1 B1 2 A2 B2 3 A3 B3 4 A4 B4 12345678function logProductInfo() &#123; var sheet = SpreadsheetApp.getActiveSheet(); var data = sheet.getDataRange().getValues(); for (var i = 0; i &lt; data.length; i++) &#123; Logger.log('Product name: ' + data[i][0]); Logger.log('Product number: ' + data[i][1]); &#125;&#125; 我们在 script editor 界面点击 Run &gt; Run function &gt; logProductInfo, 运行结束后，可以点击 View &gt; Logs 查看结果。12345678[19-08-15 01:16:37:475 PDT] Product name: A1[19-08-15 01:16:37:475 PDT] Product number: B1[19-08-15 01:16:37:476 PDT] Product name: A2[19-08-15 01:16:37:476 PDT] Product number: B2[19-08-15 01:16:37:477 PDT] Product name: A3[19-08-15 01:16:37:477 PDT] Product number: B3[19-08-15 01:16:37:477 PDT] Product name: A4[19-08-15 01:16:37:478 PDT] Product number: B4 Code.gs直接放上代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/** * Sends emails with data from the current spreadsheet. */function onOpen() &#123; SpreadsheetApp.getUi() .createMenu('Action') .addItem('Send Daily Report', 'SendEmail') .addToUi();&#125;function test() &#123; var scriptProperties = PropertiesService.getScriptProperties(); var nickname = scriptProperties.getProperty('Project'); Logger.log(nickname)&#125;function SendEmail() &#123; var client = PropertiesService.getScriptProperties().getProperty('Project'); // Property 可以在 File &gt; Project Properties &gt; Script properties 里面设置 var monitor_vn = SpreadsheetApp.getActiveSpreadsheet().getSheetByName("VN").getDataRange().getValues(); // 获取表单名称为 VN 的表内容 var monitor_th = SpreadsheetApp.getActiveSpreadsheet().getSheetByName("TH").getDataRange().getValues(); // 获取表单名称为 TH 的表内容 var sendlist = SpreadsheetApp.getActiveSpreadsheet().getSheetByName("email").getDataRange().getValues(); // 获取表单名称为 email 的表内容 var sendto = [] var sendcc = [] // 将发送和 cc 分开存储 for (var i=1;i&lt;sendlist.length;i++)&#123; if (sendlist[i][0]!='') sendto.push(sendlist[i][0]) if (sendlist[i][1]!='') sendcc.push(sendlist[i][1]) &#125; sendto = sendto.join(',') sendcc = sendcc.join(',') // Email Template 连接绑定文件 index.html var template = HtmlService.createTemplateFromFile('index'); // 传参到 template template.monitor_vn = monitor_vn; template.monitor_th = monitor_th; template.client = client; // 自定义一个数组，将数字月份映射为英文 const monthNames = ["January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December" ]; // 获取当前的日期 var d = new Date(); // 发送邮件 MailApp.sendEmail(&#123; to: sendto, cc: sendcc, subject: client+' Monitoring Daily Report ('+d.getDate()+'-'+monthNames[d.getMonth()]+'-'+d.getYear()+')', htmlBody: template.evaluate().getContent()&#125;);&#125; index.html这个是邮件的主体模板，可以看一下官网介绍条件语句嵌套&lt;? … ?&gt;12345678910111213&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;base target="_top"&gt; &lt;/head&gt; &lt;body&gt; &lt;? if (true) &#123; ?&gt; &lt;p&gt;This will always be served!&lt;/p&gt; &lt;? &#125; else &#123; ?&gt; &lt;p&gt;This will never be served.&lt;/p&gt; &lt;? &#125; ?&gt; &lt;/body&gt;&lt;/html&gt; 赋值语句&lt;?= … ?&gt; 是 12345678910111213&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;base target=&quot;_top&quot;&gt; &lt;/head&gt; &lt;body&gt; &lt;?= &apos;My favorite Google products:&apos; ?&gt; &lt;? var data = [&apos;Gmail&apos;, &apos;Docs&apos;, &apos;Android&apos;]; for (var i = 0; i &lt; data.length; i++) &#123; ?&gt; &lt;b&gt;&lt;?= data[i] ?&gt;&lt;/b&gt; &lt;? &#125; ?&gt; &lt;/body&gt;&lt;/html&gt; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;base target="_top"&gt; &lt;/head&gt; &lt;body&gt; &lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;Below is daily ops report for &lt;?=client?&gt;:&lt;/p&gt; &lt;b style="font-size: 15px;"&gt;Monitoring_VN:&lt;/b&gt;&lt;br&gt; &lt;table style="border-collapse: collapse; margin-left:20px;border: 1px solid black"&gt; &lt;tr&gt; &lt;? for (var j=1;j&lt;7;j++) &#123; ?&gt; &lt;!-- 因为只读取1-6列的数据 --&gt; &lt;th style="border: 1px solid black;background-color: grey; color: white; width:15%"&gt;&lt;?= monitor_vn[0][j] ?&gt;&lt;/th&gt; &lt;? &#125; ?&gt; &lt;/tr&gt; &lt;!--monitor_vn.length 是行数--&gt; &lt;? for(var i=1;i&lt;monitor_vn.length;i++) &#123; ?&gt; &lt;tr&gt; &lt;? for(var j=1;j&lt;7;j++) &#123; ?&gt; &lt;? if ( j&gt;2 &amp;&amp; j&lt; 6 ) &#123; ?&gt; &lt;? if (monitor_vn[i][j] &gt; 0.7) &#123;?&gt; &lt;!-- 值大于0.7 底色红色--&gt; &lt;td style="border: 1px solid black; background-color: red;"&gt;&lt;?= (monitor_vn[i][j]*100).toFixed(2)?&gt;%&lt;/td&gt; &lt;? &#125; else &#123; ?&gt; &lt;!--toFixed(2) 保留两位小数 --&gt; &lt;td style="border: 1px solid black;"&gt;&lt;?= (monitor_vn[i][j]*100).toFixed(2)?&gt;%&lt;/td&gt; &lt;? &#125; ?&gt; &lt;? &#125; else &#123; ?&gt; &lt;? if ( j==6 &amp;&amp; monitor_vn[i][j] &gt; 100) &#123;?&gt; &lt;td style="border: 1px solid black;background-color: red;"&gt;&lt;?= (monitor_vn[i][j]) ?&gt;&lt;/td&gt; &lt;? &#125; else &#123; ?&gt; &lt;td style="border: 1px solid black;"&gt;&lt;?= (monitor_vn[i][j]) ?&gt;&lt;/td&gt; &lt;? &#125; ?&gt; &lt;? &#125; ?&gt; &lt;? &#125; ?&gt; &lt;/tr&gt; &lt;? &#125; ?&gt; &lt;/table&gt;&lt;br&gt; &lt;b style="font-size: 15px;"&gt;Monitoring_TH:&lt;/b&gt;&lt;br&gt; &lt;table style="border-collapse: collapse; margin-left:20px;border: 1px solid black"&gt; &lt;tr&gt; &lt;? for (var j=1;j&lt;7;j++)&#123; ?&gt; &lt;th style="border: 1px solid black;background-color: grey; color: white; width:15%"&gt;&lt;?= monitor_th[0][j] ?&gt;&lt;/th&gt; &lt;? &#125; ?&gt; &lt;/tr&gt; &lt;? for(var i=1;i&lt;monitor_th.length;i++)&#123;?&gt; &lt;tr&gt; &lt;? for(var j=1;j&lt;7;j++)&#123;?&gt; &lt;? if ( j&gt;2 &amp;&amp; j&lt; 6 ) &#123; ?&gt; &lt;? if (monitor_th[i][j] &gt; 0.7) &#123;?&gt; &lt;td style="border: 1px solid black; background-color: red;"&gt;&lt;?= (monitor_th[i][j]*100).toFixed(2)?&gt;%&lt;/td&gt; &lt;? &#125; else &#123; ?&gt; &lt;td style="border: 1px solid black;"&gt;&lt;?= (monitor_th[i][j]*100).toFixed(2)?&gt;%&lt;/td&gt; &lt;? &#125; ?&gt; &lt;? &#125; else &#123; ?&gt; &lt;? if ( j==6 &amp;&amp; monitor_th[i][j] &gt; 100) &#123;?&gt; &lt;td style="border: 1px solid black;background-color: red;"&gt;&lt;?= (monitor_th[i][j]) ?&gt;&lt;/td&gt; &lt;? &#125; else &#123; ?&gt; &lt;td style="border: 1px solid black;"&gt;&lt;?= (monitor_th[i][j]) ?&gt;&lt;/td&gt; &lt;? &#125; ?&gt; &lt;? &#125; ?&gt; &lt;? &#125; ?&gt; &lt;/tr&gt; &lt;? &#125; ?&gt; &lt;/table&gt;&lt;br&gt; &lt;/body&gt;&lt;/html&gt; 未完]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>google</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker 学习笔记]]></title>
    <url>%2Fpost%2Fdocker%2F</url>
    <content type="text"><![CDATA[本文记录了一些基础的 docker 知识。 安装 docker分享一篇CentOS 7 下 yum 方式安装 Docker 环境1234567sudo yum install -y yum-utils device-mapper-persistent-data lvm2sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.reposudo yum install -y docker-cesudo systemctl start docker 一个“容器”，实际上是一个由 Linux Namespace、Linux Cgroups 和 rootfs 三种技术构建出来的进程的隔离环境。这三种技术介绍？Namespace 的作用是“隔离”，它让应用进程只能看到该 Namespace 内的“世界”；而 Cgroups 的作用是“限制” rootfs 只是一个操作系统所包含的文件、配置和目录，并不包括操作系统内核。在 Linux 操作系统中，这两部分是分开存放的，操作系统只有在开机启动时才会加载指定版本的内核镜像。正是由于 rootfs 的存在，容器才有了一个被反复宣传至今的重要特性：一致性。 docker commit，实际上就是在容器运行起来后，把最上层的“可读写层”，加上原先容器镜像的只读层，打包组成了一个新的镜像。当然，下面这些只读层在宿主机上是共享的，不会占用额外的空间。 docker volume镜像的各个层，保存在 /var/lib/docker/aufs/diff 目录下，在容器进程启动后，它们会被联合挂载在 /var/lib/docker/aufs/mnt/ 目录中，这样容器所需的 rootfs 就准备好了。volume /test 挂载出来之后，文件会出现在了宿主机上对应的临时目录里，但是如果你在宿主机上查看该容器的可读写层，虽然可以看到这个 /test 目录，但其内容是空的。而由于使用了联合文件系统，你在容器里对镜像 rootfs 所做的任何修改，都会被操作系统先复制到这个可读写层，然后再修改。这就是所谓的：Copy-on-Write。有些时候，会由于配置文件的出错导致容器运行失败，这个时候不能进入容器修改文件，只能去 /var/lib/docker/overlay2/ID/diff 下面，找到对应的配置文件进行修改。 docker 常用命令12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970# 启动 docker 服务 systemctl start dockersystemctl enable docker# 查看当前系统 Docker 信息 docker info# 拉取 docker 镜像 docker pull image_name# 从 Docker hub 上下载某个镜像 docker pull centos:latest# 查看宿主机上的镜像，Docker 镜像保存在 / var/lib/docker 目录下:docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEmysql 5.7 1b30b36ae96a 8 days ago 372MBzabbix/zabbix-web-nginx-mysql centos-4.0-latest 8be5f91b2fa1 3 weeks ago 415MBzabbix/zabbix-server-mysql centos-4.0-latest 8e5becf45c4e 3 weeks ago 326MB# 删除镜像 docker rmi image_name/image_iddocker rmi zabbix/zabbix-web-nginx-mysql:centos-4.0-latest 或者 docker rmi 8be5f91b2fa1# 查看当前有哪些容器正在运行 docker ps# 查看所有容器 docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESb30307ad65be zabbix/zabbix-web-nginx-mysql:centos-4.0-latest "docker-entrypoint.sh" 7 days ago Exited (255) 8 minutes ago 443/tcp, 0.0.0.0:8080-&gt;80/tcp zabbix-web-nginx-mysql0ad822cd52b7 zabbix/zabbix-server-mysql:centos-4.0-latest "docker-entrypoint.sh" 7 days ago Exited (255) 8 minutes ago 0.0.0.0:10051-&gt;10051/tcp zabbix-server-mysqld01c89a112f7 mysql:5.7 "docker-entrypoint.s…" 7 days ago Exited (255) 8 minutes ago 3306/tcp, 33060/tcp mysql-server# 启动、停止、重启容器命令：docker start container_name/container_iddocker stop container_name/container_iddocker restart container_name/container_id# 后台启动一个容器后，如果想进入到这个容器，可以使用 attachdocker attach container_name/container_id# 删除容器 docker rm container_name/container_id# 查看容器日志 docker logs -f container_name/container_id# 查看容器 IP 地址 docker inspect container_name/container_id# 进入容器 docker exec -it container_name/container_id bash# 从 Docker 容器与宿主机相互传输文件 [root@localhost tmp]# docker cp --helpUsage: docker cp [OPTIONS] CONTAINER:SRC_PATH DEST_PATH|- docker cp [OPTIONS] SRC_PATH|- CONTAINER:DEST_PATHCopy files/folders between a container and the local filesystemdocker cp zabbix_config.sql mysql-server:/tmpdocker cp mysql-server:/tmp/zabbix_config.sql /tmp# 批量删除所有已经退出的容器 docker rm -v $(docker ps -aq -f status=exited)# 批量删除所有仅创建并未成功运行的容器 docker rm -v $(docker ps -aq -f status=created) docker logs删除 logs 设置 logs Daemon configuration file docker iptablesdocker 默认会修改 iptables，但有时候会造成 iptables ip 限制失效的问题。如果对 iptables 比较了解的同学，可以 Prevent Docker from manipulating iptables 12345678910111213# vim /etc/docker/daemon.json&#123; "iptables": false, "log-driver":"json-file", "log-opts": &#123;"max-size":"500m", "max-file":"3"&#125;&#125;max-size=500m，意味着一个容器日志大小上限是500M，max-file=3，意味着一个容器有三个日志，分别是id+.json、id+1.json、id+2.json。systemctl daemon-reloadsystemctl restart docker 还有一种办法就是添加 DOCKER-USER If you need to add rules which load before Docker’s rules, add them to the DOCKER-USER chain. These rules are loaded before any rules Docker creates automatically.12345678910111213*filter:INPUT DROP [0:0]:FORWARD ACCEPT [0:0]:OUTPUT ACCEPT [0:0]:DOCKER-USER - [0:0]-A FORWARD -j DOCKER-USER-A DOCKER-USER -s x.x.x.x -p tcp --dport 80 -j RETURN-A DOCKER-USER -p tcp --dport 80 -j DROP-A DOCKER-USER -p udp --dport 80 -j DROP-A DOCKER-USER -j RETURNCOMMIT Dockerfile首先，安利一下 Dockerfile 优化指南 123456789101112FROM python:3.7RUN apt-get update \ &amp;&amp; rm -rf /var/lib/apt/lists/*WORKDIR /usr/src/appCOPY requirements.txt ./RUN pip install -r requirements.txtCOPY . .EXPOSE 8000CMD [&quot;python&quot;, &quot;manage.py&quot;, &quot;runserver&quot;, &quot;0.0.0.0:8000&quot;] tagdocker 镜像有许多的版本，不同版本间差异需要注意。举个例子，我遇到因为版本不同，ip route get 结尾多了一个 uid 012345678910111213141516171819202122232425262728#执行这个命令，在不同 tag 版本的 Python 镜像里，结果会不一样。ip -4 route get 8.8.8.8# 结果多了一个 uid 会导致提取 IP 时出错。8.8.8.8 via 192.168.0.1 dev eth0 src 192.168.0.105 uid 0 8.8.8.8 via 192.168.0.1 dev eth0 src 192.168.0.105 # 两者之间的差异是因为 iproute2 版本不一样root@feiy:/etc/apt# apt-cache policy iproute2iproute2: Installed: 4.9.0-1+deb9u1 Candidate: 4.9.0-1+deb9u1 Version table: *** 4.9.0-1+deb9u1 500 500 http://deb.debian.org/debian stretch/main amd64 Packages 100 /var/lib/dpkg/statusroot@feiy:/etc/apt# apt-cache policy iproute2iproute2: Installed: 4.20.0-2 Candidate: 4.20.0-2 Version table: *** 4.20.0-2 500 500 http://deb.debian.org/debian buster/main amd64 Packages 100 /var/lib/dpkg/status networkdocker 是基于 iptables 进行流量的转发，添加了一个虚拟的网卡 docker0 。 如果想从容器内部访问宿主机的 IP，比如从 172.17.0.3 访问宿主机的内网地址 192.168.1.10 ， 我们从 tcpdump 在网卡 docker0 抓包可以看到流量。 但是在内网网卡抓包没有结果。猜测原因是：当数据包从网卡 docker0 转发后，直接在内核进入 INPUT 链。所以，如果想访问宿主机的 IP， 需要在 INPUT 添加相关规则，允许访问。 iptables -A INPUT -i docker0 -j ACCEPT mac 上使用 docker volume 的位置和 linux 不同，解决办法如下 12345678cd ~/Library/Containers/com.docker.docker/Data/vms/0/# 连接进入 ttyscreen tty# 在 tty 可以就可以找到 docker container volumecd /var/lib/docker/volumes/ 如果想从容器内部访问 mac，也和 Linux 不同。详情可以看官网docker mac network 在容器内部可以使用 host.docker.internal 就可以访问到 Mac 宿主机啦。 而在 Linux 机器上, 容器可以直接使用宿主机的 IP 来访问宿主机 docker-proxycan refer this (article)[https://mp.weixin.qq.com/s/S4aMdmQW50HR7Lj2z2-VZQ]123# /etc/docker/daemon.json# disable docker-proxy“userland-proxy”:false]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[singtel 路由器踩坑经历]]></title>
    <url>%2Fpost%2Fsingtel%2F</url>
    <content type="text"><![CDATA[2019-07-26 搬新家后贫困山区终于通网了。然而当我打算修改路由器的名称(SSID)和密码的时候，却发现被坑了。 背景介绍singtel 光纤宽带，有一个光模转换器，还有一个 WiFi 路由器。如下图所示： 踩坑过程当家里有网后，我想修改一下 WiFi 名称和密码，谷歌一下，答案如下。Step 1Visit http://192.168.1.254 to view your router configuration page. Step 2Scroll down to Device Info and Internet Connection to find the information. Step 3By default, you will see the 2.4GHz and 5GHz wireless settings, Band Steering and WPS feature.. Step 4To apply any changes made to your Wireless settings, click on the Apply button or cancel to disregard the changes. 我按照以上说明操作一下，结果碰到的却是如下的画面：当时就懵逼了，账号和密码都不知道。结果看到右上角有一个型号 hg8244h, 谷歌一下 hg8244h default password. 结果如下The default network address is 192.168.1.254 and so within a browser connect to http://192.168.1.254. The root user login’s default password is admin and should be changed. 登录进去之后，画面如下:我仔仔细细找了一圈，都没有发现修改 SSID 的地方。当我点开 Lan devices, 列表里发现了一个 Singtel-ACPlus, 我才意识到我登录到光模装换器的控制台了。在这里我看到了 Singtel-ACPlus 的 mac 地址。我在自己的苹果电脑命令行查询一下：1arp -a 得到的网关 192.168.1.254 mac 地址与 Singtel-ACPlus 的 mac 地址不符合 我的电脑连接的是 WiFi 路由器，结果网关却是光模路由器。其中必有蹊跷！ 解决的办法先将光模路由器和 WiFi 路由器之间的网线断开，（一定要）再重启WiFi 路由器。等待一段时间连上 WiFi, 再次访问 http://192.168.1.254. 这次终于大功告成！猜测根本原因是：两个路由器之间是桥接模（也许有错）。 WiFi 路由器连上光模路由器了，都没有自己的 IP。 真是卑微，就像我这种菜菜子，不配拥有姓名。]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[运维中踩过的坑]]></title>
    <url>%2Fpost%2Fops-bug%2F</url>
    <content type="text"><![CDATA[仅以本文记录那些年弟弟背过的锅！ iptables 大部分时候，iptables 只存了 filter 表。 对于 nat 表，我们一旦 restart iptables， nat 表的规则就会被刷新。建议用 reload -A INPUT -m state –state RELATED,ESTABLISHED -j ACCEPT 这一条 iptables 的规则也非常重要。Packets in a RELATED or ESTABLISHED state are those ones which belong to an already opened connection; you’ll generally want to accept them, otherwise connections will get established correctly but nothing will be able to flow after the initial handshake. 如果没有这一条，会遇到 DNS 解析失败， curl 失败。 凡是 iptables 没有允许的 IP, 都不能正常的工作。 例如 DNS 查询发包后，三次握手建立。回包收到了，却会被 iptables 阻挡，上层应用无法拿到解析的结果，导致 hang 住。 tcpdump 抓包分析时， 进入的包都可以抓到，不会受到 iptables, 发出的包会受到 iptables 影响，可能被 iptables 阻挡导致抓包失败。 如果是命令行添加的规则，例如 iptables -t nat -A OUTPUT -s 172.17.0.3 -p tcp –dport 10050 -j ACCEPT 在使用 reload 后，其规则一样会被刷掉。但 docker 的规则不会被 reload 刷掉，会被 restart 刷掉。这一点有疑问，期待大神给弟弟解惑。 docker iptables filter 表 input 链规则失效，原因是：从 NAT 表 prerouting 链匹配到 DOCKER 链，直接与容器沟通，跳过了 input 链。感谢智凯哥哥的解决办法如下：12345678910111213*filter:INPUT DROP [0:0]:FORWARD ACCEPT [0:0]:OUTPUT ACCEPT [0:0]:DOCKER-USER - [0:0]-A FORWARD -j DOCKER-USER-A DOCKER-USER -s x.x.x.x -p tcp --dport 80 -j RETURN-A DOCKER-USER -p tcp --dport 80 -j DROP-A DOCKER-USER -p udp --dport 80 -j DROP-A DOCKER-USER -j RETURNCOMMIT python tab 键与空格不能混用 函数不要嵌套，例如 result=A(B(C())) 这样不利于 debug，检查每个函数的返回值 在 multiprocessing 多进程中 apply_async() 报错不容易定位，最好使用 try catch 把报错写成 log jenkins 在部署的时候，ansible 一直在 gathering facts 卡住了，直到 timeout。 网友解答 我遇到的是 control_path 文件太多了，导致了 jenkins deploy node 卡住，任务无法进行。 需要删除该部署节点下面的 control_path_dir 的文件，清空。 docker docker container IP default is 172.17.0.0/16 检查 iptables 是否阻挡 docker -v 挂载出来的时候，要注意文件夹权限问题。 docker logs 日志文件很大的时候，记得删除。Docker容器日志查看与清理 也可以在 /etc/docker/daemon 里设置 log 的大小。 NetworkManager 报错 bus-manager: could not create org.freedesktop.DBus proxy 直接 stop NetworkManager 就行了。 文件权限问题 /tmp permission 又搞我 drwxrwxrwt. 对于目录文件来说，可读表示能够读取目录内的文件列表；可写表示能够在目录内新增、删除、重命名文件；可执行表示能够进入该目录。 file and directory permission atime, ctime and mtime in Unix filesystems DNS /etc/resolv.conf 有两个默认的值至关重要，一个是超时的 timeout，一个是重试的 attempts，默认情况下，前者是 5s 后者是 2 次。对于日常的应用来说，包括 web server、mail client、db 以及各种 app server 等等，任何使用 glibc resolver 都需要经过 resolv.conf 文件。对于 libresolv 来说，只认 resolv.conf 的前三个 nameserver，所以写的再多也没什么意义。正常情况下，resolver 会从上至下进行解析，每个 nameserver 等待 timeout 的时间，如果一直到第三个都没结果，resolver 会重复上面的步骤 (attempts – 1) 次。 Ansible user 模块，密码必须要加密。需要用到 Ansible ad-hoc command Markdown Markdown链接括号的问题: %28 代替(, %29代替) 主要是后者会歧义链接部分的结束. 这是使用url符号码去代替ascii的符号. 能够解决这个问题 Django 文件名第一个字符为空格，设置 static 时，多了一个空格 %20 ，导致资源路径出出错。 Mac Django 连接 MySQL, 官网上提供了两种方法。第一种 mysqlclient 在 Mac 上有遇到错误。 mysqlclient 官网也有说明, Google一下后发现是因为我的 Mac 没有安装 clang，安装完 xcode-select –install 再 pip install mysqlclient 就可以了。第二种，安装Connector 在 setting.py 里添加1234567891011DATABASES = &#123; 'default': &#123; 'NAME': 'user_data', 'ENGINE': 'mysql.connector.django', 'USER': 'mysql_user', 'PASSWORD': 'password', 'OPTIONS': &#123; 'autocommit': True, &#125;, &#125;&#125; Linux 内存泄漏指由于疏忽或错误造成程序未能释放已经不再使用的内存。内存泄漏并非指内存在物理上的消失，而是应用程序分配某段内存后，由于设计错误，导致在释放该段内存之前就失去了对该段内存的控制，从而造成了内存的浪费。 有一个服务跑在容器里面，当连接的时候总是 time out， 查看日志 是 stream is closed，还有一个错误是 No such file or directory: u’/var/run/tmp/summary_398320.db’ 当我们进入 /var/run/tmp 发现有太多的文件，每一个进程都会有一个 pid 命名的文件。说明 worker 进程不断被 kill 然后又被 master 拉起，当我们到宿主机上看 docker stats ， 发现该容器的内存已经满了，导致了 worker 进程不断被杀。需要增大容器的 memory 初始值。补充（gunicorn 会启动一组 worker进程，所有worker进程公用一组listener，在每个worker中为每个listener建立一个wsgi server。每当有HTTP链接到来时，wsgi server创建一个协程来处理该链接，协程处理该链接的时候，先初始化WSGI环境，然后调用用户提供的app对象去处理HTTP请求。） xargs与管道的区别 其实基本上linux的命令中很多的命令的设计是先从命令行参数中获取参数，然后从标准输入中读取。另外很多程序是不处理标准输入的，例如 kill , rm 这些程序如果命令行参数中没有指定要处理的内容则不会默认从标准输入中读取。所以 xargs 可以用来传递参数 读取超大日志文件如果日志没有做切割，有可能会导致生成一个超大的日志，这个时候对应我们查看日志文件非常的不方便。这个时候有一个方法就是用 split 将大日志均分为小日志12345678910111213#按行数均分split -l 10000 test.log -d -a 4 test_#-l, --lines=NUMBER：对file进行切分，每个文件有NUMBER行。#每个文件10000行(-l 10000)#文件名称后缀系数不是字母而是数字（-d）#后缀系数为四位数（-a 4）# 也可以按文件大小均分split -b test.log -d -a 4 test_#-b, --bytes=SIZE：对file进行切分，每个小文件大小为SIZE。可以指定单位b,k,m,g。 kafka kafka listen all interfacesIn order to listen all the interfaces, comment #host.name in /etc/kafka/server.propertiesThis action can make Kafka listen in all interfaces, but it will change hostname to localhost, then Kafka cannot communicate with each other.Error is the connection with node -1 failed.1Info in zookeeper: "PLAINTEXT://localhost:29092"],"jmx_port":9093,"host":"localhost", solution: Kafka can listen in all interfaces and connection with each other.123listeners=PLAINTEXT://0.0.0.0:29092advertised.host.name=10.66.236.43host.name=10.66.236.43]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>docker</tag>
        <tag>elk</tag>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zabbix 从入门到放弃]]></title>
    <url>%2Fpost%2Fzabbix%2F</url>
    <content type="text"><![CDATA[docker 安装 zabbix, 添加主机，设置报警，性能调优。 监控模式zabbix 有两种监控模式，主动和被动。在客户端与服务端之间还可以加一个 proxy，入下图所示 需要注意 iptables问题: 跨地区监控的时候， proxy 必须监听在所有网卡上。内网是为了和客户端通信，外网是为了和服务端通信。我曾试过 proxy 只监听在内网，因为是主动模式，层层上报信息，在 zabbix server 还是能发现 proxy 的存活。但是当我打算添加一台 host 时，却一直报错。原因就是 proxy 和 服务端是外网通信的，proxy 发包查询 host 的信息（监控项等），因为只监听内网，服务端的回包 proxy 无法获取，导致通信失败。 LAMP 架构安装基于官方的 LAMP 架构，按照最简单的原生方式来部署，不做任何多余优化。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273# 安装必要依赖包 yum install -y httpd mariadb-server mariadb php php-mysql php-gd libjpeg* php-ldap php-odbc php-pear php-xml php-xmlrpc php-mhash# 修改 apache 配置 vim /etc/httpd/conf/httpd.confServerName 192.168.1.10:8080Listen 192.168.1.10:8080DirectoryIndex index.html index.php# 修改 php 时区 vim /etc/php.inidate.timezone = Asia/Singapore# 启动 httpd 服务 systemctl start httpd.service# 修改数据库存储的位置 /datavim /etc/my.cnf[mysqld]datadir=/data/mysqlsocket=/data/mysql/mysql.sockbind-address = 127.0.0.1max_connections = 1000# 最大连接数很关键，如果 zabbix-server StartPollers= 设置过大# 则很容易出现，connection to database &apos;zabbix&apos; failed: [1040] Too many connections[client]port=3306socket=/data/mysql/mysql.sock# 启动 mariadb 服务 systemctl start mariadb.service# 初始化 mysql 数据库，并配置 root 用户密码 mysql_secure_installation# 万一新版本忘记随机密码可以通过日志获取 grep &apos;temporary password&apos; /var/log/mysqld.log# 创建一个测试页，测试 LAMP 是否搭建成功 cat &gt; /var/www/html/index.php &lt;&lt; EOF&lt;?phpphpinfo();?&gt;EOF# 创建 zabbix 数据库 mysql -uroot -pmysql&gt; create database zabbix character set utf8 collate utf8_bin;mysql&gt; grant all privileges on zabbix.* to zabbix@localhost identified by &apos;zabbix&apos;;mysql&gt; quit;# 部署 zabbixrpm -Uvh https://repo.zabbix.com/zabbix/4.2/rhel/7/x86_64/zabbix-release-4.2-2.el7.noarch.rpmyum clean allyum install -y zabbix-server-mysql zabbix-web-mysql zabbix-agentzcat /usr/share/doc/zabbix-server-mysql*/create.sql.gz | mysql -uzabbix -p zabbix# 输入密码 zabbix# 配置数据库用户及密码 vim /etc/zabbix/zabbix_server.confDBPassword=zabbixDBSocket=/data/mysql/mysql.sock# 修改时区 vim /etc/httpd/conf.d/zabbix.confphp_value date.timezone Asia/Singapore# 启动 zabbix 并设置自启动服务 systemctl restart zabbix-server zabbix-agent httpdsystemctl enable zabbix-server zabbix-agent httpd mariadb 一切就绪，打开浏览器，输入 http://ServerName:port/zabbix zabbix_server.conf 参数调优123456CacheSize=8G # Host 过多时，需要增大 CacheSizeTrendCacheSize=2G # __zbx_mem_realloc(): out of memory (requested 108424 bytes) Timeout=30 # __zbx_mem_realloc(): please increase CacheSize configuration parameterStartPollers=500 # Poller 会导致连接数增大。需要调大数据库的最大连接数StartPollersUnreachable=100HousekeepingFrequency=0 docker 搭建12345678910# install docker-cesudo yum install -y yum-utils device-mapper-persistent-data lvm2sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.reposudo yum install -y docker-cesudo systemctl start docker# 做数据映射后的方案 mkdir -p /data/docker/mysql/zabbix/datamkdir -p /data/docker/zabbix/alertscriptsmkdir -p /data/docker/zabbix/externalscripts 然后是安装 zabbix 前端，后端，数据库。123456789# 数据库。docker run --name mysql-server -t \-e MYSQL_DATABASE="zabbix" \-e MYSQL_USER="zabbix" \-e MYSQL_PASSWORD="feiyang@2019+" \-e MYSQL_ROOT_PASSWORD="feiyang@2019+" \-v /data/zabbix_data:/var/lib/mysql \-d mysql:5.7 \--character-set-server=utf8 --collation-server=utf8_bin 12345678910111213141516171819# 后端 参数已经调优docker run --name zabbix-server-mysql \-e DB_SERVER_HOST="mysql-server" \-e MYSQL_DATABASE="zabbix" \-e MYSQL_USER="zabbix" \-e MYSQL_PASSWORD="feiyang@2019+" \-e MYSQL_ROOT_PASSWORD="feiyang@2019+" \-e ZBX_TIMEOUT=30 \-e ZBX_CACHESIZE=8G \-e ZBX_TRENDCACHESIZE=2G \-e ZBX_STARTPOLLERS=500 \-e ZBX_STARTPOLLERSUNREACHABLE=100 \-e ZBX_HOUSEKEEPINGFREQUENCY=0 \-v /data/zabbix/alertscripts:/usr/lib/zabbix/alertscripts \-v /data/zabbix/externalscripts:/usr/lib/zabbix/externalscripts \-v /data/zabbix/conf:/etc/zabbix \--link mysql-server:mysql \-p 10051:10051 \-d zabbix/zabbix-server-mysql:centos-4.2-latest 123456789101112# 前端docker run --name zabbix-web-nginx-mysql \-e DB_SERVER_HOST="mysql-server" \-e MYSQL_DATABASE="zabbix" \-e MYSQL_USER="zabbix" \-e MYSQL_PASSWORD="feiyang@2019+" \-e MYSQL_ROOT_PASSWORD="feiyang@2019+" \-e PHP_TZ="Asia/Singapore" \--link mysql-server:mysql \--link zabbix-server-mysql:zabbix-server \-p 8080:80 \-d zabbix/zabbix-web-nginx-mysql:centos-4.2-latest 安装完成后，在浏览器打开 http://localhost:8080 默认的账户是 Admin 密码是 zabbix ansible 批量添加主机123456789101112131415161718192021222324--- - name: add zabbix hosts local_action: module: zabbix_host server_url: "&#123;&#123; var_server_url &#125;&#125;" login_user: "&#123;&#123; var_login_user &#125;&#125;" login_password: "&#123;&#123; var_login_password &#125;&#125;" host_name: "&#123;&#123; inventory_hostname &#125;&#125;" visible_name: "&#123;&#123; inventory_hostname &#125;&#125;-&#123;&#123;function&#125;&#125;" host_groups: - "&#123;&#123; var_host_group &#125;&#125;" link_templates: - Template Sea Ops OS Linux - Template Sea Ops Disk IO Linux #status: disabled status: enabled state: present interfaces: - type: 1 main: 1 useip: 1 ip: "&#123;&#123; var_lanip | default(inventory_hostname) &#125;&#125;" dns: "" port: 10050 Action设置触发警告的 Action 时，当 Step 设置为从 1 到 0 时，会一直发送告警信息，直到事件状态变成 OK，当 Step 设置为从 1 到 1 时，则只会发送一次告警，后面不会继续发送告警信息。 Zabbix 监控监控网页状态zabbix 自带的 Web monitoring 就可以进行简单的网页监控。目前官方的 zabbix 版本是 4.2 此时日期 2019-07-09首先是找到一台机器 Go to Configuration → Hosts, pick a host and click on Web in the row of that host. Then click on Create web scenario. 详情请看官方文档，然后是添加报警，网页监控的官方文档也是介绍的非常详细。具体的监控图表信息，可以在 zabbix 主页的 Monitoring -&gt; Web 可以看到网页监控的详细信息。 监控 DNS官方文档 4.2 版本zabbix默认支持检查解析成功与否和具体的解析结果。对应内置的KEY123456789net.dns[&lt;ip&gt;,zone,&lt;type&gt;,&lt;timeout&gt;,&lt;count&gt;]net.dns.record[&lt;ip&gt;,zone,&lt;type&gt;,&lt;timeout&gt;,&lt;count&gt;]ip 指DNS服务器地址。zone 指要解析的域名type 指解析的记录类型timeout 指超时时间 默认1 秒count 指解析失败重试的次数 默认 2次trigger &#123;host:net.dns[dns_server,domain,A,1,2].count(#3)&#125;=0 数据库表优化123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176DELIMITER $$CREATE PROCEDURE `partition_create`(SCHEMANAME varchar(64), TABLENAME varchar(64), PARTITIONNAME varchar(64), CLOCK int)BEGIN /* SCHEMANAME = The DB schema in which to make changes TABLENAME = The table with partitions to potentially delete PARTITIONNAME = The name of the partition to create */ /* Verify that the partition does not already exist */ DECLARE RETROWS INT; SELECT COUNT(1) INTO RETROWS FROM information_schema.partitions WHERE table_schema = SCHEMANAME AND table_name = TABLENAME AND partition_description &gt;= CLOCK; IF RETROWS = 0 THEN /* 1. Print a message indicating that a partition was created. 2. Create the SQL to create the partition. 3. Execute the SQL from #2. */ SELECT CONCAT( "partition_create(", SCHEMANAME, ",", TABLENAME, ",", PARTITIONNAME, ",", CLOCK, ")" ) AS msg; SET @sql = CONCAT( 'ALTER TABLE ', SCHEMANAME, '.', TABLENAME, ' ADD PARTITION (PARTITION ', PARTITIONNAME, ' VALUES LESS THAN (', CLOCK, '));' ); PREPARE STMT FROM @sql; EXECUTE STMT; DEALLOCATE PREPARE STMT; END IF;END$$DELIMITER ;DELIMITER $$CREATE PROCEDURE `partition_drop`(SCHEMANAME VARCHAR(64), TABLENAME VARCHAR(64), DELETE_BELOW_PARTITION_DATE BIGINT)BEGIN /* SCHEMANAME = The DB schema in which to make changes TABLENAME = The table with partitions to potentially delete DELETE_BELOW_PARTITION_DATE = Delete any partitions with names that are dates older than this one (yyyy-mm-dd) */ DECLARE done INT DEFAULT FALSE; DECLARE drop_part_name VARCHAR(16); /* Get a list of all the partitions that are older than the date in DELETE_BELOW_PARTITION_DATE. All partitions are prefixed with a "p", so use SUBSTRING TO get rid of that character. */ DECLARE myCursor CURSOR FOR SELECT partition_name FROM information_schema.partitions WHERE table_schema = SCHEMANAME AND table_name = TABLENAME AND CAST(SUBSTRING(partition_name FROM 2) AS UNSIGNED) &lt; DELETE_BELOW_PARTITION_DATE; DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = TRUE; /* Create the basics for when we need to drop the partition. Also, create @drop_partitions to hold a comma-delimited list of all partitions that should be deleted. */ SET @alter_header = CONCAT("ALTER TABLE ", SCHEMANAME, ".", TABLENAME, " DROP PARTITION "); SET @drop_partitions = ""; /* Start looping through all the partitions that are too old. */ OPEN myCursor; read_loop: LOOP FETCH myCursor INTO drop_part_name; IF done THEN LEAVE read_loop; END IF; SET @drop_partitions = IF(@drop_partitions = "", drop_part_name, CONCAT(@drop_partitions, ",", drop_part_name)); END LOOP; IF @drop_partitions != "" THEN /* 1. Build the SQL to drop all the necessary partitions. 2. Run the SQL to drop the partitions. 3. Print out the table partitions that were deleted. */ SET @full_sql = CONCAT(@alter_header, @drop_partitions, ";"); PREPARE STMT FROM @full_sql; EXECUTE STMT; DEALLOCATE PREPARE STMT; SELECT CONCAT(SCHEMANAME, ".", TABLENAME) AS `table`, @drop_partitions AS `partitions_deleted`; ELSE /* No partitions are being deleted, so print out "N/A" (Not applicable) to indicate that no changes were made. */ SELECT CONCAT(SCHEMANAME, ".", TABLENAME) AS `table`, "N/A" AS `partitions_deleted`; END IF;END$$DELIMITER ;DELIMITER $$CREATE PROCEDURE `partition_maintenance`(SCHEMA_NAME VARCHAR(32), TABLE_NAME VARCHAR(32), KEEP_DATA_DAYS INT, HOURLY_INTERVAL INT, CREATE_NEXT_INTERVALS INT)BEGIN DECLARE OLDER_THAN_PARTITION_DATE VARCHAR(16); DECLARE PARTITION_NAME VARCHAR(16); DECLARE OLD_PARTITION_NAME VARCHAR(16); DECLARE LESS_THAN_TIMESTAMP INT; DECLARE CUR_TIME INT; CALL partition_verify(SCHEMA_NAME, TABLE_NAME, HOURLY_INTERVAL); SET CUR_TIME = UNIX_TIMESTAMP(DATE_FORMAT(NOW(), '%Y-%m-%d 00:00:00')); SET @__interval = 1; create_loop: LOOP IF @__interval &gt; CREATE_NEXT_INTERVALS THEN LEAVE create_loop; END IF; SET LESS_THAN_TIMESTAMP = CUR_TIME + (HOURLY_INTERVAL * @__interval * 3600); SET PARTITION_NAME = FROM_UNIXTIME(CUR_TIME + HOURLY_INTERVAL * (@__interval - 1) * 3600, 'p%Y%m%d%H00'); IF(PARTITION_NAME != OLD_PARTITION_NAME) THEN CALL partition_create(SCHEMA_NAME, TABLE_NAME, PARTITION_NAME, LESS_THAN_TIMESTAMP); END IF; SET @__interval=@__interval+1; SET OLD_PARTITION_NAME = PARTITION_NAME; END LOOP; SET OLDER_THAN_PARTITION_DATE=DATE_FORMAT(DATE_SUB(NOW(), INTERVAL KEEP_DATA_DAYS DAY), '%Y%m%d0000'); CALL partition_drop(SCHEMA_NAME, TABLE_NAME, OLDER_THAN_PARTITION_DATE);END$$DELIMITER ;DELIMITER $$CREATE PROCEDURE `partition_verify`(SCHEMANAME VARCHAR(64), TABLENAME VARCHAR(64), HOURLYINTERVAL INT(11))BEGIN DECLARE PARTITION_NAME VARCHAR(16); DECLARE RETROWS INT(11); DECLARE FUTURE_TIMESTAMP TIMESTAMP; /* * Check if any partitions exist for the given SCHEMANAME.TABLENAME. */ SELECT COUNT(1) INTO RETROWS FROM information_schema.partitions WHERE table_schema = SCHEMANAME AND table_name = TABLENAME AND partition_name IS NULL; /* * If partitions do not exist, go ahead and partition the table */ IF RETROWS = 1 THEN /* * Take the current date at 00:00:00 and add HOURLYINTERVAL to it. This is the timestamp below which we will store values. * We begin partitioning based on the beginning of a day. This is because we don't want to generate a random partition * that won't necessarily fall in line with the desired partition naming (ie: if the hour interval is 24 hours, we could * end up creating a partition now named "p201403270600" when all other partitions will be like "p201403280000"). */ SET FUTURE_TIMESTAMP = TIMESTAMPADD(HOUR, HOURLYINTERVAL, CONCAT(CURDATE(), " ", '00:00:00')); SET PARTITION_NAME = DATE_FORMAT(CURDATE(), 'p%Y%m%d%H00'); -- Create the partitioning query SET @__PARTITION_SQL = CONCAT("ALTER TABLE ", SCHEMANAME, ".", TABLENAME, " PARTITION BY RANGE(`clock`)"); SET @__PARTITION_SQL = CONCAT(@__PARTITION_SQL, "(PARTITION ", PARTITION_NAME, " VALUES LESS THAN (", UNIX_TIMESTAMP(FUTURE_TIMESTAMP), "));"); -- Run the partitioning query PREPARE STMT FROM @__PARTITION_SQL; EXECUTE STMT; DEALLOCATE PREPARE STMT; END IF;END$$DELIMITER ;DELIMITER $$CREATE PROCEDURE`partition_maintenance_all`(SCHEMA_NAME VARCHAR(32))BEGIN CALL partition_maintenance(SCHEMA_NAME, 'history', 30, 24, 14); CALL partition_maintenance(SCHEMA_NAME, 'history_log', 30, 24, 14); CALL partition_maintenance(SCHEMA_NAME, 'history_str', 30, 24, 14); CALL partition_maintenance(SCHEMA_NAME, 'history_text', 30, 24, 14); CALL partition_maintenance(SCHEMA_NAME, 'history_uint', 30, 24, 14); CALL partition_maintenance(SCHEMA_NAME, 'trends', 120, 24, 14); CALL partition_maintenance(SCHEMA_NAME, 'trends_uint', 120, 24, 14);END$$DELIMITER ; Trends 120,(‘history’, 30, 24, 14), 最多保存 30 天的数据，每隔 24 小时生成一个分区，每次生成 14 个分区 首先进入容器内部，将上面这个 partition.sql 导入数据库 mysql12345678mysql -uzabbix -pfeiyang@2019+ zabbix &lt; partition.sql# 在 mysql 容器内部 vim /opt/mysql.sh#!bin/bashmysql -uzabbix -pfeiyang@2019+ zabbix -e"CALL partition_maintenance_all('zabbix')" chmod 755 /opt/mysql.sh 退出容器，在宿主机上，建立定时任务 12345# vim /etc/crontab23 03 * * * root /bin/docker exec [mysql 容器 ID] bash -c "cd /opt &amp;&amp; bash mysql.sh" systemctl restart crond Zabbix api官方文档 123456789101112131415161718192021222324252627282930# auth_zabbiximport requestsimport jsonurl = 'http://IP:port/api_jsonrpc.php' #docker 方式# 非 docker 方式为 "http://IP:port/zabbix/api_jsonrpc.php"post_data = &#123; "jsonrpc": "2.0", "method": "user.login", "params": &#123; "user": "xxx", "password": "xxx" &#125;, "id": 1, &#125;post_header = &#123;'Content-Type': 'application/json'&#125;ret = requests.post(url, data=json.dumps(post_data), headers=post_header)#print(ret)zabbix_ret = json.loads(ret.text)if not zabbix_ret.has_key('result'): print 'login error'else: print zabbix_ret.get('result') 123456789101112131415161718192021222324252627282930313233343536373839404142# get hostidimport requestsimport jsonurl = 'http://IP:port/api_jsonrpc.php'server_list=["1.1.1.1","233.233.233.233"]post_data = &#123; "jsonrpc": "2.0", "method": "host.get", "params": &#123; "filter": &#123; "host": server_list &#125;, "sortfield": "host", &#125;, "id": 1, "auth": "由上文中的 auth_zabbix.py 得出"&#125;post_header = &#123;'Content-Type': 'application/json'&#125;ret = requests.post(url, data=json.dumps(post_data), headers=post_header)zabbix_ret = json.loads(ret.text)print zabbix_retif not zabbix_ret.has_key('result'): print 'login error'else: print zabbix_ret.get('result') hostid_list=[]for i in zabbix_ret.get('result'): hostid_list.append(str(i['hostid']))print hostid_list 123456789101112131415161718192021222324252627282930313233343536373839404142# get hist_dataimport requestsimport jsonimport timeimport datetimetoday = datetime.date.today()today_unix = int(time.mktime(today.timetuple()))tomorrow = today+datetime.timedelta(days=1)tomorrow_unix = int(time.mktime(tomorrow.timetuple()))print today_unixprint tomorrow_unixurl = 'http://IP:port/api_jsonrpc.php'post_data = &#123; "jsonrpc": "2.0", "method": "history.get", "params": &#123; "output": "extend", "history": 3, # 0,1,2,3,4 History object types "itemids": "31023", "sortfield": "clock", "sortorder": "DESC", "time_from": "today_unix", "time_till": "tomorrow_unix" &#125;, "auth": "由上文中的 auth_zabbix.py 得出", "id": 1&#125;post_header = &#123;'Content-Type': 'application/json'&#125;ret = requests.post(url, data=json.dumps(post_data), headers=post_header)zabbix_ret = json.loads(ret.text)print zabbix_ret.get('result') 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# get trend_dataimport requestsimport jsonimport timeimport datetimetoday = datetime.date.today()today_unix = int(time.mktime(today.timetuple()))tomorrow = today+datetime.timedelta(days=1)tomorrow_unix = int(time.mktime(tomorrow.timetuple()))url = 'http://IP:port/api_jsonrpc.php'post_data = &#123; "jsonrpc": "2.0", "method": "trend.get", "params": &#123; "output": [ # 定义 output 格式 "itemid", "clock", # 当前时间 "num", # trend 一小时采集次数 "value_min", "value_avg", "value_max" ], "itemids": [ "28959", "28972" ], "time_from": today_unix, "time_till": tomorrow_unix &#125;, "auth": "由上文中的 auth_zabbix.py 得出", "id": 1&#125;post_header = &#123;'Content-Type': 'application/json'&#125;ret = requests.post(url, data=json.dumps(post_data), headers=post_header)zabbix_ret = json.loads(ret.text)print (zabbix_ret.get('result')) zabbix_get从 server 端检测到 client 端的网络是否通畅，可能是 iptables 或者 server host 白名单造成的问题。1zabbix_get -s 10.10.1.1 -k system.uname zabbix proxy如果 server cluster 规模不大，则我们可以采用被动模式（对 client 而言是被动的，被 zabbix server来拉取数据）。如果集群规模大，那么 zabbix server 的压力就很大了，如果要去抓取的客户端数量过于庞大。 还有一种建议用 proxy 模式的情况是跨地区监控，集中到一台中心 zabbix， 方便管理。本次演示，我们用了三台机器，client: 10.66.236.98 , proxy: 10.66.236.99 , zabbix_server: 10.66.236.100 （可用上文的 docker 方式安装），非 docker 方式可用参考 奥哥博客 首先是安装 zabbix proxy 在 10.66.236.99 123456789101112131415161718192021222324252627282930313233343536373839# zabbix proxy 的依赖就只有数据库了，用于存储配置信息 yum install -y mariadb-server mariadb# 启动 mariadb 服务 systemctl start mariadb.servicesystemctl enable mariadb.service# 初始化 mysql 数据库，并配置 root 用户密码 mysql_secure_installation# 初始化，然后输入密码# 创建 zabbix_proxy 数据库 mysql -uroot -pmysql&gt; create database zabbix_proxy character set utf8 collate utf8_bin;mysql&gt; grant all privileges on zabbix_proxy.* to zabbix@localhost identified by 'zabbix';mysql&gt; quit;# 部署 zabbix_proxy 4.2 版本的rpm -Uvh https://repo.zabbix.com/zabbix/4.2/rhel/7/x86_64/zabbix-release-4.2-2.el7.noarch.rpmyum install -y zabbix-proxy-mysqlzcat /usr/share/doc/zabbix-proxy-mysql*/schema.sql.gz | mysql -uzabbix -p zabbix_proxy#输入密码: zabbix# 配置数据库用户及密码 vim /etc/zabbix/zabbix_proxy.confServer=10.66.236.100 #填写 zabbix server 的IPHostname=zabbix_proxyDBName=zabbix_proxyDBUser=zabbixDBPassword=zabbix# 网页上配置 Zabbix Server ProxyAdministration -&gt; Proxies -&gt; Create proxyProxy name: zabbix_proxyProxy mode: ActiveProxy address: 10.66.236.99# 启动 zabbix_proxysystemctl restart zabbix-proxy 如果是 Active 模式，这里不需要填写 IP， Passive 被动模式才一定要填写 IP 修改 client 端的配置 1234567# 修改 zabbix_agent 配置指向 zabbix_proxyvim /etc/zabbix/zabbix_agentd.confServerActive=10.66.236.99 #proxy 的 IPHostname=10.66.236.98 #本机的 IP# 重启 zabbix_agentsystemctl restart zabbix-agent 在网页上添加 host 一定要选择 proxy，IP 就填写 0.0.0.0 还有一个非常重要的就是 template 所有的 item 需要采用 Zabbix agent (active) 遇到的坑 在测试 proxy 时，host name 不匹配造成的，我最开始随便填了一个 hostname ， 然后发现和 agent config 不一致，又在zabbix 网页上更新为 10.66.236.98 ，感觉数据库并没有更新，所以导致 host name 匹配不了，一直报错 no active checks on server no active checks on server [10.66.236.99:10051]: host [10.66.236.98] not found解决的办法是：重新删除了 host , 再添加上 ，一次性填对了 host name 10.66.236.98 就可以了]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络相关]]></title>
    <url>%2Fpost%2Fnetwork%2F</url>
    <content type="text"><![CDATA[记录网络相关知识与 Linux 网络相关的命令。 Linux 命令 route 某些 IP 段故障 mtr 连通性测试 Linux 常用网络指令 iptables 奥哥篇 iptables-NAT iperf 测试网速的工具 删除网卡 ip link delete [网卡名称] 检查未被正确关闭的文件 lsof 或者 linux中如何解决文件已删除但空间不释放的案例 磁盘监控命令iostat 高 disk IO 检查: iotop -oP 看哪个进程占用大量的 disk IO 查看链接状态 12345678netstat -n | awk '/^tcp/ &#123;++state[$NF]&#125; END &#123;for(key in state) print key,"\t",state[key]&#125;'# 如下结果FIN_WAIT2 5683SYN_RECV 311CLOSE_WAIT 3TIME_WAIT 127952ESTABLISHED 52032FIN_WAIT1 22345 What is a TCP Reset RST iptables1234567891011iptables -t nat -nL # 查看 nat 表iptables -t 表名 -N 自定义链名 # 创建一个链iptables -t 表名 -L default filteriptables -t 表名 -L 链名iptables -t 表名 -nL --lineiptables -t 表名 -D 链名 要删除的序号iptables -t 表名 -P 链名 动作 修改默认规则 DROP-A INPUT -m iprange --src-range x.x.x.x-x.x.x.x -p tcp --dport 11211 -j ACCEPT #多 IP-A INPUT -p tcp -m multiport --dports 80,443 -j ACCEPT #多端口类型匹配 -p tcp udp udplite icmp icmpv6 esp ah sctp mh转发功能 cat /proc/sys/net/ipv4/ip_forward 小知识Public IP Class A: 0.x.x.x ~ 127.x.x.x Class B: 128.x.x.x ~ 191.x.x.x Class C: 192.x.x.x ~ 223.x.x.x Class D: 224.x.x.x ~ 239.x.x.x #multicast Class E: 240.x.x.x ~ 255.x.x.x #保留 Private IP Class A: 10.0.0.0 ~ 10.255.255.255 Class B: 172.16.0.0 ~ 172.31.255.255 Class C: 192.168.0.0 ~ 192.168.255.255 169.254.x.x 临时 IP DHCP is full. 就用这个 IP Loopback IP Class A: 127.0.0.1/8 设置 NAT server开启转发功能12345vim /etc/sysctl.confnet.ipv4.ip_forward=1 # 添加此行，开启转发功能sysctl -p # 执行生效 还需要在 iptable 里设置转发规则12345678910111213vim /etc/sysconfig/iptables*nat:PREROUTING ACCEPT [0:0]:INPUT ACCEPT [0:0]:OUTPUT ACCEPT [0:0]:POSTROUTING ACCEPT [0:0]-A POSTROUTING -o em2 -j MASQUERADECOMMIT# em2 是公网网卡，当其他内网机器设置 NAT 机器的内网 IP 为网关时。 # 内网机器发包给 NAT 机器， NAT 机器根据路由规则，将会由 em2 公网网卡转发出去。# 转发时，会将包的源 IP 替换为自己公网的 IP 网络扫描安装 nmap 官网12345678910111213141516171819202122232425262728293031rpm -vhU https://nmap.org/dist/nmap-7.80-1.x86_64.rpm# yum install nmap 版本比较老#主机发现 ICMP ARP 两种方式nmap -v -n -sn -PE 192.168.21.0/24# -v 指定详细输出# -n 不进行 DNS 解析# -sn 使用 ping 扫描，禁用端口扫描# -PE 指定使用 ICMP Echo Request 发现主机# 192.168.21.0/24 为目标网段# -PR 指定使用 ARP Request 发现主机# TCP Connect 端口扫描nmap -v -n -sT --max-retries 1 -p1-65535 192.168.21.19# -sT 使用 TCP Connect # --max-retries 每个端口最多重试的次数# -p1-65535 指定端口的扫描范围# 192.168.21.19 为被扫描主机的 IP# TCP SYN 端口扫描,导致服务器出现大量的半连接nmap -v -n -sS --max-retries 1 -p1-65535 192.168.21.19# -sS 使用 TCP SYN # UDP 扫描nmap -v -n -sU --max-retries 1 -p1-65535 192.168.21.19# -sU 使用 UDP 进行扫描# 识别指定端口应用nmap -v -n -sV -p2333 192.168.21.19# -sV 指定识别该端口上的应用 tcpdump 抓包分析 进入 INPUT 的流量不会被 iptable 影响 出口 OUTPIT 流量会受到 iptable 影响 ICMP ping 默认发 4、5 个包 traceroute 显示的是不同 AS 之间的跳数。其实一个 AS 内部可能有很多路由器，TTL 与实际跳数是不符合的。 TCP 三次握手建立连接 四次分手，等待 2 MSL 只走一条路径 详细介绍 UDP 更轻更快 多条路径同时发送 DDos 攻击DDOS 攻击，它在短时间内发起大量请求，耗尽服务器的资源，无法响应正常的访问，造成网站实质下线。DDOS 里面的 DOS 是 denial of service（停止服务）的缩写，表示这种攻击的目的，就是使得服务中断。最前面的那个 D 是 distributed （分布式），表示攻击不是来自一个地方，而是来自四面八方，因此更难防。 比较常见的一种攻击是 cc 攻击。它就是简单粗暴地送来大量正常的请求，超出服务器的最大承受量，导致宕机。 SYN攻击属于DoS攻击的一种，它利用TCP协议缺陷，通过发送大量的半连接请求，耗费CPU和内存资源。SYN攻击除了能影响主机外，还可以危害路由器、防火墙等网络系统，事实上SYN攻击并不管目标是什么系统，只要这些系统打开TCP服务就可以实施。服务器接收到连接请求（syn= j），将此信息加入未连接队列，并发送请求包给客户（syn=k,ack=j+1），此时进入SYN_RECV状态。当服务器未收到客户端的确认包时，重发请求包，一直到超时，才将此条目从未连接队列删除。配合IP欺骗，SYN攻击能达到很好的效果，通常，客户端在短时间内伪造大量不存在的IP地址，向服务器不断地发送syn包，服务器回复确认包，并等待客户的确认，由于源地址是不存在的，服务器需要不断的重发直至超时，这些伪造的SYN包将长时间占用未连接队列，正常的SYN请求被丢弃，目标系统运行缓慢，严重者引起网络堵塞甚至系统瘫痪。 踩过的坑 docker container IP default is 172.17.0.0/16 检查 iptables 是否阻挡 -A INPUT -m state –state RELATED,ESTABLISHED -j ACCEPT 这一条 iptables 的规则也非常重要。Packets in a RELATED or ESTABLISHED state are those ones which belong to an already opened connection; you’ll generally want to accept them, otherwise connections will get established correctly but nothing will be able to flow after the initial handshake. 如果没有这一条，会遇到 DNS 解析失败， curl 失败。 凡是 iptables 没有允许的 IP, 都不能正常的工作。 例如 DNS 查询发包后，三次握手建立。回包收到了，却会被 iptables 阻挡，上层应用无法拿到解析的结果，导致 hang 住。 抓包分析时， 进入的包都可以抓到，不会受到 iptables, 发出的包会受到 iptables 影响，可能被 iptables 阻挡导致抓包失败。 在此衷心的感谢，皇族后裔，八旗子弟，爱新觉罗·高Li，提供的帮助！ IP 冲突，导致服务不稳定。需要使用 arping -c 3 -I em2 192.168.1.1 docker 服务失败导致 telnet 不通，抓包表现为 宿主机收到了 telnet 的包，但不会转发给容器内部。123456789101112131415161718192021[root@localhost feiyang]# tcpdump -i any port 9200 -nnntcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes21:44:12.657584 IP 192.168.1.78.12118 &gt; 192.168.1.89.9200: Flags [S], seq 2957511281, win 29200, options [mss 1460,nop,nop,sackOK,nop,wscale 7], length 021:44:12.657654 IP 192.168.1.78.12118 &gt; 172.17.0.2.9200: Flags [S], seq 2957511281, win 29200, options [mss 1460,nop,nop,sackOK,nop,wscale 7], length 021:44:12.657658 IP 192.168.1.78.12118 &gt; 172.17.0.2.9200: Flags [S], seq 2957511281, win 29200, options [mss 1460,nop,nop,sackOK,nop,wscale 7], length 021:44:12.657711 IP 172.17.0.2.9200 &gt; 192.168.1.78.12118: Flags [S.], seq 3836648310, ack 2957511282, win 29200, options [mss 1460,nop,nop,sackOK,nop,wscale 7], length 021:44:12.657711 IP 172.17.0.2.9200 &gt; 192.168.1.78.12118: Flags [S.], seq 3836648310, ack 2957511282, win 29200, options [mss 1460,nop,nop,sackOK,nop,wscale 7], length 021:44:12.657727 IP 192.168.1.89.9200 &gt; 192.168.1.78.12118: Flags [S.], seq 3836648310, ack 2957511282, win 29200, options [mss 1460,nop,nop,sackOK,nop,wscale 7], length 021:44:12.657913 IP 192.168.1.78.12118 &gt; 192.168.1.89.9200: Flags [.], ack 1, win 229, length 021:44:12.657921 IP 192.168.1.78.12118 &gt; 172.17.0.2.9200: Flags [.], ack 1, win 229, length 021:44:12.657922 IP 192.168.1.78.12118 &gt; 172.17.0.2.9200: Flags [.], ack 1, win 229, length 021:45:31.315560 IP 192.168.1.78.12118 &gt; 192.168.1.89.9200: Flags [F.], seq 1, ack 1, win 229, length 021:45:31.315596 IP 192.168.1.78.12118 &gt; 172.17.0.2.9200: Flags [F.], seq 1, ack 1, win 229, length 021:45:31.315598 IP 192.168.1.78.12118 &gt; 172.17.0.2.9200: Flags [F.], seq 1, ack 1, win 229, length 021:45:31.315985 IP 172.17.0.2.9200 &gt; 192.168.1.78.12118: Flags [F.], seq 1, ack 2, win 229, length 021:45:31.315985 IP 172.17.0.2.9200 &gt; 192.168.1.78.12118: Flags [F.], seq 1, ack 2, win 229, length 021:45:31.316012 IP 192.168.1.89.9200 &gt; 192.168.1.78.12118: Flags [F.], seq 1, ack 2, win 229, length 021:45:31.316225 IP 192.168.1.78.12118 &gt; 192.168.1.89.9200: Flags [.], ack 2, win 229, length 021:45:31.316277 IP 192.168.1.78.12118 &gt; 172.17.0.2.9200: Flags [.], ack 2, win 229, length 021:45:31.316279 IP 192.168.1.78.12118 &gt; 172.17.0.2.9200: Flags [.], ack 2, win 229, length 0 ubuntu network config12345678910111213141516171819202122232425262728293031323334353637# /etc/network/interfacesauto loiface lo inet loopbackauto eno1iface eno1 inet manual bond-master bond0auto eno2iface eno2 inet manual bond-master bond0auto bond0iface bond0 inet manual bond-slaves eno1 eno2 bond-mode 4 bond-miimon 100 bond-lacp-rate 1 bond-xmit_hash_policy layer3+4auto bond0.1000iface bond0.1000 inet static address 10.1.1.10 netmask 255.255.255.0 gateway 10.1.1.254 vlan-raw-device bond0#----------------------------------#ubuntu default# The loopback network interfaceauto loiface lo inet loopback# The primary network interfaceauto ens33iface ens33 inet dhcp]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Google Sheet API 学习]]></title>
    <url>%2Fpost%2Fgoogle-api%2F</url>
    <content type="text"><![CDATA[Python 修改 Google sheet官方文档记录一下自己调用 Google Api 的方法。 几个重要的概念 spreadsheetId 整个总表的 ID 是很长的一串字符 sheetId 单页的 ID 是纯数字 Get获取数据get 方法 1234567SAMPLE_SPREADSHEET_ID = spreadsheetIdSAMPLE_RANGE_NAME = 'feiyang!G1:G4'sheet = service.spreadsheets()result = sheet.values().get(spreadsheetId=SAMPLE_SPREADSHEET_ID, range=SAMPLE_RANGE_NAME).execute()values = result.get('values', []) Append Data123456789101112range_ = 'capacity-raw!A:E' # 表内的页名称 ! 范围value_input_option = 'USER_ENTERED' insert_data_option = 'INSERT_ROWS' value_range_body = &#123; "range": "capacity-raw!A:E","values": getdata.get_data(today,product),"majorDimension": "ROWS"&#125;request = service.spreadsheets().values().append(spreadsheetId=spreadsheet_id, range=range_, valueInputOption=value_input_option, insertDataOption=insert_data_option, body=value_range_body)response = request.execute() Update Data举个例子 1234567891011121314151617SAMPLE_SPREADSHEET_ID = 'xxxxxxxxx'SAMPLE_RANGE_NAME = 'daily_report!A9:D9'value_input_option = "RAW" # 还有其他的方式value_body = &#123; "majorDimension": "ROWS", "range": "daily_report!A9:D9", "values": [["test","123","a","b"]],&#125;sheet = service.spreadsheets()result = sheet.values().update(spreadsheetId=SAMPLE_SPREADSHEET_ID, range=SAMPLE_RANGE_NAME, valueInputOption=value_input_option,body=value_body)response = result.execute()pprint(response) Sheet Operation删除行，插入行，复制一行，最重要的是 post body 格式。 官方文档写得不够详细。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455delete_body =&#123; "requests": [ &#123; "deleteDimension": &#123; "range": &#123; "sheetId": 233333333, "dimension": "ROWS", "startIndex": 1, "endIndex": 2 &#125; &#125; &#125;, ],&#125;insert_body =&#123; "requests": [ &#123; "insertDimension": &#123; "range": &#123; "sheetId": 233333333, "dimension": "ROWS", "startIndex": 8, "endIndex": 9 &#125; &#125; &#125;, ],&#125;copy_body =&#123; "requests": [ &#123; "copyPaste": &#123; "source": &#123; "sheetId": 233333333, "startRowIndex": 6, "endRowIndex": 7, "startColumnIndex": 1, "endColumnIndex": 5 &#125;, "destination": &#123; "sheetId": 1172952310, "startRowIndex": 7, "endRowIndex": 8, "startColumnIndex": 1, "endColumnIndex": 5 &#125;, "pasteType": "PASTE_NORMAL", "pasteOrientation": "NORMAL" &#125; &#125; ]&#125; 然后是 Python post 部分123request = sheet.batchUpdate(spreadsheetId=SAMPLE_SPREADSHEET_ID, body=body_item)response = request.execute()pprint(response)]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>google</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Prometheus 监控 memcached]]></title>
    <url>%2Fpost%2Fmemcached%2F</url>
    <content type="text"><![CDATA[本文记录如何用安装 memcached_exporter 来收集 memcached 信息并且暴露给 Prometheus 监听程序，Prometheus 将收集的信息传递给 grafana 进行信息可视化。 安装 Memcached Exporterprometheus 官方的 memcached_exporter 文档 bridge 桥接方式在 192.168.21.16 服务器上运行了三个 memcached 端口分别为 11211:11213 目前官方的这个版本还不支持多个地址，社区的解决方案点这里12345docker run -d -p 9211:9150 --name=memcached_11211 quay.io/prometheus/memcached-exporter:v0.5.0 --memcached.address=192.168.21.16:11211docker run -d -p 9212:9150 --name=memcached_11212 quay.io/prometheus/memcached-exporter:v0.5.0 --memcached.address=192.168.21.16:11212docker run -d -p 9213:9150 --name=memcached_11213 quay.io/prometheus/memcached-exporter:v0.5.0 --memcached.address=192.168.21.16:11213 在这里我们启动了三个 docker container 用的是 bridge 网络方式来分别监听 11211–11213 需要注意的是 memcached.address 默认监听的是 localhost:11211 如果是 bridge 方式的话，用默认的方法 localhost 只能监听到容器内部。 host 网络方式如果服务器只有一个 memcached 进程的话，那么我们可以用 host 网络的方式。 容器和服务器共享网络，优点是网络高性能，缺点就是需要注意端口冲突。1docker run --network=host --name=memcached_11211 quay.io/prometheus/memcached-exporter:v0.5.0 --memcached.address=localhost:11211 注意 iptable一旦使用了 docker 我们需要特别注意的就是 iptable -A INPUT -s 172.16.0.0/12 -j DROP #检查iptables filter 表 INPUT 链是否阻止了docker container IP，因为 docker 默认 IP 是 172.17.0.0/24， -A INPUT -s 172.17.0.0/24 -p tcp –dport 11211:11213 -j ACCEPT #若采用 bridge 桥接方式， 需要允许容器连接到 memcached -A INPUT -s Prometheus_IP -p tcp –dport 9211:9213 -j ACCEPT #给 Prometheus 开放监听的白名单 检查 memcached_exporter 结果1234567891011121314151617181920212223242526curl 172.17.0.2:9150/metrics #直接访问容器内部curl localhost:9211/metrics # 从docker暴露出来的端口访问# 结果中的字段在 grafana 设置图表时，相关的图表就要用对应的字段# 比如当前连接上 (memcached_current_connections&#123;instance=~"$node"&#125;) # TYPE memcached_connections_listener_disabled_total countermemcached_connections_listener_disabled_total 0# HELP memcached_connections_total Total number of connections opened since the server started running.# TYPE memcached_connections_total countermemcached_connections_total 255174# HELP memcached_connections_yielded_total Total number of connections yielded running due to hitting the memcached's -R limit.# TYPE memcached_connections_yielded_total countermemcached_connections_yielded_total 0# HELP memcached_current_bytes Current number of bytes used to store items.# TYPE memcached_current_bytes gaugememcached_current_bytes 2.57801625e+08# HELP memcached_current_connections Current number of open connections.# TYPE memcached_current_connections gaugememcached_current_connections 663# HELP memcached_current_items Current number of items stored by this instance.# TYPE memcached_current_items gaugememcached_current_items 1.117251e+06# HELP memcached_items_evicted_total Total number of valid items removed from cache to free memory for new items.# TYPE memcached_items_evicted_total countermemcached_items_evicted_total 0 如果看到输出的结果，那说明 memcached_exporter 已经收集到 memcached 的信息并将此暴露出来了。memcached 一些字段的含义 常见错误 配置错误 connection failed，注意地址 –memcached.address=192.168.21.16:11212 启动新的容器失败，地址端口占用，需要重启一下docker iptables 一般 reload， restart 会刷新 NAT 表，导致 docker 路由失败。这种情况需要重启 docker， docker 会在 NAT 表添加路由 Grafana 数据可视化grafana 官方文档，添加数据源，模板。prometheus function 函数在画图时非常重要12# 每分钟 command 的数量 sum(rate(memcached_commands_total&#123;instance=~"$node"&#125;[1m])) by (command)]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK 日志收集系统快速搭建]]></title>
    <url>%2Fpost%2Felk%2F</url>
    <content type="text"><![CDATA[ELK 官方文档 是一个分布式、可扩展、实时的搜索与数据分析引擎。目前我在工作中只用来收集 server 的 log, 开发锅锅们 debug 的好助手。 参考文章 腾讯云Elasticsearch Service 这个腾讯云的专栏非常的不错，请您一定要点开看一眼，总有你想要的。 ELK重难点总结和整体优化配置 安装设置单节点 ELK如果你想快速的搭建单节点 ELK, 那么使用 docker 方式肯定是你的最佳选择。使用三合一的镜像，文档详情注意：安装完 docker, 记得设置 mmap counts 大小至少 262144什么是 mmap12345678910111213141516171819# 设置 mmap 命令 二选一# 一临时添加法sysctl -w vm.max_map_count=262144 # 二永久写入文件vim /etc/sysctl.confvm.max_map_count=262144 # 保存好文件执行以下命令查看sysctl -p# 安装 docker 基于centossudo yum install -y yum-utils device-mapper-persistent-data lvm2sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.reposudo yum install -y docker-cesudo systemctl start docker 单节点的机器，不必暴露 9200(Elasticsearch JSON interface) 和 9300(Elasticsearch transport interface) 端口。如果想在 docker 上暴露端口，用 -p 如果没有填写监听的地址，默认是 0.0.0.0 所有的网卡。建议还是写明确监听的地址，安全性更好。12-p 监听的IP:宿主机端口:容器内的端口-p 192.168.10.10:9300:9300 命令行启动一个 ELK12345sudo docker run -p 5601:5601 -p 5044:5044 \-v /data/elk-data:/var/lib/elasticsearch \-v /data/elk/logstash:/etc/logstash/conf.d \-it -e TZ="Asia/Singapore" -e ES_HEAP_SIZE="20g" \-e LS_HEAP_SIZE="10g" --name elk-ubuntu sebp/elk 将配置和数据挂载出来，即使 docker container 出现了问题。可以立即销毁再重启一个，服务受影响的时间很短。123456# 注意挂载出来的文件夹的权限问题chmod 755 /data/elk-data chmod 755 /data/elk/logstashchown -R root:root /data -v /data/elk-data:/var/lib/elasticsearch # 将 elasticsearch 存储的数据挂载出来，数据持久化。-v /data/elk/logstash:/etc/logstash/conf.d # 将 logstash 的配置文件挂载出来，方便在宿主机上修改。 elasticsearch 重要的参数调优 ES_HEAP_SIZE Elasticsearch will assign the entire heap specified in jvm.options via the Xms (minimum heap size) and Xmx (maximum heap size) settings. You should set these two settings to be equal to each other. Set Xmx and Xms to no more than 50% of your physical RAM.the exact threshold varies but is near 32 GB. the exact threshold varies but 26 GB is safe on most systems, but can be as large as 30 GB on some systems.利弊关系: The more heap available to Elasticsearch, the more memory it can use for its internal caches, but the less memory it leaves available for the operating system to use for the filesystem cache. Also, larger heaps can cause longer garbage collection pauses. LS_HEAP_SIZE 如果 heap size 过低，会导致 CPU 利用率到达瓶颈，造成 JVM 不断的回收垃圾。 不能设置 heap size 超过物理内存。 至少留 1G 给操作系统和其他的进程。 若是采用上述这个三合一的 docker 镜像，官方文档, 对于 ELK 的日志，处理的方式为 Note that ELK’s logs are rotated daily and are deleted after a week, using logrotate. You can change this behaviour by overwriting the elasticsearch, logstash and kibana files in /etc/logrotate.d 12# 每天的 6:25 会对日志进行分割压缩处理，此时对机器的 disk 有大量的 IO 工作，会导致 system load 上升。 25 6 * * * root test -x /usr/sbin/anacron || ( cd / &amp;&amp; run-parts --report /etc/cron.daily ) 这里的解决办法请看下文的自定义部分。 使用Elasticsearch的REST Client的An HTTP line is larger than 4096 bytes{“type”:”too_long_frame_exception”,”reason”:”An HTTP line is larger than 4096 bytes.”}，默认情况下ES对请求参数设置为4K，如果遇到请求参数长度限制可以在elasticsearch.yml中修改如下参数： 请参考官方文档12http.max_initial_line_length: 8khttp.max_header_size: 16k elasticsearch 普通方式安装也是非常的简单，官方文档，或者是民间文档，其实也就是安装一个 JDK， 然后添加一个 repo 仓库。 filebeat 配置在 client 端，我们需要安装并且配置 filebeat 请参考Filebeat 模块与配置配置文件 filebeat.yml1234567891011121314151617181920filebeat.inputs:- type: log enabled: true paths: # 需要收集的日志 - /var/log/app/** ## ** need high versiob filebeat can support recursive fields: #需要添加的字段 host: "&#123;&#123;inventory_hostname&#125;&#125;" function: "xxx" multiline: # 多行匹配 match: after negate: true # pay attention the format pattern: '^\[[0-9]&#123;4&#125;-[0-9]&#123;2&#125;-[0-9]&#123;2&#125;' #\[ ignore_older: 24h clean_inactive: 72houtput.logstash: hosts: ["&#123;&#123;elk_server&#125;&#125;:25044"] # ssl: # certificate_authorities: ["/etc/filebeat/logstash.crt"] 批量部署 filebeat.yml 最好使用 ansible1234567891011121314151617181920212223242526272829---- hosts: all become: yes gather_facts: yes tasks: - name: stop filebeat service: name: filebeat state: stopped enabled: yes - name: upload filebeat.yml template: src: filebeat.yml dest: /etc/filebeat/filebeat.yml owner: root group: root mode: 0644 - name: remove file: #delete all files in this directory path: /var/lib/filebeat/registry state: absent - name: restart filebeat service: name: filebeat state: restarted enabled: yes 查看 filebeat output首先需要修改配置，将 filebeat 输出到本地的文件，输出的格式为 json.123456789101112131415161718filebeat.inputs:- type: log enabled: true paths: - /var/log/app/** fields: host: "x.x.x.x" region: "sg" multiline: match: after negate: true pattern: '^[0-9]&#123;4&#125;-[0-9]&#123;2&#125;-[0-9]&#123;2&#125;' ignore_older: 24h clean_inactive: 72houtput.file: path: "/home/feiyang" filename: feiyang.json 通过上述的配置，我们就可以在路径 /home/feiyang 下得到输出结果文件 feiyang.json 在这里需要注意的是，不同版本的 filebeat 输出结果的格式会有所不同，这会给 logstash 解析过滤造成一点点困难。下面举例说明 6.x 和 7.x filebeat 输出结果的不同 12345678910111213141516171819202122232425# 这是 6.4.2 的 filebeat&#123; "@timestamp": "2019-06-27T15:53:27.682Z", "@metadata": &#123; "beat": "filebeat", "type": "doc", "version": "6.4.2" &#125;, "fields": &#123; "host": "x.x.x.x", "region": "sg" &#125;, "host": &#123; "name": "x.x.x.x" &#125;, "beat": &#123; "name": "x.x.x.x", "hostname": "feiyang-localhost", "version": "6.4.2" &#125;, "offset": 1567983499, "message": "[2019-06-27T22:53:25.756327232][Info][@http.go.177] [48552188]request", "source": "/var/log/feiyang/scripts/all.log"&#125; 6.4 与 7.2 还是有很大的差异，在结构上。 1234567891011121314151617181920212223242526272829303132333435363738# 这是 7.2.0 的 filebeat&#123; "@timestamp": "2019-06-27T15:41:42.991Z", "@metadata": &#123; "beat": "filebeat", "type": "_doc", "version": "7.2.0" &#125;, "agent": &#123; "id": "3a38567b-e6c3-4b5a-a420-f0dee3a3bec8", "version": "7.2.0", "type": "filebeat", "ephemeral_id": "b7e3c0b7-b460-4e43-a9af-6d36c25eece7", "hostname": "feiyang-localhost" &#125;, "log": &#123; "offset": 69132192, "file": &#123; "path": "/var/log/app/feiyang/scripts/info.log" &#125; &#125;, "message": "2019-06-27 22:41:25.312|WARNING|14186|Option|data|unrecognized|fields=set([u'id'])", "input": &#123; "type": "log" &#125;, "fields": &#123; "region": "sg", "host": "x.x.x.x" &#125;, "ecs": &#123; "version": "1.0.0" &#125;, "host": &#123; "name": "feiyang-localhost" &#125;&#125; 只需要配置logstash接下来，我们再来看一看 logstash.conf 记得看注释参考链接: SSL详情可参考 grok 正则捕获 grok插件语法介绍 logstash 配置语法 grok 内置 pattern Logstash详细记录 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182input &#123; beats &#123; port =&gt; 5044 #host =&gt; "192.168.1.1" 监听内网 #ssl =&gt; true #ssl_certificate =&gt; "/etc/logstash/logstash.crt" #ssl_key =&gt; "/etc/logstash/logstash.key"# 1. SSL详情可参考 &#125;&#125;# filter 模块主要是数据预处理，提取一些信息，方便 elasticsearch 好归类存储。# 2. grok 正则捕获# 3. grok插件语法介绍 # 4. logstash 配置语法 # 5. grok 内置 pattern filter &#123; grok &#123; match =&gt; &#123;"message" =&gt; "%&#123;EXIM_DATE:timestamp&#125;\|%&#123;LOGLEVEL:log_level&#125;\|%&#123;INT:pid&#125;\|%&#123;GREEDYDATA&#125;"&#125;# message 字段是 log 的内容，例如 2018-12-11 23:46:47.051|DEBUG|3491|helper.py:85|helper._save_to_cache|shop_session# 在这里我们提取出了 timestamp log_level pid，grok 有内置定义好的patterns: EXIM_DATE, EXIM_DATE, INT# GREEDYDATA 贪婪数据，代表任意字符都可以匹配 &#125;# 我们在 filebeat 里面添加了这个字段[fields][function]的话，那就会执行对应的 match 规则去匹配 path# source 字段就是 log 的来源路径，例如 /var/log/nginx/feiyang233.club.access.log# match 后我们就可以得到 path=feiyang233.club.access if [fields][function]=="nginx" &#123; grok &#123; match =&gt; &#123;"source" =&gt; "/var/log/nginx/%&#123;GREEDYDATA:path&#125;.log%&#123;GREEDYDATA&#125;"&#125; &#125; &#125; # 例如 ims 日志来源是 /var/log/ims_logic/debug.log# match 后我们就可以得到 path=ims_logic else if [fields][function]=="ims" &#123; grok &#123; match =&gt; &#123;"source" =&gt; "/var/log/%&#123;GREEDYDATA:path&#125;/%&#123;GREEDYDATA&#125;"&#125; &#125; &#125; else &#123; grok &#123; match =&gt; &#123;"source" =&gt; "/var/log/app/%&#123;GREEDYDATA:path&#125;/%&#123;GREEDYDATA&#125;"&#125; &#125; &#125;# filebeat 有定义 [fields][function] 时，我们就添加上这个字段，例如 QA if [fields][function] &#123; mutate &#123; add_field =&gt; &#123; "function" =&gt; "%&#123;[fields][function]&#125;" &#125; &#125; &#125; # 因为线上的机器更多，线上的我默认不在 filebeat 添加 function，所以 else 我就添加上 live else &#123; mutate &#123; add_field =&gt; &#123; "function" =&gt; "live" &#125; &#125; &#125;# 在之前 filter message 时，我们得到了 timestamp，这里我们修改一下格式，添加上时区。 date &#123; match =&gt; ["timestamp" , "yyyy-MM-dd HH:mm:ss Z"] target =&gt; "@timestamp" timezone =&gt; "Asia/Singapore" &#125;# 将之前获得的 path 替换其中的 / 替换为 - , 因为 elasticsearch index name 有要求# 例如 feiyang/test feiyang_test mutate &#123; gsub =&gt; ["path","/","-"] add_field =&gt; &#123;"host_ip" =&gt; "%&#123;[fields][host]&#125;"&#125; remove_field =&gt; ["tags","@version","offset","beat","fields","exim_year","exim_month","exim_day","exim_time","timestamp"] &#125;# remove_field 去掉一些多余的字段&#125;# 单节点 output 就在本机，也不需要 SSL, 但 index 的命名规则还是需要非常的注意output &#123; elasticsearch &#123; hosts =&gt; ["localhost:9200"] index =&gt; "sg-%&#123;function&#125;-%&#123;path&#125;-%&#123;+xxxx.ww&#125;"# sg-nginx-feiyang233.club.access-2019.13 ww代表周数 &#125;&#125; 最终的流程图如下所示index 的规则 参考链接 Lowercase only Cannot include \, /, *, ?, “, &lt;, &gt;, |, ` ` (space character), ,, # Indices prior to 7.0 could contain a colon (:), but that’s been deprecated and won’t be supported in 7.0+ Cannot start with -, _, + Cannot be . or .. Cannot be longer than 255 bytes (note it is bytes, so multi-byte characters will count towards the 255 limit faster) Kibana 简单的使用在搭建 ELK 时，暴露出来的 5601 端口就是 Kibana 的服务。访问 http://your_elk_ip:5601 安装设置集群 ELK 版本 6.7ELK 安装文档集群主要是高可用，多节点的 Elasticsearch 还可以扩容。本文中用的官方镜像 The base image is centos:7 Elasticsearch 多节点搭建官方安装文档 Elasticsearch1234567# 挂载出来的文件夹权限非常的重要mkdir -p /data/elk-data &amp;&amp; chmod 755 /data/elk-datachown -R root:root /data docker run -p WAN_IP:9200:9200 -p 10.66.236.116:9300:9300 \-v /data/elk-data:/usr/share/elasticsearch/data \--name feiy_elk \docker.elastic.co/elasticsearch/elasticsearch:6.7.0 接下来是修改配置文件 elasticsearch.yml1234567891011121314# Master 节点 node-1# 进入容器 docker exec -it [container_id] bash# docker exec -it 70ada825aae1 bash# vi /usr/share/elasticsearch/config/elasticsearch.ymlcluster.name: "feiy_elk"network.host: 0.0.0.0node.master: truenode.data: truenode.name: node-1network.publish_host: 10.66.236.116discovery.zen.ping.unicast.hosts: ["10.66.236.116:9300","10.66.236.118:9300","10.66.236.115:9300"]# exit# docker restart 70ada825aae1 123456789101112# slave 节点 node-2# 进入容器 docker exec -it [container_id] bash# vi /usr/share/elasticsearch/config/elasticsearch.ymlcluster.name: "feiy_elk"network.host: "0.0.0.0"node.name: node-2node.data: truenetwork.publish_host: 10.66.236.118discovery.zen.ping.unicast.hosts: ["10.66.236.116:9300","10.66.236.118:9300","10.66.236.115:9300"]# exit# docker restart 70ada825aae1 123456789101112# slave 节点 node-3# 进入容器 docker exec -it [container_id] bash# vi /usr/share/elasticsearch/config/elasticsearch.ymlcluster.name: "feiy_elk"network.host: "0.0.0.0"node.name: node-3node.data: truenetwork.publish_host: 10.66.236.115discovery.zen.ping.unicast.hosts: ["10.66.236.116:9300","10.66.236.118:9300","10.66.236.115:9300"]# exit# docker restart 70ada825aae1 检查集群节点个数，状态等1234567891011121314151617181920curl http://127.0.0.1:9200/_cluster/health?pretty# curl http://wan_ip:9200/_cluster/health?pretty&#123; "cluster_name" : "feiy_elk", "status" : "green", "timed_out" : false, "number_of_nodes" : 3, "number_of_data_nodes" : 3, "active_primary_shards" : 9, "active_shards" : 18, "relocating_shards" : 0, "initializing_shards" : 0, "unassigned_shards" : 0, "delayed_unassigned_shards" : 0, "number_of_pending_tasks" : 0, "number_of_in_flight_fetch" : 0, "task_max_waiting_in_queue_millis" : 0, "active_shards_percent_as_number" : 100.0&#125; 最终结果图在 kibana 上可以看到集群状态 Kibana 搭建官方安装文档 Kibana123456# docker run --link YOUR_ELASTICSEARCH_CONTAINER_NAME_OR_ID:elasticsearch -p 5601:5601 &#123;docker-repo&#125;:&#123;version&#125;docker run -p 外网IP:5601:5601 --link elasticsearch容器的ID:elasticsearch docker.elastic.co/kibana/kibana:6.7.0# 注意的是 --link 官方其实并不推荐的，推荐的是 use user-defined networks https://docs.docker.com/network/links/# 测试不用 --link 也可以通。直接用容器的 IPdocker run -p 外网IP:5601:5601 docker.elastic.co/kibana/kibana:6.7.0 we recommend that you use user-defined networks to facilitate communication between two containers instead of using –link 12345678910111213141516# vi /usr/share/kibana/config/kibana.yml# 需要把 hosts IP 改为 elasticsearch 容器的 IP# 我这里 elasticsearch 容器的 IP 是 172.17.0.2# 如何查看 docker inspect elasticsearch_IDserver.name: kibanaserver.host: "0.0.0.0"elasticsearch.hosts: [ "http://172.17.0.2:9200" ]xpack.monitoring.ui.container.elasticsearch.enabled: true# 退出容器并重启docker restart [container_ID]# 非容器安装xpack.monitoring.enabled: enablexpack.monitoring.elasticsearch: ["http://127.0.0.1:9200"] Logstash 搭建官方安装文档 Logstash1234# docker -d 以后台的方式启动容器 --name 参数显式地为容器命名docker run -p 5044:5044 -d --name test_logstash docker.elastic.co/logstash/logstash:6.7.0# 也可以指定网卡，监听在内网或者外网 监听在内网 192.168.1.2docker run -p 192.168.1.2:5044:5044 -d --name test_logstash docker.elastic.co/logstash/logstash:6.7.0 1234# vi /usr/share/logstash/pipeline/logstash.conf# 配置详情请参考下面的链接,记得 output hosts IP 指向 Elasticsearch 的 IP# Elasticsearch 的默认端口是9200，在下面的配置中可以省略。hosts =&gt; ["IP Address 1:port1", "IP Address 2:port2", "IP Address 3"] logstash 过滤规则 见上文的配置和 grok 语法规则123456789101112# vi /usr/share/logstash/config/logstash.yml# 需要把 url 改为 elasticsearch master 节点的 IPhttp.host: "0.0.0.0"xpack.monitoring.elasticsearch.url: http://elasticsearch_master_IP:9200node.name: "feiy"pipeline.workers: 24 # same with cores# 非容器安装xpack.monitoring.enabled: truexpack.monitoring.elasticsearch.hosts: ["http://127.0.0.1:9200"]xpack.monitoring.collection.pipeline.details.enabled: true 改完配置 exit 从容器里退出到宿主机，然后重启这个容器。更多配置详情，参见官方文档1234# 如何查看 container_IDdocker ps -adocker restart [container_ID] 容灾测试我们把当前的 master 节点 node-1 关机，通过 kibana 看看集群的状态是怎样变化的。当前集群的状态变成了黄色，因为还有 3 个 Unassigned Shards。颜色含义请参考官方文档，再过一会发现集群状态变成了绿色。 kibana 控制台 ConsoleQuick intro to the UIThe Console UI is split into two panes: an editor pane (left) and a response pane (right). Use the editor to type requests and submit them to Elasticsearch. The results will be displayed in the response pane on the right side. Console understands requests in a compact format, similar to cURL:12345678# index a docPUT index/type/1&#123; "body": "here"&#125;# and get it ...GET index/type/1 While typing a request, Console will make suggestions which you can then accept by hitting Enter/Tab. These suggestions are made based on the request structure as well as your indices and types. A few quick tips, while I have your attention Submit requests to ES using the green triangle button. Use the wrench menu for other useful things. You can paste requests in cURL format and they will be translated to the Console syntax. You can resize the editor and output panes by dragging the separator between them. Study the keyboard shortcuts under the Help button. Good stuff in there! Console 常用的命令Kibana 控制台ELK技术栈中的那些查询语法12345678910111213141516171819202122232425262728293031323334353637383940414243444546curl -X GET "https://user:passwd@IP:9200/_cat/indices?v"GET _search&#123; "query": &#123; "match_all": &#123;&#125; &#125;&#125;GET /_cat/health?vGET /_cat/nodes?vGET /_cluster/allocation/explainGET /_cluster/stateGET /_cat/thread_pool?vGET /_cat/indices?health=red&amp;vGET /_cat/indices?v#将当前所有的 index 的 replicas 设置为 0PUT /*/_settings&#123; "index" : &#123; "number_of_replicas" : 0, "refresh_interval": "30s" &#125;&#125;GET /_template# 在单节点的时候，不需要备份，所以将 replicas 设置为 0PUT _template/app-logstash&#123; "index_patterns": ["app-*"], "settings": &#123; "number_of_shards": 3, "number_of_replicas": 0, "refresh_interval": "30s" &#125;&#125; 123456789101112# Delete all indices in your Elastic Search clusterfor i in `curl 'localhost:9200/_cat/indices?v' | tail -n +2 | awk '&#123;print $3&#125;'`; do curl -XDELETE "http://127.0.0.1:9200/$i"; done# vim delete_elk.sh#!/bin/bashfor i in `curl 'localhost:9200/_cat/indices?v' | tail -n +2 | awk '&#123;print $3&#125;'`; do curl -XDELETE "http://127.0.0.1:9200/$i"; donesleep 5ssystemctl restart elasticsearch Elasticsearch 数据迁移Elasticsearch 数据迁移官方文档感觉不是很详细。容器化的数据迁移，我太菜用 reindex 失败了，snapshot 也凉凉。最后是用一个开源工具 An Elasticsearch Migration Tool 进行数据迁移的。 123wget https://github.com/medcl/esm-abandoned/releases/download/v0.4.2/linux64.tar.gztar -xzvf linux64.tar.gz./esm -s http://127.0.0.1:9200 -d http://192.168.21.55:9200 -x index_name -w=5 -b=10 -c 10000 --copy_settings --copy_mappings --force --refresh Nginx 代理转发因为有时候 docker 重启，iptables restart 也会刷新，所以导致了我们的限制规则会被更改，出现安全问题。这是由于 docker 的网络隔离基于 iptable 实现造成的问题。为了避免这个安全问题，我们可以在启动 docker 时，就只监听在内网，或者本地 127.0.0.1 然后通过 nginx 转发。12345678910111213141516171819202122232425# cat kibana.confserver &#123; listen 25601; server_name x.x.x.x; access_log /var/log/nginx/kibana.access.log; error_log /var/log/nginx/kibana.error.log; location / &#123; allow x.x.x.x; allow x.x.x.x; deny all; proxy_http_version 1.1; proxy_buffer_size 64k; proxy_buffers 32 32k; proxy_busy_buffers_size 128k; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://127.0.0.1:5601; &#125;&#125; ! 这里需要注意的是， iptable filter 表 INPUT 链 有没有阻挡 172.17.0.0/16 docker 默认的网段。是否阻挡了 25601 这个端口。 踩过的坑iptables 防不住需要看上一篇博客里的 iptable 问题。或者监听在内网，用 Nginx 代理转发。 elk 网络问题elk node单节点discovery.type=single-node在测试单点时可用，搭建集群时不能设置这个环境变量，详情见官方文档 ELK的一次吞吐量优化filebeat 版本版本过低导致 recursive glob patterns ** 不可用用 ansible 升级 filebeat123456789101112131415161718192021222324252627282930313233343536373839404142---- hosts: all become: yes gather_facts: yes tasks: - name: upload filebeat.repo copy: src: elasticsearch.repo dest: /etc/yum.repos.d/elasticsearch.repo owner: root group: root mode: 0644 - name: install the latest version of filebeat yum: name: filebeat state: latest #6.x.1-1 7.x.1-1 - name: restart filebeat service: name: filebeat state: restarted enabled: yes # vim /etc/yum.repos.d/elasticsearch.repo[elasticsearch-6.x]name=Elasticsearch repository for 6.x packagesbaseurl=https://artifacts.elastic.co/packages/6.x/yumgpgcheck=1gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearchenabled=1autorefresh=1type=rpm-md[elasticsearch-7.x]name=Elasticsearch repository for 7.x packagesbaseurl=https://artifacts.elastic.co/packages/7.x/yumgpgcheck=1gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearchenabled=1autorefresh=1type=rpm-md filebeat 兼容性7.x 与 6.x 不兼容问题. 关键字变化很大, 比如说 “sorce” 变为了 [log][file][path] kibana报错“message”:”Alias [.kibana] has more than one indices associated with it [[.kibana_1, .kibana_2]] 这是因为 kibana 连接了一台机器，如果我们把这台 host 和 kibana 删除，但 kibana 的数据还会在另外两台 host上。当重新创建 host 加入时，会自动同步 .kibana，kibana 就会报错 7.4版本日志位置没有写在 /var/log/ 对应的文件夹，而是 sttout , 在 /var/log/messages 下。 或者可以用命令 journalctl -fu logstash 查看最新的日志。 logstash-7.4 启动错误CentOS 添加好 repo 后， 用 yum install logstash 安装好了以后。正在启动，但没有看到有进程监听 5044 端口， journalctl -fu logstash 查看日志发现报错如下12345678910111213[2019-11-21T19:18:21,800][FATAL][logstash.runner] An unexpected error occurred! &#123;:error=&gt;&lt;ArgumentError: Path "/var/lib/logstash/dead_letter_queue" must be a writable directory. It is not writable.#然后我们修改权限chmod 777 /var/lib/logstash/dead_letter_queue# 新的报错为 Access Denied &#123;:error=&gt;java.nio.file.AccessDeniedException: /var/lib/logstash/.lock#再次修改权限，终于成功了chmod 777 /var/lib/logstash/.lock disk fullonce disk usage more than 95%, elasticseach will stop store new data to index, and will add lock for these index. Even you release the disk, the lock is still enable. elk cannot store new data to read-only index. you will see the error log in logstash12[logstash.outputs.elasticsearch] retrying failed action with response code: 403 (&#123;&quot;type&quot;=&gt;ggq&quot;cluster_block_exception&quot;, &quot;reason&quot;=&gt;&quot;blocked by: [FORBIDDEN/12/index read-only / allow delete (api)];&quot;&#125;) The solution is resetting the read-only index123456789101112# In consloe PUT /index_name/_settings&#123; "index.blocks.read_only_allow_delete": null&#125;# URLcurl -X PUT "localhost:9200/index_name/_settings?pretty" -H 'Content-Type: application/json' -d'&#123; "index.blocks.read_only_allow_delete": null&#125;' kibana cannot create indexIn elasticsearch log, about Fielddata The index option controls whether field values are indexed. It accepts true or false and defaults to true. Fields that are not indexed are not queryable.123456789101112131415161718192021222324252627282930313233343536373839404142434445[2020-02-03T00:00:12,211][DEBUG][o.e.a.s.TransportSearchAction] [localhost.localdomain] All shards failed for phase: [query]org.elasticsearch.ElasticsearchException$1: Fielddata is disabled on text fields by default. Set fielddata=true on [type] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead. at org.elasticsearch.ElasticsearchException.guessRootCauses(ElasticsearchException.java:639) ~[elasticsearch-7.4.2.jar:7.4.2] at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:137) [elasticsearch-7.4.2.jar:7.4.2] at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:273) [elasticsearch-7.4.2.jar:7.4.2] at org.elasticsearch.action.search.InitialSearchPhase.onShardFailure(InitialSearchPhase.java:105) [elasticsearch-7.4.2.jar:7.4.2] at org.elasticsearch.action.search.InitialSearchPhase.access$200(InitialSearchPhase.java:50) [elasticsearch-7.4.2.jar:7.4.2] at org.elasticsearch.action.search.InitialSearchPhase$2.onFailure(InitialSearchPhase.java:273) [elasticsearch-7.4.2.jar:7.4.2] at org.elasticsearch.action.search.SearchExecutionStatsCollector.onFailure(SearchExecutionStatsCollector.java:73) [elasticsearch-7.4.2.jar:7.4.2] at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:59) [elasticsearch-7.4.2.jar:7.4.2] at org.elasticsearch.action.search.SearchTransportService$ConnectionCountingHandler.handleException(SearchTransportService.java:424) [elasticsearch-7.4.2.jar:7.4.2] at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1120) [elasticsearch-7.4.2.jar:7.4.2] at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1229) [elasticsearch-7.4.2.jar:7.4.2] at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1203) [elasticsearch-7.4.2.jar:7.4.2] at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:60) [elasticsearch-7.4.2.jar:7.4.2] at org.elasticsearch.action.support.ChannelActionListener.onFailure(ChannelActionListener.java:56) [elasticsearch-7.4.2.jar:7.4.2] at org.elasticsearch.action.ActionListener$1.onFailure(ActionListener.java:70) [elasticsearch-7.4.2.jar:7.4.2] at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:64) [elasticsearch-7.4.2.jar:7.4.2] at org.elasticsearch.search.SearchService.lambda$rewriteShardRequest$7(SearchService.java:1043) [elasticsearch-7.4.2.jar:7.4.2] at org.elasticsearch.action.ActionRunnable$1.doRun(ActionRunnable.java:45) [elasticsearch-7.4.2.jar:7.4.2] at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [elasticsearch-7.4.2.jar:7.4.2] at org.elasticsearch.common.util.concurrent.TimedRunnable.doRun(TimedRunnable.java:44) [elasticsearch-7.4.2.jar:7.4.2] at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:773) [elasticsearch-7.4.2.jar:7.4.2] at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [elasticsearch-7.4.2.jar:7.4.2] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?] at java.lang.Thread.run(Thread.java:830) [?:?]Caused by: java.lang.IllegalArgumentException: Fielddata is disabled on text fields by default. Set fielddata=true on [type] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead. at org.elasticsearch.index.mapper.TextFieldMapper$TextFieldType.fielddataBuilder(TextFieldMapper.java:759) ~[elasticsearch-7.4.2.jar:7.4.2] at org.elasticsearch.index.fielddata.IndexFieldDataService.getForField(IndexFieldDataService.java:116) ~[elasticsearch-7.4.2.jar:7.4.2] at org.elasticsearch.index.query.QueryShardContext.getForField(QueryShardContext.java:191) ~[elasticsearch-7.4.2.jar:7.4.2] at org.elasticsearch.search.aggregations.support.ValuesSourceConfig.resolve(ValuesSourceConfig.java:112) ~[elasticsearch-7.4.2.jar:7.4.2] at org.elasticsearch.search.aggregations.support.ValuesSourceAggregationBuilder.resolveConfig(ValuesSourceAggregationBuilder.java:350) ~[elasticsearch-7.4.2.jar:7.4.2] at org.elasticsearch.search.aggregations.support.ValuesSourceAggregationBuilder.doBuild(ValuesSourceAggregationBuilder.java:322) ~[elasticsearch-7.4.2.jar:7.4.2] at org.elasticsearch.search.aggregations.support.ValuesSourceAggregationBuilder.doBuild(ValuesSourceAggregationBuilder.java:39) ~[elasticsearch-7.4.2.jar:7.4.2] at org.elasticsearch.search.aggregations.AbstractAggregationBuilder.build(AbstractAggregationBuilder.java:139) ~[elasticsearch-7.4.2.jar:7.4.2] at org.elasticsearch.search.aggregations.AggregatorFactories$Builder.build(AggregatorFactories.java:332) ~[elasticsearch-7.4.2.jar:7.4.2] at org.elasticsearch.search.SearchService.parseSource(SearchService.java:784) ~[elasticsearch-7.4.2.jar:7.4.2] at org.elasticsearch.search.SearchService.createContext(SearchService.java:586) ~[elasticsearch-7.4.2.jar:7.4.2] at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:545) ~[elasticsearch-7.4.2.jar:7.4.2] at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:348) ~[elasticsearch-7.4.2.jar:7.4.2] at org.elasticsearch.search.SearchService.lambda$executeQueryPhase$1(SearchService.java:340) ~[elasticsearch-7.4.2.jar:7.4.2] at org.elasticsearch.action.ActionListener.lambda$map$2(ActionListener.java:145) ~[elasticsearch-7.4.2.jar:7.4.2] at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:62) ~[elasticsearch-7.4.2.jar:7.4.2] ... 9 more 自定义部分自动删除 index因为我是按周数来 %{+xxxx.ww} 存 index，由于我们的存储机器硬盘有限，最多能存放两个周的日志，所以需要删除两周之前的 index1234567# vim /opt/delete.sh# 这个是放在容器内的脚本。#!/bin/bashweek=$(date -d "-2 week " +%V)year=$(date +%Y)echo $year"-"$weekcurl -XDELETE 'http://127.0.0.1:9200/*-'$year'.'$week'' 宿主机上跑 cronjob12345# vim /etc/crontab0 03 * * 1 root /bin/docker exec [mysql 容器 ID] bash -c "cd /opt &amp;&amp; bash delete.sh" systemctl restart crond 内部log过大因为默认的 log level 的 INFO, 所以 logstash 的日志特别大。导致在做 logrotate 日志切割时 disk I/O 特别的大。解决的办法就是修改 log.level 在 /opt/logstash/config/log4j2.properties 里修改 log.level， 还有修改 /opt/logstash/config/logstash.yml 然后重启 logstash1234567891011121314151617# ------------ Debugging Settings --------------## Options for log.level:# * fatal# * error# * warn# * info (default)# * debug# * trace#日志输出的等级# Trace:是追踪，就是程序推进一下.# Debug:指出细粒度信息事件对调试应用程序是非常有帮助的.# Info:消息在粗粒度级别上突出强调应用程序的运行过程.# Warn:输出警告及warn以下级别的日志.# Error:输出错误信息日志.# Fatal:输出每个严重的错误事件将会导致应用程序的退出的日志. log.level: info 修改日志保留的日期，默认是一周，可以修改 /etc/logrotate.d/logstash rotate 天数]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GitHub-Hexo-NexT 免费搭建自己的博客]]></title>
    <url>%2Fpost%2Fhexo-blog%2F</url>
    <content type="text"><![CDATA[关于写博客2018 年我在 GitHub 上看到强哥的博客，当时为之心动然后 Fork 了那个 repository。后来搬砖沦为社会人就断更了，在工作之后看到奥哥的博客，在他的安利之下，把之前的 Jekyll 的博客换成如今的 Hexo-NexT，打算记录下工作的点滴，学习的心得。一是分享，二是方便自己回顾和查阅。 GitHub如果没有 GitHub 账号，需要去官网 https://github.com/ 注册一个账号。账号注册好之后，需要创建一个 repository，名称格式为 xxx.github.io 例如我的 GitHub 名称为 fainyang, 浏览器显示的 URL 为 https://github.com/fainyang 那么我新建的 repository 就是 fainyang.github.io 申请完账号，创建了 repository 之后，下一步就是在自己的电脑上安装 Hexo。 HexoHexo——快速、简洁且高效的博客框架。官网 https://hexo.io/zh-cn/ 文档还有视频讲解如何安装 安装前提在安装 Hexo 之前，需要安装 Git 与 Node.js 请参考 这里 开始安装12345npm install hexo-cli -g npm install hexo --savehexo init &lt;folder&gt; # &lt;folder&gt; 自己取个文件夹名字cd &lt;folder&gt; # 移动到 &lt;folder&gt; 文件夹，不清楚当前路径，可以执行 Mac:pwd Windows: chdirnpm install #feiy-mac:Coding feiy$ pwd /Users/feiy/Coding 新建完成后，指定文件夹的目录如下：12345678.├── _config.yml #站点配置文件,主要就是修改这个文件├── package.json├── scaffolds #模板文件夹├── source #存放文章，图片的文件夹| ├── _drafts| └── _posts└── themes #主题文件夹，等会下载 NexT 主题 这是我的 _config.yml 配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596# Hexo Configuration## Docs: https://hexo.io/docs/configuration.html## Source: https://github.com/hexojs/hexo/# Sitetitle: feiyang's blog subtitle: 费洋的博客description: 佛系咸鱼keywords:author: feiyanglanguage: zh-CN #网址的显示语言timezone:# URL## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'url: https://fainyang.github.io #需要修改为自己的网址enforce_ssl: fainyang.github.ioroot: /permalink: post/:title/permalink_defaults:# Directorysource_dir: sourcepublic_dir: publictag_dir: tagsarchive_dir: archivescategory_dir: categoriescode_dir: downloads/codei18n_dir: :langskip_render:# Writingnew_post_name: :title.md # File name of new postsdefault_layout: posttitlecase: false # Transform title into titlecaseexternal_link: true # Open external links in new tabfilename_case: 0render_drafts: falsepost_asset_folder: falserelative_link: falsefuture: truehighlight: enable: true line_number: true auto_detect: false tab_replace: # Home page setting# path: Root path for your blogs index page. (default = '')# per_page: Posts displayed per page. (0 = disable pagination)# order_by: Posts order. (Order by date descending by default)index_generator: path: '' per_page: 10 order_by: -date # Category &amp; Tagdefault_category: uncategorizedcategory_map: tag_map: # Date / Time format## Hexo uses Moment.js to parse and display date## You can customize the date format as defined in## http://momentjs.com/docs/#/displaying/format/date_format: YYYY-MM-DDtime_format: HH:mm:ss# Pagination## Set per_page to 0 to disable paginationper_page: 10pagination_dir: page# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: next# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: https://github.com/fainyang/fainyang.github.io.git #部署上传的地址，需要修改为自己的 branch: master message: new hexo_blog # Othersarchive_generator: per_page: 10 yearly: true monthly: truetag_generator: per_page: 10 Hexo 常用命令官网详细的介绍 https://hexo.io/zh-cn/docs/commands 123456命令缩写hexo n "文章标题" == hexo new "文章标题" #新建文章hexo p == hexo publishhexo g == hexo generate #生成hexo s == hexo server #启动服务预览hexo d == hexo deploy #部署 本地运行网站123hexo s # 当前路径要在之前创建的 hexo init &lt;folder&gt; 这个 &lt;folder&gt; 下INFO Start processingINFO Hexo is running at http://localhost:4000 . Press Ctrl+C to stop. 在浏览器输入 http://localhost:4000 就可以看到初始的网站 NexTElegant Theme for Hexo——精于心，简于形。 官网 http://theme-next.iissnan.com/ 安装 Next1234# 当前路径要在之前创建的 hexo init &lt;folder&gt; 这个 &lt;folder&gt; 下 # 查看当前路径 Mac:pwd Windows: chdircd &lt;folder&gt;git clone https://github.com/theme-next/hexo-theme-next themes/next git clone 完成后，指定文件夹的目录如下：1234567891011.├── _config.yml #站点配置文件，主要就是修改这个文件├── package.json├── scaffolds #模板文件夹├── source #存放文章，图片的文件夹| ├── _drafts| └── _posts└── themes #主题文件夹，等会下载 NexT 主题 ├── landscape └── next └── _config.yml #主题配置文件，修改博客主题样式等 主题配置文件 _config.yml12345678910111213# 菜单显示项menu: home: / || home about: /about/ || user tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive# Schemes 主题选择，我用的是 Gemini#scheme: Muse#scheme: Mist#scheme: Piscesscheme: Gemini 此时可以在本地预览一下运行命令 hexo s 部署到 GitHub部署前，需要修改配置文件里面的 git 地址12345deploy: type: git repo: https://github.com/yourname/yourname.github.io.git #部署上传的地址，需要修改为自己的 branch: master message: new hexo_blog 修改好 _config.yml 里的 deploy 模块后，就可以开始进行部署，上传到 GitHub1234npm install hexo-deployer-git --savehexo cleanhexo generatehexo deploy 第一次部署需要输入自己的 GitHub 账号和密码。后面更新部署时就不需要再次输入密码了。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188feiy-mac:fainyang.github.io feiy$ hexo clean #清除缓存INFO Deleted database.INFO Deleted public folder.feiy-mac:fainyang.github.io feiy$ hexo generate #生成文件INFO Start processingINFO Files loaded in 902 msINFO Generated: sitemap.xmlINFO Generated: atom.xmlINFO Generated: index.htmlINFO Generated: tags/index.htmlINFO Generated: categories/index.htmlINFO Generated: archives/index.htmlINFO Generated: about/index.htmlINFO Generated: img/combination1.pngINFO Generated: images/algolia_logo.svgINFO Generated: img/combination2.pngINFO Generated: img/404-southpark.jpgINFO Generated: img/favicon.icoINFO Generated: img/gay.jpegINFO Generated: images/apple-touch-icon-next.pngINFO Generated: img/old_driver.jpgINFO Generated: img/qq-qrcode.pngINFO Generated: img/zjxc.jpgINFO Generated: images/cc-by-nc-nd.svgINFO Generated: images/avatar.gifINFO Generated: images/cc-by-nc-sa.svgINFO Generated: images/cc-by-nc.svgINFO Generated: images/cc-by-sa.svgINFO Generated: images/cc-by-nd.svgINFO Generated: images/cc-by.svgINFO Generated: images/favicon.icoINFO Generated: images/quote-l.svgINFO Generated: images/cc-zero.svgINFO Generated: images/loading.gifINFO Generated: images/logo.svgINFO Generated: images/searchicon.pngINFO Generated: images/favicon-16x16-next.pngINFO Generated: images/placeholder.gifINFO Generated: images/favicon-32x32-next.pngINFO Generated: images/quote-r.svgINFO Generated: images/feiy.gifINFO Generated: archives/2019/03/index.htmlINFO Generated: archives/2018/02/index.htmlINFO Generated: post/hexo-blog/index.htmlINFO Generated: img/work/6.jpgINFO Generated: archives/2018/08/index.htmlINFO Generated: categories/学习/index.htmlINFO Generated: archives/2018/11/index.htmlINFO Generated: archives/2018/07/index.htmlINFO Generated: img/robot/run.pngINFO Generated: tags/python/index.htmlINFO Generated: categories/生活/index.htmlINFO Generated: tags/hexo/index.htmlINFO Generated: tags/工作/index.htmlINFO Generated: archives/2018/04/index.htmlINFO Generated: archives/2018/03/index.htmlINFO Generated: tags/links/index.htmlINFO Generated: post/Canteen/index.htmlINFO Generated: tags/NUS/index.htmlINFO Generated: img/2019/github.pngINFO Generated: post/Onboarding/index.htmlINFO Generated: post/Combination/index.htmlINFO Generated: post/Robot/index.htmlINFO Generated: post/blog/index.htmlINFO Generated: post/Master/index.htmlINFO Generated: post/links/index.htmlINFO Generated: post/timestampdiff/index.htmlINFO Generated: img/time3.pngINFO Generated: img/start.jpgINFO Generated: img/robot/tencent.pngINFO Generated: img/work/8.JPGINFO Generated: img/work/7.JPGINFO Generated: img/avatar-icon.jpgINFO Generated: img/time1.pngINFO Generated: img/wx-qrcode.pngINFO Generated: archives/2018/index.htmlINFO Generated: archives/2019/index.htmlINFO Generated: img/146228364162795.jpgINFO Generated: lib/font-awesome/HELP-US-OUT.txtINFO Generated: css/main.cssINFO Generated: img/canteen/bird.jpgINFO Generated: img/combination3.pngINFO Generated: img/time4.pngINFO Generated: img/path.jpgINFO Generated: img/time2.pngINFO Generated: img/work/10.JPGINFO Generated: img/work/5.JPGINFO Generated: img/work/13.JPGINFO Generated: img/work/2.JPGINFO Generated: img/work/1.JPGINFO Generated: lib/font-awesome/bower.jsonINFO Generated: img/robot/ditou.jpgINFO Generated: img/canteen/contact.pngINFO Generated: img/robot/happy.jpgINFO Generated: img/robot/poor.jpegINFO Generated: img/robot/miaohui.jpgINFO Generated: img/robot/qidai.jpegINFO Generated: img/robot/python.pngINFO Generated: img/robot/tuling.pngINFO Generated: img/canteen/B.pngINFO Generated: img/robot/dalao.pngINFO Generated: lib/velocity/velocity.ui.min.jsINFO Generated: lib/font-awesome/fonts/fontawesome-webfont.woff2INFO Generated: img/robot/result3.jpegINFO Generated: img/canteen/F-319.pngINFO Generated: img/canteen/F-320.pngINFO Generated: img/robot/result1.jpegINFO Generated: img/work/9.JPGINFO Generated: img/canteen/wen.pngINFO Generated: lib/font-awesome/fonts/fontawesome-webfont.woffINFO Generated: img/canteen/E1.pngINFO Generated: lib/velocity/velocity.ui.jsINFO Generated: lib/velocity/velocity.min.jsINFO Generated: img/canteen/E-322.pngINFO Generated: img/canteen/F-321.pngINFO Generated: img/canteen/F-322.pngINFO Generated: img/canteen/F-323.pngINFO Generated: img/canteen/Y-320.pngINFO Generated: img/canteen/Y-321.pngINFO Generated: img/canteen/Y-322.pngINFO Generated: img/work/12.JPGINFO Generated: img/robot/result2.jpegINFO Generated: img/work/4.JPGINFO Generated: img/canteen/E-319.pngINFO Generated: js/src/algolia-search.jsINFO Generated: lib/font-awesome/css/font-awesome.css.mapINFO Generated: img/issue.pngINFO Generated: img/combination4.pngINFO Generated: img/canteen/E-323.pngINFO Generated: img/work/3.JPGINFO Generated: img/canteen/Y-319.pngINFO Generated: img/canteen/table.pngINFO Generated: img/work/11.JPGINFO Generated: img/work/15.JPGINFO Generated: js/src/affix.jsINFO Generated: js/src/exturl.jsINFO Generated: js/src/next-boot.jsINFO Generated: js/src/scrollspy.jsINFO Generated: js/src/js.cookie.jsINFO Generated: js/src/post-details.jsINFO Generated: js/src/scroll-cookie.jsINFO Generated: lib/font-awesome/fonts/fontawesome-webfont.eotINFO Generated: img/canteen/Y-323.pngINFO Generated: img/canteen/E-321.pngINFO Generated: img/canteen/E-320.pngINFO Generated: lib/font-awesome/css/font-awesome.min.cssINFO Generated: lib/font-awesome/css/font-awesome.cssINFO Generated: js/src/motion.jsINFO Generated: js/src/utils.jsINFO Generated: js/src/schemes/pisces.jsINFO Generated: img/work/16.JPGINFO Generated: js/src/schemes/muse.jsINFO Generated: img/work/14.JPGINFO Generated: lib/jquery/index.jsINFO Generated: img/install-steps.gifINFO Generated: lib/velocity/velocity.jsINFO Generated: img/work/17.JPGINFO Generated: img/work/18.jpgINFO 152 files generated in 1.86 sfeiy-mac:fainyang.github.io feiy$ hexo deploy #部署到 GitHubINFO Deploying: gitINFO Clearing .deploy_git folder...INFO Copying files from public folder...INFO Copying files from extend dirs...[master f55c4d1] new hexo_blog 34 files changed, 8859 insertions(+), 150 deletions(-) create mode 100644 archives/2019/03/index.html create mode 100644 archives/2019/index.html create mode 100644 categories/index.html create mode 100644 categories/学习/index.html create mode 100644 categories/生活/index.html create mode 100644 img/2019/github.png create mode 100644 post/hexo-blog/index.html create mode 100644 tags/NUS/index.html create mode 100644 tags/hexo/index.html rename tags/&#123;life =&gt; links&#125;/index.html (87%) create mode 100644 tags/工作/index.htmlEnumerating objects: 114, done.Counting objects: 100% (114/114), done.Delta compression using up to 4 threadsCompressing objects: 100% (43/43), done.Writing objects: 100% (69/69), 97.73 KiB | 3.62 MiB/s, done.Total 69 (delta 33), reused 0 (delta 0)remote: Resolving deltas: 100% (33/33), completed with 17 local objects.To https://github.com/fainyang/fainyang.github.io.git 2b7a9ca..f55c4d1 HEAD -&gt; masterBranch 'master' set up to track remote branch 'master' from 'https://github.com/fainyang/fainyang.github.io.git'.INFO Deploy done: git 在浏览器输入: yourname.github.io 如果看到博客页面那就大功告成。如果有错误的话，在 GitHub repository Setting 里的 GitHub Pages 可以看到错误信息。 图片管理奥哥推荐的一个非常好用的图片管理工具 PicGo 图片上传+管理新体验。 踩过的一些坑 在添加标签和分类的时候，当在source下面创建了文件夹的 index.md 这个文件一定要添加相关的 type. 例如 type: “categories”, type: “tags”. 添加搜索功能时，一直转圈加载不出来。是因为自己写的文章中存在不可见字符：一个backspace的字符。 详情可以参考： 一&nbsp; 二]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实用链接分享]]></title>
    <url>%2Fpost%2Flinks%2F</url>
    <content type="text"><![CDATA[站在大佬们的肩膀上工作，学习，健身。 其实我蛮担心自己的发际线 运维博客 朱双印博客 iptables, ansible, zabbix 都很不错 骏马金龙 rsync, LVS, nginx, zooKeeper 金步国作品集 Linux HelloDog 大佬同事奥哥的博客 实用文档 ansible 批量操作的好帮手 zabbix 实时监控了解一下 ELK 收集过滤检索神器 Docker 方便快速的容器 Kubernetes DevOps 玩转容器部署 学习一个 慕课网 程序员的梦工厂 菜鸟教程 学的不仅是技术，更是梦想 w3school Web 技术教程 提高生产力 任务管理看板 Trello 免费无广告 数据库引擎 Free All-in-One Database Version Management Tool TablePlus Database management made easy 和 2 中的数据库引擎可以自动关联。 好书推荐 强哥 安利的 设计数据密集型应用 这或许是东半球分析十大排序算法最好的一篇文章]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>links</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[新加坡留学分享]]></title>
    <url>%2Fpost%2FMaster%2F</url>
    <content type="text"><![CDATA[为什么要留学这篇博客主要分享我的新加坡水硕留学经历，希望能对你有一点点帮助。更新：1.史上最全 NUS 硕士项目攻略 2.史上最全的 NTU 硕士项目攻略 留学的优势： 时间短，新加坡的授课型 Master 一年就读完。因为 EE 学院是按照学期收学费的，最快一年修完。大家当然都早点修完省学费。当国内同届的同学毕业时，你已经是有 2年工作经验的初级混子了。 毕业容易，毕业要求是修满 10 门课，没有导师。轻轻松松考完 10 门课混水硕文凭，考试是按照正态分布评分的，想挂科太难了。当然，你也可以选一个project跟着导师做一年项目，一个project抵两门课的学分。不要抱太大的期望，关键还是靠自学。比如我自己就选了项目，认识一个不错的导师，受益良多。 读博容易， 读完一年硕士，再去申请博士，比本科毕业申请直博容易太多，而且有机会面对面和导师交流，提前了解实验室的情况。我一届的 master 同学，想读博的，最后硕士毕业都梦想成真读博了。 长见识，肉身翻墙出来看看世界之大，结识外国的朋友们，体验东南亚文化，然后你就知道还是祖国安逸，我多么想念四川美食。 当跳板，如果想出国工作，那么出国读书无疑是最好的一条路，毕业后留下来找工作。当然也有 Master 毕业再去美国读博的。 海龟光环，像华为的海外招聘回国工作的起薪比国内稍高一点。还有一个巨大的优势就是走海外选调生考公务员竞争小，学而优则仕。 出游方便，因为新加坡是东南亚的经济中心，飞东南亚的航班又多又便宜。读书期间假期飞东南亚各国旅游，马来西亚，泰国，越南，印度尼西亚，菲律宾都非常方便。例如去年 recess week 学校假期我去槟城往返一共才 350 人民币的机票。 留学的劣势： 花费贵，硕士读下来至少 18 万人民币的学费，每个月生活费至少五千人民币。新加坡留学贷款，可以在学校里贷款 8 千新币，相当于 4 万人民币，利息很低，工作以后慢慢还。 压力大，虽然是水硕，但是全英文环境最开始还是有点不适应。如果选了 project，还要做项目写论文答辩。相当于把国内三年完成的事情全部压缩到一年内完成，十分的匆忙。 水土不服，这个因人而异。来新加坡读书有的同学瘦了十多斤，有的冒痘痘，有的脱发。但大家相同的感受就是太晒了（其实温度还好，最高 33° 左右）。 什么样的人适合来新加坡留学 安全第一，新加坡治安在世界数一数二了。在大学里去食堂吃饭把电脑，手机放图书馆桌上很安全。在我读书的一年里，没有听说中国留学生有谁遇到失窃的。晚上大街上走也很安全的。（相比于犯罪分子，倒不如小心野生动物，蛇，蜥蜴那些） 英语差的人，比如我自己（雅思考了三次才上 6.5）。其实新加坡留学一年生活中，除了上课，和导师，外国人交流用英文。平时都用中文，本地华人的中文也非常好。（其实这也是缺点，读书一年，发现自己的口语没有怎么提高） 怕离家太远。新加坡回国非常快，飞中国南方的城市大约4个小时，而且往返机票（非节假日）大约两千多人民币。 负担不起美国留学，又想出来看看的人，比如我自己。新加坡的学费和生活费相对便宜。 国内考研失败，但想继续读研的同学，出国读研肯定比二战的压力小太多了。国内考研每年一次，国外读研申请分两次，有秋季和春季。 留学的条件就三点：学科成绩，英语成绩，留学费用。 学科成绩：NUS NTU 两所学校 Master 招生大部分都是 985，211的毕业生。NUS ECE学院基本上要本科成绩平均分85+，NTU EEE学院要求会稍微低一点。 英语成绩：NUS 雅思最低 6 分，NTU 雅思最低 6.5 分 但最好 NUS 也是6.5 分。 这样的话开学就不用额外再上英语课。雅思的备考，可以报班，也可以自学。推荐APP: 雅思哥，小站雅思。个人的建议是英语不好的同学还是花钱报班吧，老师带着入门快点。 留学费用：今年2018 NUS Master 入学的费用国际留学生一共是36,750新币。 以前是有 government-subsidised graduate programmes 补贴的(减免一半的学费)，读下来学费加生活费一年至少18+7=25万。申请的时候还需要银行开具存款证明，每年的标准不同，大概在人民币 15 万左右。 卖身契详情请点击NUS 卖身契介绍 或者 新加坡教育局卖身契介绍，目前的消息是从 2020 年入学开始就没有授课硕士的卖身契了 From AY2019 onwards, all graduate coursework programmes are not eligible for the SO Scheme with exception of following programmes which are not eligible for the SO scheme only from AY2020 onwards:Master of Science (Electrical Engineering) 因为违约的人还是有很多，详情请看新闻 如何申请 找中介，适用于时间匆忙，想花钱省事的同学们。中介费大概在2万左右，中介老师会帮你写简历，文书，推荐信，邮寄申请材料等。推荐找大机构知名的中介，私人中介小心被坑。就算找了中介，自己还是要上点心，看看中介帮写的材料，注意申请的截止时间等。 DIY，适用于时间充足，想省钱的同学。作为 DIY 过来人，新加坡和香港的申请都比较简单，申请的 requirement 网上都有，截止时间也都写的明明白白，只是需要自己细心的去寻找。如果有学长学姐指导，老司机带是最稳的。要自己 DIY 记得让父母申请一张 Visa 或者 Master Card，因为申请缴费的时候是不能用银联卡的。寄托论坛上也有前辈们分享如何申请的。 申请所需资料：本科成绩单，本科在读证明，语言成绩单(雅思或托福)，个人简历，2 封推荐信，个人陈述（申请的理由，规划那些），银行存款证明。 申请分秋季入学和春季入学。秋季是每年的一月开始申请，先在学校官网上申请，最后把所需的资料邮寄到学校，顺丰到新加坡只需 3 天。 入学前准备秋季入学是每年的五月初发 offer，寄托论坛也有 offer榜，可以看看其他人的录取情况。很多人都会收到两所学校的录取，那么上 NUS 还是选 NTU 呢？首先当然看看自己所读的专业哪个学校好啦，其实两所学校都差不多。NUS 在本地人眼里更好一点，离城里近，但申请不到宿舍，ECE 学院选课自由，有点像国内的按大类招生。 NTU 坐落在郊区，能申到宿舍。食堂十分的优秀，比 NUS 好吃。环境优美，有野猪等野生动物出没。 在收到 offer 后最重要的一件事情就是：申请宿舍，NUS Master 申请宿舍成功率低，因为 NUS 地方小宿舍少，几乎在 NUS 读 Master 的同学们都是在外面租房子，一般都是六个人租一整套三室一厅，两个人 share 一间房。租房子必须找中介，Property Guru 和 99co新加坡本地发布平台，房源多。查找 NUS 周围的房源，然后联系中介看房，最后签合同。最近还有一个不错的小程序叫：小坡岛新加坡租房。一般 share 住房租人均在 450-600 新币/月，水电气网大约 50 新/月。 NTU 就很优秀了，人人都能申请到宿舍。宿舍的配套设施很完善，厨房，洗衣房，自习室，健身房，食堂都有。宿舍价格根据宿舍类型定价 300-600 新币/月。 把住的地方提前搞定后，等学校帮忙办理好学生签证，接下来就是收拾行李准备留学啦。 新加坡的手机网络，国内全网通手机就 OK，最好提前在淘宝买一张 3天无限流量卡。等安顿好了，再去 711、Cheers、手机店买本地 prepaid 电话卡，等有了住址证明再办理 postpaid 电话卡，比如 zero1 无限流量电话卡。 必备的APP: Grab（东南亚滴滴，打车方便，可以绑定支付宝付款）SG Buses or MyTransport (坐公交车必备，查询等车时间) Google Maps (出国当然用谷歌) 银行卡：推荐办理在境外取现不收取手续费的银联卡。比如华夏银行，成都银行金卡等，在新加坡办理银行卡一定需要住址证明，这点非常重要，没有新加坡本地银行卡生活体验很差，不能扫码支付，最终体会硬币多的烦恼。 机票：廉价航空没有行李额！留学生过来读书肯定都带了箱子，等到机场再买行李额要贵哭。我建议是不要买廉价航空，川航、国航都是两件行李额，南航是一件。 新加坡全年夏天，衣服最多带一件外套抵御教室里寒冷的空调。这面的优衣库，H&amp;M 价格和国内差不多。 再安利一下同事奥哥写的新加坡生活指南，网站有可能被墙了。 学生生活 报到：到新加坡安顿好以后，就去学校注册报到，然后就是校医院体检（主要检查肺结核和 HIV ）。接下来还有半个月开学，可以出去逛一逛，熟悉一下环境。 选课：开学之前要网上选课，这个根据自己的爱好和学长学姐的建议，不要踩雷。可以选择 10门课，也可以选择 8门课 + 1个项目。 上课：时间都是在晚上（本科课程在白天，NUS 可以跨选本科的课程，NTU 不能选本科课程），因为有非全日制的同学（人家白天要上班的），一周最多也就5节课。 每周都会有作业，有的 project 还需要小组组队完成。 编程作业 GitHub了解一下，说不定有答案。 考试：在选课时就能看到期末考试日期，所以可以提前买回家的机票呀。考试时间一般是 2-2.5 小时，有开卷，闭卷，一页纸。评分按正态分布，想挂科和得 A+ 一样的困难。 混个水硕还是非常容易的。 项目： 选一个导师跟着做一年。NUS 最后要写论文答辩，NTU 写论文不答辩。确认过眼神，遇上对的老师收获巨大，选错就以泪洗面！ 毕业要求：总学分平均绩点大于3.0， 3.0 我上我也行！（满绩是5.0 允许单科低于3.0 只要不是 F 就行，一般缺考才会得 F ） 就业去向：留下来的大多去了半导体和互联网公司，回国大部分都去互联网公司，少部分走海外选调生当公务员。 2019 年，我刚刚成功内推一个我电 14 级的通信学妹入职我司。 继续读博： 其实有很大比例的同学是把新加坡水硕当跳板，硕士毕业后去 NUS NTU 读博，还有去美国读博的。先读了 master 再读 PhD 比较容易，我认识的同学中，想读 PhD 的最终都梦想成真，只是有的老师手里名额多，硕士毕业马上入学 PhD。有的导师名额少，需要先做 RA 排队等待。 提醒一下：女生在校期间可以去校医院预约打 HPV 疫苗。学生卡还有很多优惠，比如学生邮箱免费注册 jetbrains 全家桶，环球影城半价，看电影半价，苹果学生优惠等。 我的学生日常：白天早起去学校图书馆自习，起晚了太阳晒呀。白天自习，晚上上课。每周和导师一次 meeting 汇报情况。周末做作业、项目，偶尔出去聚餐。 如今回想起来，白天不上课应该去找一个实习做一做的，这样有助于就业。 第二学期就比较忙一点，因为要找工作。每学期学校都有招聘会，不容错过。其他找工作的途径，在我上一篇博客咸鱼入海Sea里有。新加坡半导体和互联网就业相对容易，新加坡工程系毕业的 Master 起薪大概在 4000-5000 新币/月。 新加坡个人所得税很低。Master 刚毕业一年大概就交 600 新的税。 更新：2018 年，工作半年我自己交的税是 72 新币。在新加坡上班，目前我自己的感受：互联网比国内轻松一点，很少加班，周末双休。 半导体上班同学的感受：Micron上班 8小时，加班有工资美滋滋。 Liteon 同学感觉就像在国企上班一样，每天 5:30pm 下班，闲的都想回学校再读一个part-time Phd.如果能有幸进入 全球大厂例如 Facebook（包三餐，早9:30晚6），Google，Apple,或者新加坡的国企（早10晚6），可能真的会乐不思蜀了。 曾经的我，以为出国遥不可及，担心给家里带来巨大的负担。如今看来一切付出都是值得的，至少在我的留学圈子里，没有任何一个同学后悔出来看一看。很惭愧，只做了一点微小的贡献。 最后祝大家前程似锦，梦想成真感谢阅读，如果看完文章的你，还对新加坡留学有疑问的话，可以加我的QQ: 601300870 NUS 学生邮箱好处多NUS 的学生邮箱 6 个月强制重置邮箱密码一次。我在工作后太懒，忘记了改密码。发邮件给学校的 IT Center 结果没有回复我。谷歌一下找到了两个网站可以，其中一个是NUS-ID Password Reset, 在这个网站上我重置失败，原因未知。还有一个是 Reset Password - NUS AlumMAIL Portal，在这个网上我重置邮箱密码成功了。学生邮箱还可以用来申请 jetbrains 全家桶，每年一续还可以使用一些微软的工具，如下图 还有一点是，可以使用全球大学的校园网 Eduroam 详情请点击 NUS WiFi 介绍]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>NUS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[咸鱼入海Sea]]></title>
    <url>%2Fpost%2FOnboarding%2F</url>
    <content type="text"><![CDATA[求职经历今年研究生毕业，在经历了一波三折的找工作之后。我这个咸鱼最终也找到了一份心仪的工作，在sea公司做Operation Enginner. 我是在春季的校园招聘会上投的简历，今天还翻到了曾经在招聘会上领取的资料 一般找工作的途径： 找学长学姐内推 通过校园招聘会现场投简历 公司官网在线投简历 招聘网站，像Monster，Glassdoor，Jobstreet. 如果是NUS学生，还可以通过NUS校园求职网站 NUS 找工作的 APP NUS career+ 总的来说，有内推是最好的，响应速度快，简历不容易被刷。其次是校园招聘会和校园求职网，都是校招，难度也低。 公司官网也可以，但要自己去一家家的找。专门的求职网站选择就很多，但自己要耐心的筛选（太多不知道的公司了，我曾经就被一家小作坊坑了，去面试的时候被那工作环境震惊）.心得体会，应届生进入社会第一课：虚心好学，接受自己的平庸。早做准备，明白自己想干什么。举例子，如果打算未来从事某一项工作，那就去看看网上这份工作的描述，对求职者的要求是什么。（PS:简历也是很重要的一部分）祝大家求职顺利! 公司介绍Sea是东南亚的一家互联网公司，旗下有garena（东南亚游戏代理：王者荣耀，英雄联盟等），shopee（东南亚的电商），airpay（想做东南亚的支付宝）。 公司的员工都很年轻，几乎都是美丽小姐姐、帅气小哥哥们，超级nice的。公司的环境也相当的棒： 入职大礼包 还有互联网公司标配的Mac Pro 两个戴尔 U2417H 显示器 公司福利 每个月还有两次马杀鸡按摩 还有最受好评的小卖部，视频链接在这里 早上还有一盒水果和晚上的工作餐 环境福利虽好，但每晚大概吃完饭都八点了，搬砖狗住！ 谨以此文记录咸鱼开启职场生活公司的职位在这里，需要内推的欢迎联系我email: fainyang@foxmail.com]]></content>
      <tags>
        <tag>工作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NUS食堂排队指南]]></title>
    <url>%2Fpost%2FCanteen%2F</url>
    <content type="text"><![CDATA[人生苦短，珍惜时间这个其实是我读研在 NUS 跟着导师做的项目，自认为还是比较有趣的。在食堂吃饭，最怕长队漫漫，还有狗粮作伴。人生很多事情因为慢慢等待就错过了，本来打算去食堂吃某一样菜，却因为漫长的排队等待而望而却步。所以嘛，年轻人要想少留遗憾，就要主动出击，爱要提前行动呀！ 本文主要基于WiFi历史数据，分析NUS 工学院、文学院、YIH食堂的规律（本来最初还计划有computing食堂的，奈何系统上没有相关的信息记录）。用数据说话，分析每天食堂的拥挤时段给大家参考，以便大家减少排队时间，提高用餐体验。 假设每个用户只有一个设备（在食堂用电脑的也比较少），并且在校园内大多数人都会打开WiFi。我们根据WiFi的连接情况，可以间接的判断人群密度，停留时间，流入流出速率。人群密度只需要统计每分钟连接WiFi的设备数即可。停留时间是由思科WiFi系统记录设备第一次连接WiFi的时间和当前时间作差得出，当设备离线超过15min系统会刷新first located time。流入流出速率则是比较前后两分钟mac address的不同来判断得出。 首先举个例子好啦WiFi原始数据如下图所示： 可以在NUS Data Commons用NUS ID登录后在Cisco MSE下载。 经过分析处理，得到的结果图如下所示： 蓝色线条 代表总人数 红色线条 代表平均停留时间 绿色线条 代表流入率 黑色线条 代表流出率 上图是周一E1 6楼3月19号周一的情况。首先蓝色线条有5个峰值，而红色线条有5段逐渐增加的区间。两者结合，我们可知E1该天有5个上课时间段：9AM-11AM, 12PM-2PM, 2PM-4PM, 4PM-6PM, 6PM-9PM。因为在上课时间，总人数肯定比较多，随着课堂的进行，dwell time停留时间也在逐渐增加。在课堂的开始和下课阶段，流入和流出率较大。晚上的课大多数都是master课程，老师一般会在7:30左右课间休息，8:30左右下课。图中的结果和作者亲身经历也完全吻合，侧面说明我们处理分析数据的方法是正确的。 那如何分析食堂的数据呢？大家肯定想知道，什么时间去吃饭，不算太早但排队的人也不多呢？本文分析了2018年春季第10周（3.19-3.23）的数据，采用的分析方法是: 一般在总人数陡增前到达食堂是比较稳的，迟了队伍前面就是千军万马。 平均停留时间开始急速下降的时候，因为如果没有顾客，食堂的device平均停留时间会由于食堂员工的存在随时间不断增加。当有大量学生来时，平均停留时间会因为基数的增大而开始下降。 流入流出率这里不采用，是因为根据作者的亲身经历来看，每分钟的流入流出率因为不可能那么大。误差估计是WiFi系统对于每分钟probing（探测）设备的统计误差，时有时无造成的。 工学院食堂Engineer师生常去的食堂，价格也超级便宜。队伍比较长的一般是西餐，日本料理等。感觉这学期在E1的课比较少，每周只有周一晚上才去吃饭。下图是周一工学院食堂的情况，时间为10AM–7:30PM 从图中我们可以看出，在11:31时，蓝色线条总人数开始迅速增加，红色线条平均停留时间也开始下降，这不正是暴风雨前的预兆吗？说明这个时间是用餐高峰期的开始。同样的，晚餐的用餐高峰期是从5:20开始的。 文学院食堂文学院食堂作者最爱的是海南鸡饭，三楼凉拌功夫的麻辣香锅也很不错。作者曾经有一次亲身经历，11:40到文学院食堂，人多到我想放弃。。。下图是周一文学院食堂的情况，时间为10AM–7:30PM 文学院食堂从11:20，平均停留时间就开始下滑，11:30后总人数也是急剧攀升。说明这个11:20-11:30时间是用餐高峰期的开始。 晚餐虽然呈现出变化趋势，但真的一点都不拥挤。。。 YIH食堂YIH的mixed food每次人都超级多，排队时还经常可以吃狗粮，美滋滋。但YIH相比于前面两个食堂，人数明显是少很多，因为食堂本来就不是很大。下图是周一YIH食堂的情况，时间为10AM–7:30PM 从上图可以看出，午餐高峰时间从11:20开始，总人数开始增加，平均停留时间迅速减小，这不正是千军万马来临前的预兆吗？ 同样的，晚餐高峰是从5:29开始的。 表格总结 划重点每个食堂都分析了一周5天的数据，结果汇总在下表： 敲黑板:一般来说中午下课时间大概是在11:30，所以赶在这之前去食堂，人都会比较少。晚上除了YIH，其余两个食堂也不怎么拥挤。 剩下的图：周二到周五 最后的吐槽最后的一个月，马上就要毕业了，最近忙的没有时间写博客。课程，项目，论文，期末考试，找工作都是一座座艰难的大山。狗住Uo･ｪ･oU加油٩(●˙▿˙●)۶…⋆ฺ PS: 如果对这个分析有更好的建议，或者发现了错误之处。欢迎交流，联系方式: QQ: 601300870 Email: fainyang@foxmail.com]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>NUS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小白也会用的微信机器人]]></title>
    <url>%2Fpost%2FRobot%2F</url>
    <content type="text"><![CDATA[天下武功，唯快不破这一篇文章是讲解如何安装微信自动回复机器人，独乐乐不如众乐乐。画风突转，先来一段鸡汤：并不是每个人都值得你腾出手中的事情秒回，如果有那么一个人，让你情不自禁地秒回了，说明那个人何其重要。或者说，你被别人秒回，又是多么温暖的存在啊。他秒回的瞬间，仿佛在说：你在我心中，很重要。 只可惜我只是一个机器人，惊不惊喜，意不意外？ 废话不多说，先来看一看效果图： 看完是不是有点小激动呢？ 老司机通道：会python的直接往下拉--看运行机器人程序 第一步安装python环境俗话说，人生苦短，我用python。安装python就像安装游戏、QQ一样，去官网下载安装包，只是安装后桌面没有运行图标，而是在我们的计算机上安装了一个运行环境。 看Windows-python安装教程如果不明白的话，再看看视频教程。 用苹果电脑的点击观看苹果电脑python安装教程。如果看不懂，可以去视频网站搜python安装教学视频，一边看视频一边依葫芦画瓢，我在这里就不详细说明了。万事开头难，搞不懂的话你可以微信问我呀，哈哈。 安装好以后，在Windows命令行 or Mac的终端检查一下python的version版本 安装python包当你把python环境安装好了以后，接下来你需要安装python包（python包是武林高手们打造的神兵利器，从新手村刚刚出来的你再也不用苦苦磨炼，自己去做武器啦） 那么问题来了，你怎么样从大侠手中借到神兵利器？？？这个时候就需要我们常说的py交易（开玩笑的啦），肯定是用pip。此外，这个机器人自动回复，需要两个包：一个是requests，另一个是itchat。 在Windows命令行 or Mac的终端 输入以下命令123pip install requestspip install itchat 当用pip借到大佬们的神兵利器后，终于可以开始打怪升级了，美滋滋。 运行机器人程序第一步需要点击下载源代码 下载时存放的地点要记住。下载之后解压文件Test-master.zip，其中的tuling.py就是我们的微信自动回复机器人小程序 。在文件夹中找到tuling.py,右键=&gt;打开方式用记事本打开，其中有两个地方需要修改。 第一个是图灵机器人的apikey，首先点击打开图灵机器人网站注册一个账号，免费创建一个机器人，得到机器人的apikey 第二个地方就是： 1itchat.auto_login(enableCmdQR=2) 修改为如下形式：这样是为了弹窗形式二维码比命令行形式的二维码清晰1itchat.auto_login() 至此，我们得到了机器人apikey，修改了二维码形式，就可以运行tuling.py了。 （最简单的小白法） Mac的终端 输入以下命令：12cd xxxx/Test-master #注释：先移动到tuling.py 所在文件python tuling.py #释：运行tuling.py Windows用户在Test-master文件夹打开命令行，方法为：在此文件夹窗口内空白区域右键单击（需要同时按住Shift），从菜单中选择＂在此处打开命令行窗口＂的项。 然后在命令窗口输入1python tuling.py 如果前面python包成功安装，运行将会弹出来一个网页登录的二维码。微信扫码登录后，看到命令窗口的个人信息就意味着大功告成了。12345678Getting uuid of QR code.Downloading QR code.Please scan the QR code to log in.Please press confirm on your phone.Loading the contact, this may take a little while.TERM environment variable not set.Login successfully as Fain 费洋Start auto replying. 但是这个机器人有一个缺陷：一旦运行python的电脑断网、休眠、关机。那么自动回复将会失效，因为自动回复的核心是电脑运行python调用图灵机器人来进行回复。但是对于小白来讲，这就已经足够~(≧▽≦)~啦啦啦。 兽（瘦）人永不为奴，云服务器永不关机我这周刚买的腾讯云服务器，学生优惠下来3年360，超级划算的。 我选择的系统是CentOS7.4（自带python2.7），需要安装python3.6–请参考教程1 和教程2 安装好python环境以后，也需要安装两个包，一个是requests，另一个是itchat。然后是在本地用scp把tuling.py 上传到云服务器，方法请参考这里。1scp 本地文件地址 云服务器登录名@云服务器公网IP/域名:云服务器文件地址 上传完成以后，运行试一试？结果你会发现当我们远程ssh关闭的时候，云服务器中的python程序也关闭了。为了能长时间运行，你需要使用Linux 技巧来续几年。 1setsid python tuling.py 这样我们的机器人就能在云服务器一直运行啦，直到天荒地老，海枯石烂？ 错！是服务器到期欠费。]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排列组合了解一下(阿里笔试题改编)]]></title>
    <url>%2Fpost%2FCombination%2F</url>
    <content type="text"><![CDATA[问题介绍假设有小费和小王俩好基友，天天形影不离，二人分在一个组。为了减少迟到的现象，班主任规定如下： 迟到一次，则当事人记一分。（迟到） 小组两人在某一天同时迟到，则小组总分再记一分。（连坐） 一周内如果每天都迟到，则当事人再扣一分。（太皮了） 三种情况可累加记分，一周结算一次小组总分。 肯定有人想说 举个例子好了下图为某一周的出勤情况：从图中看出小费同学比较皮，天天迟到！小王同学有两天和小费同学一起开黑，导致了迟到。所以总的小组计分为10分！ 那么问题来了：如果已知某一周小组的总分情况，列出可能的出勤表 咸鱼采用的暴力破解法有两个对象，每个对象有5个记录。每次记录的值有两种情况，我们可以暴力枚举出所有的情况。一共也才2的10次方，1024种情况。对于计算机来说，当然是小菜一碟咯。 所有上面的出勤图就可以简化为0-1数值图： 接下来问题就变成，如何枚举出这1024种情况？ 2行5列一共10个值，循环列出0-1111111111二进制数的情况，高位补0。 如上图，2列是独立的，我们只需求出5位的情况，两列互相遍历即可。 用python自带的 itertools库，分分钟枚举完成。 12345678910111213import itertoolsdef main(start_day,faith_score): Calendar=[[6,6,6,6,6],[6,6,6,6,6]] #模拟日历2行5列 L=list(itertools.product([0,1],repeat=5)) #枚举产生 leng=len(L) for i in range(leng):#两次遍历 for j in range(leng): Calendar[0]=L[i] Calendar[1]=L[j] if sumcredit(Calendar)==faith_score:#判断情况 gg=todate(Calendar,start_day) print(gg) #输出情况 只要有了枚举的所有情况，判断计分则是很容易的事情。 123456789101112131415161718192021222324252627282930def credit1(L): #迟到一次扣一分 credit=0 for i in L[0]: if i==1: credit+=1 for j in L[1]: if j==1: credit+=1 return creditdef credit2 (L): #一起迟到，多扣一分 credit=0 for i in range(5): if L[0][i]==1 and L[1][i]==1: credit+=1 return creditdef credit3 (L): #连续迟到，罪加一等 credit=0 if 0 not in L[0]: credit+=1 if 0 not in L[1]: credit+=1 return creditdef sumcredit(L): #累加总分 c1=credit1(L) c2=credit2(L) c3=credit3(L) return c1+c2+c3 时间问题，不容小觑因为时间在跨月份是时候有点麻烦，不能用普通的++，但我们可以用datetime库呀。输入一个起始时间，我们要得到那一周的5天。timedelta()函数你值得拥有。 12345678910111213141516171819from datetime import datetime,timedeltadef get_date(date): #Y表示四位数的年份(0-9999) date_begin=datetime.strptime(date,'%Y%m%d') time=str(datetime.date(date_begin)) date1=date_begin+timedelta(days=1) #加1s，哈哈 time1=str(datetime.date(date1)) date2=date_begin+timedelta(days=2) time2=str(datetime.date(date2)) date3=date_begin+timedelta(days=2) time3=str(datetime.date(date3)) date4=date_begin+timedelta(days=2) time4=str(datetime.date(date4)) return time,time1,time2,time3,time4 最后我们在输出 出勤情况的时候，将那个简化后的二维数组List=&gt;日历对应起来。 1234567891011def todate(L,date): time,time1,time2,time3,time4=get_date(date) record=[time+'-小费',time+'-小王',time1+'-小费',time1+'-小王',time2+'-小费',\ time2+'-小王',time3+'-小费',time3+'-小王',time4+'-小费',time4+'-小王'] L1=L[0]+L[1] for i in range(len(L1)): if L1[i]==1: #1 代表迟到 record[i]+='(迟到)' else: #0代表没迟到 record[i]+='(正常)' return record 假设我们输入date=20180305,score=0输出如下结果：123456['2018-03-04-小费(正常)', '2018-03-04-小王(正常)', '2018-03-05-小费(正常)', '2018-03-05-小王(正常)', '2018-03-06-小费(正常)', '2018-03-06-小王(正常)', '2018-03-06-小费(正常)', '2018-03-06-小王(正常)', '2018-03-06-小费(正常)', '2018-03-06-小王(正常)'][Finished in 0.2s] 假设我们输入date=20180305,score=1输出如下结果： 最后一点废话有一篇关于itertools的文章这段代码很Pythonic | 相见恨晚的 itertools 库 安利一下。此文章的python源代码 马上就要毕业了，再也没有当学生那种无忧无虑的感觉了。以前贪玩欠的债，找工作的时候哭出来。去面试才知道自己有多菜，所幸我还比较naive，未来珍惜时间，好好学习。狗住，加油！]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[时间求差值timestampdiff]]></title>
    <url>%2Fpost%2Ftimestampdiff%2F</url>
    <content type="text"><![CDATA[问题背景介绍因为Prof想知道每个building里的average-dwell时间，NUS-WiFi数据库提供了current-time，first-time。思路就是根据两者时间的差值来估计stay in building的时间。 人生苦短，首选python第一想法当然是用python，用python包：time, datetime.代码如下： 123456789101112131415import timeimport datetimedef difftime(begintime,endtime): date1=time.strptime(begintime,'%Y-%m-%d %H:%M:%S') date2=time.strptime(endtime,'%Y-%m-%d %H:%M:%S') #y两位数的年份表示（00-99） #Y四位数的年份表示（000-9999） date3=datetime.datetime( date1[0],date1[1],date1[2],date1[3],date1[4],date1[5]) date4=datetime.datetime( date2[0],date2[1],date2[2],date2[3],date2[4],date2[5]) minutes=(date4-date3).seconds/60 return minutes 结果如下图： MySQL 表示不服上学期我用python做的统计工作，例如每分钟有多少device，或者每个device在building里停留了多少分钟.用python至少得写30+行代码去读写csv文件，在MySQL就是一行代码的事情，相见恨晚。无知限制了我的生产力！MySQL的数据类型里有时间戳格式timestamp。也有专门计算时间差值的timestampdiff函数。TIMESTAMPDIFF(unit,begin,end);返回end-begin的时间差值。 1select timestampdiff(minute,firstime,curtime) 结果如下图： INSERT遇到问题了我想把timestampdiff返回的值插入在表的最后，先创建一个字段（列），再用insert插入，结果遇到了问题。 1insert into test (diff) select timestampdiff(minute,firstime,curtime) from test; 如图所示，结果是插在了diff列，但直接插在表的后面，没有覆盖原有的值。经过思考后，我尝试了 update的方法。 1update test set diff=timestampdiff(minute,firstime,curtime); 终于成功了！ 咸鱼的叹息python定义了一个函数来计算，用MySQL就是一句话的事情。光找这句话我就花了好久，还得好好学习。还是在项目中体会得多，光看书是不行的，纸上得来终觉浅。 很惭愧，连一点微小的工作都没有做，打扰了。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开始blog啦]]></title>
    <url>%2Fpost%2Fblog%2F</url>
    <content type="text"><![CDATA[第一篇post虽然还不知道怎么用，先把这个网站搭起再说。生活需要多点乐趣，人生苦短，需要一点甜甜的爱。最幸福的事情莫过于找到所爱，所爱的事业，所爱的人。除了美食，最近我也慢慢开始发现编程的乐趣。今年的愿望是：每晚早点休息，年轻人不要熬夜。晚安于 2018-02-20 深夜]]></content>
      <categories>
        <category>生活</category>
      </categories>
  </entry>
</search>
